%%%%(c)
%%%%(c)  This file is a portion of the source for the textbook
%%%%(c)
%%%%(c)    A First Course in Linear Algebra
%%%%(c)    Copyright 2004 by Robert A. Beezer
%%%%(c)
%%%%(c)  See the file COPYING.txt for copying conditions
%%%%(c)
%%%%(c)
%%%%%%%%%%%
%%
%%  Section SS
%%  Spanning Sets
%%
%%%%%%%%%%%
%
In this section we will describe a compact way to indicate the elements of an infinite set of vectors, making use of linear combinations.  This will give us a convenient way to describe the elements of a set of solutions to a linear system, or the elements of the null space of a matrix, or many other sets of vectors.
%
\subsect{SSV}{Span of a Set of Vectors}
%
In \acronymref{example}{VFSAL} we saw the solution set of a homogeneous system described as all possible linear combinations of two particular vectors.  This happens to be a useful way to construct or describe infinite sets of vectors, so we encapsulate this idea in a definition.
%
%  Any changes here should be carried forward to Definition SS.
%
\begin{definition}{SSCV}{Span of a Set of Column Vectors}{span}
Given a set of vectors $S=\{\vectorlist{u}{p}\}$, their \define{span}, $\spn{S}$, is the set of all possible linear combinations of $\vectorlist{u}{p}$.  Symbolically,
%
\begin{align*}
\spn{S}&=\setparts{\lincombo{\alpha}{u}{p}}{\alpha_i\in\complex{\null},\,1\leq i\leq p}\\
&=\setparts{\sum_{i=1}^{p}\alpha_i\vect{u}_i}{\alpha_i\in\complex{\null},\,1\leq i\leq p}
\end{align*}
%
\denote{SSV}{Span of a Set of Vectors}{$\spn{S}$}{span}
\end{definition}
%
The span is just a set of vectors, though in all but one situation it is an infinite set.  (Just when is it not infinite?)  So we start with a finite collection of vectors $S$ ($p$ of them to be precise), and use this finite set to describe an infinite set of vectors, $\spn{S}$.  Confusing the {\em finite} set $S$ with the {\em infinite} set $\spn{S}$ is one of the most pervasive problems in understanding introductory linear algebra.  We will see this construction repeatedly, so let's work through some examples to get comfortable with it.  The most obvious question about a set is if a particular item of the correct type is in the set, or not.
%
%
\begin{example}{ABS}{A basic span}{span!basic}
Consider the set of 5 vectors, $S$, from $\complex{4}$
%
\begin{equation*}
S=\set{
 \colvector{1 \\ 1 \\ 3 \\ 1},\,
 \colvector{2 \\ 1 \\ 2 \\ -1},\,
 \colvector{7 \\ 3 \\ 5 \\ -5},\,
 \colvector{1 \\ 1 \\ -1 \\ 2},\,
 \colvector{-1 \\ 0 \\ 9 \\ 0}
}
\end{equation*}
%
and consider the infinite set of vectors $\spn{S}$ formed from all possible linear combinations of the elements of $S$.  Here are four vectors we definitely know are elements of $\spn{S}$, since we will construct them in accordance with \acronymref{definition}{SSCV},
%
\begin{equation*}
\vect{w}=
(2)\colvector{1 \\ 1 \\ 3 \\ 1}+
(1)\colvector{2 \\ 1 \\ 2 \\ -1}+
(-1)\colvector{7 \\ 3 \\ 5 \\ -5}+
(2)\colvector{1 \\ 1 \\ -1 \\ 2}+
(3)\colvector{-1 \\ 0 \\ 9 \\ 0}
=
\colvector{-4\\2\\28\\10}
\end{equation*}
%
%
\begin{equation*}
\vect{x}=
(5)\colvector{1 \\ 1 \\ 3 \\ 1}+
(-6)\colvector{2 \\ 1 \\ 2 \\ -1}+
(-3)\colvector{7 \\ 3 \\ 5 \\ -5}+
(4)\colvector{1 \\ 1 \\ -1 \\ 2}+
(2)\colvector{-1 \\ 0 \\ 9 \\ 0}
=
\colvector{-26\\-6\\2\\34}
\end{equation*}
%
\begin{equation*}
\vect{y}=
(1)\colvector{1 \\ 1 \\ 3 \\ 1}+
(0)\colvector{2 \\ 1 \\ 2 \\ -1}+
(1)\colvector{7 \\ 3 \\ 5 \\ -5}+
(0)\colvector{1 \\ 1 \\ -1 \\ 2}+
(1)\colvector{-1 \\ 0 \\ 9 \\ 0}
=
\colvector{7\\4\\17\\-4}
\end{equation*}
%
\begin{equation*}
\vect{z}=
(0)\colvector{1 \\ 1 \\ 3 \\ 1}+
(0)\colvector{2 \\ 1 \\ 2 \\ -1}+
(0)\colvector{7 \\ 3 \\ 5 \\ -5}+
(0)\colvector{1 \\ 1 \\ -1 \\ 2}+
(0)\colvector{-1 \\ 0 \\ 9 \\ 0}
=
\colvector{0\\0\\0\\0}
\end{equation*}
%
The purpose of a set is to collect objects with some common property, and to exclude objects without that property.  So the most fundamental question about a set is if a given object is an element of the set or not.  Let's learn more about $\spn{S}$ by investigating which vectors are elements of the set, and which are not.\par
%
First, is $\vect{u}=\colvector{-15\\-6\\19\\5}$ an element of $\spn{S}$?  We are asking if there are scalars $\alpha_1,\,\alpha_2,\,\alpha_3,\,\alpha_4,\,\alpha_5$ such that
%
\begin{equation*}
\alpha_1\colvector{1 \\ 1 \\ 3 \\ 1}+
\alpha_2\colvector{2 \\ 1 \\ 2 \\ -1}+
\alpha_3\colvector{7 \\ 3 \\ 5 \\ -5}+
\alpha_4\colvector{1 \\ 1 \\ -1 \\ 2}+
\alpha_5\colvector{-1 \\ 0 \\ 9 \\ 0}
=\vect{u}
=\colvector{-15\\-6\\19\\5}
\end{equation*}
%
Applying \acronymref{theorem}{SLSLC} we recognize the search for these scalars as a solution to a linear system of equations with augmented matrix
%
\begin{equation*}
\begin{bmatrix}
 1 & 2 & 7 & 1 & -1 & -15 \\
 1 & 1 & 3 & 1 & 0 & -6 \\
 3 & 2 & 5 & -1 & 9 & 19 \\
 1 & -1 & -5 & 2 & 0 & 5
\end{bmatrix}
\end{equation*}
%
which row-reduces to
%
\begin{equation*}
\begin{bmatrix}
 \leading{1} & 0 & -1 & 0 & 3 & 10 \\
 0 & \leading{1} & 4 & 0 & -1 & -9 \\
 0 & 0 & 0 & \leading{1} & -2 & -7 \\
 0 & 0 & 0 & 0 & 0 & 0
\end{bmatrix}
\end{equation*}
%
At this point, we see that the system is consistent (\acronymref{theorem}{RCLS}), so we know there {\em is} a solution for the five scalars $\alpha_1,\,\alpha_2,\,\alpha_3,\,\alpha_4,\,\alpha_5$.  This is enough evidence for us to say that $\vect{u}\in\spn{S}$.  If we wished further evidence, we could compute an actual solution, say
%
\begin{align*}
\alpha_1&=2
&
\alpha_2&=1
&
\alpha_3&=-2
&
\alpha_4&=-3
&
\alpha_5&=2
\end{align*}
%
This particular solution allows us to write
%
\begin{equation*}
(2)\colvector{1 \\ 1 \\ 3 \\ 1}+
(1)\colvector{2 \\ 1 \\ 2 \\ -1}+
(-2)\colvector{7 \\ 3 \\ 5 \\ -5}+
(-3)\colvector{1 \\ 1 \\ -1 \\ 2}+
(2)\colvector{-1 \\ 0 \\ 9 \\ 0}
=\vect{u}
=\colvector{-15\\-6\\19\\5}
\end{equation*}
%
making it even more obvious that $\vect{u}\in\spn{S}$.\par
%
Lets do it again.   Is $\vect{v}=\colvector{3\\1\\2\\-1}$ an element of $\spn{S}$?  We are asking if there are scalars $\alpha_1,\,\alpha_2,\,\alpha_3,\,\alpha_4,\,\alpha_5$ such that
%
\begin{equation*}
\alpha_1\colvector{1 \\ 1 \\ 3 \\ 1}+
\alpha_2\colvector{2 \\ 1 \\ 2 \\ -1}+
\alpha_3\colvector{7 \\ 3 \\ 5 \\ -5}+
\alpha_4\colvector{1 \\ 1 \\ -1 \\ 2}+
\alpha_5\colvector{-1 \\ 0 \\ 9 \\ 0}
=\vect{v}
=\colvector{3\\1\\2\\-1}
\end{equation*}
%
Applying \acronymref{theorem}{SLSLC} we recognize the search for these scalars as a solution to a linear system of equations with augmented matrix
%
\begin{equation*}
\begin{bmatrix}
 1 & 2 & 7 & 1 & -1 & 3 \\
 1 & 1 & 3 & 1 & 0 & 1 \\
 3 & 2 & 5 & -1 & 9 & 2 \\
 1 & -1 & -5 & 2 & 0 & -1
\end{bmatrix}
\end{equation*}
%
which row-reduces to
%
\begin{equation*}
\begin{bmatrix}
 \leading{1} & 0 & -1 & 0 & 3 & 0 \\
 0 & \leading{1} & 4 & 0 & -1 & 0 \\
 0 & 0 & 0 & \leading{1} & -2 & 0 \\
 0 & 0 & 0 & 0 & 0 & \leading{1}
\end{bmatrix}
\end{equation*}
%
At this point, we see that the system is inconsistent by \acronymref{theorem}{RCLS}, so we know there {\em is not} a solution for the five scalars $\alpha_1,\,\alpha_2,\,\alpha_3,\,\alpha_4,\,\alpha_5$.  This is enough evidence for us to say that $\vect{v}\not\in\spn{S}$.  End of story.
%
\end{example}
%
\begin{example}{SCAA}{Span of the columns of Archetype A}{span of columns!Archetype A}
Begin with the finite set of three vectors of size $3$
%
\begin{equation*}
S=\{\vect{u}_1,\,\vect{u}_2,\,\vect{u}_3\}
=\left\{
\colvector{1\\2\\1},\,\colvector{-1\\1\\1},\,\colvector{2\\1\\0}
\right\}
\end{equation*}
%
and consider the infinite set $\spn{S}$.  The vectors of $S$ could have been chosen to be anything, but for reasons that will become clear later, we have chosen the three columns of the coefficient matrix in \acronymref{archetype}{A}.
%
First, as an example, note that
%
\begin{equation*}
\vect{v}=(5)\colvector{1\\2\\1}+(-3)\colvector{-1\\1\\1}+(7)\colvector{2\\1\\0}=
\colvector{22\\14\\2}
\end{equation*}
%
is in $\spn{S}$, since it is a linear combination of $\vect{u}_1,\,\vect{u}_2,\,\vect{u}_3$.  We write this succinctly as $\vect{v}\in\spn{S}$.  There is nothing magical about the scalars $\alpha_1=5,\,\alpha_2=-3,\,\alpha_3=7$, they could have been chosen to be anything.  So repeat this part of the example yourself, using different values of $\alpha_1,\,\alpha_2,\,\alpha_3$.  What happens if you choose all three scalars to be zero?\par
%
So we know how to quickly construct sample elements of the set $\spn{S}$.  A slightly different question arises when you are handed a vector of the correct size and asked if it is an element of $\spn{S}$.  For example, is $\vect{w}=\colvector{1\\8\\5}$ in $\spn{S}$?  More succinctly, $\vect{w}\in\spn{S}$?\par
%
To answer this question, we will look for scalars $\alpha_1,\,\alpha_2,\,\alpha_3$ so that
%
\begin{align*}
\alpha_1\vect{u}_1+\alpha_2\vect{u}_2+\alpha_3\vect{u}_3&=\vect{w}\\
%
\intertext{By \acronymref{theorem}{SLSLC} solutions to this vector equation are solutions to the system of equations}
%
\alpha_1-\alpha_2+2\alpha_3&=1\\
2\alpha_1+\alpha_2+\alpha_3&=8\\
\alpha_1+\alpha_2&=5
%
\intertext{Building the augmented matrix for this linear system, and row-reducing, gives}
%
\archetypepart{A}{augmentedreduced}
\end{align*}
%
This system has infinitely many solutions (there's a free variable in $x_3$), but all we need is one solution vector.  The solution,
\begin{align*}
\alpha_1 &= 2
&
\alpha_2 &= 3
&
\alpha_3 &= 1
\end{align*}
%
tells us that
%
\begin{equation*}
(2)\vect{u}_1+(3)\vect{u}_2+(1)\vect{u}_3=\vect{w}
\end{equation*}
%
so we are convinced that $\vect{w}$ really is in $\spn{S}$.  Notice that there are an infinite number of ways to answer this question affirmatively.  We could choose a different solution, this time choosing the free variable to be zero,
\begin{align*}
\alpha_1 &= 3
&
\alpha_2 &= 2
&
\alpha_3 &= 0
\end{align*}
%
shows us that
%
\begin{equation*}
(3)\vect{u}_1+(2)\vect{u}_2+(0)\vect{u}_3=\vect{w}
\end{equation*}
%
Verifying the arithmetic in this second solution will make it obvious that $\vect{w}$ is in this span.  And of course, we now realize that there are an infinite number of ways to realize $\vect{w}$ as element of $\spn{S}$.
%
Let's ask the same type of question again, but this time with $\vect{y}=\colvector{2\\4\\3}$, i.e.\ is  $\vect{y}\in\spn{S}$?\par
%
So we'll look for scalars $\alpha_1,\,\alpha_2,\,\alpha_3$ so that
%
\begin{align*}
\alpha_1\vect{u}_1+\alpha_2\vect{u}_2+\alpha_3\vect{u}_3&=\vect{y}\\
%
\intertext{By \acronymref{theorem}{SLSLC} solutions to this vector equation are the solutions to the system of equations}
%
\alpha_1-\alpha_2+2\alpha_3&=2\\
2\alpha_1+\alpha_2+\alpha_3&=4\\
\alpha_1+\alpha_2&=3
%
\intertext{Building the augmented matrix for this linear system, and row-reducing, gives}
%
\begin{bmatrix}
\leading{1} & 0 & 1 & 0\\
0 & \leading{1} & -1 & 0\\
0 & 0 & 0 & \leading{1}
\end{bmatrix}
\end{align*}
%
This system is inconsistent (there's a leading 1 in the last column, \acronymref{theorem}{RCLS}), so there are no scalars $\alpha_1,\,\alpha_2,\,\alpha_3$ that will create a linear combination of $\vect{u}_1,\,\vect{u}_2,\,\vect{u}_3$ that equals $\vect{y}$.  More precisely, $\vect{y}\not\in\spn{S}$.\par
%
There are three things to observe in this example.  (1) It is easy to construct vectors in $\spn{S}$.  (2) It is possible that some vectors are in $\spn{S}$ (e.g.\ $\vect{w}$), while others are not (e.g.\ $\vect{y}$).  (3)  Deciding if a given vector is in $\spn{S}$ leads to solving a linear system of equations and asking if the system is consistent.\par
%
With a computer program in hand to solve systems of linear equations, could you create a program to decide if a vector was, or wasn't, in the span of a given set of vectors?  Is this art or science?\par
%
This example was built on vectors from the columns of the coefficient matrix of \acronymref{archetype}{A}.  Study the determination that $\vect{v}\in\spn{S}$ and see if you can connect it with some of the other properties of \acronymref{archetype}{A}.
%
\end{example}
%
Having analyzed \acronymref{archetype}{A} in \acronymref{example}{SCAA}, we will of course subject \acronymref{archetype}{B} to a similar investigation.
%
%
\begin{example}{SCAB}{Span of the columns of Archetype B}{span of columns!Archetype B}
Begin with the finite set of three vectors of size $3$ that are the columns of the coefficient matrix in \acronymref{archetype}{B},
%
\begin{equation*}
R=\{\vect{v}_1,\,\vect{v}_2,\,\vect{v}_3\}
=\left\{
\colvector{-7\\5\\1},\,\colvector{-6\\5\\0},\,\colvector{-12\\7\\4}
\right\}
\end{equation*}
%
and consider the infinite set $\spn{R}$.
%
First, as an example, note that
%
\begin{equation*}
\vect{x}=(2)\colvector{-7\\5\\1}+(4)\colvector{-6\\5\\0}+(-3)\colvector{-12\\7\\4}=
\colvector{-2\\9\\-10}
\end{equation*}
%
is in $\spn{R}$, since it is a linear combination of $\vect{v}_1,\,\vect{v}_2,\,\vect{v}_3$.  In other words, $\vect{x}\in\spn{R}$.  Try some different values of $\alpha_1,\,\alpha_2,\,\alpha_3$ yourself, and see what vectors you can create as elements of $\spn{R}$.\par
%
Now ask if a given vector is an element of $\spn{R}$.  For example, is $\vect{z}=\colvector{-33\\24\\5}$ in $\spn{R}$?   Is $\vect{z}\in\spn{R}$?\par
%
To answer this question, we will look for scalars $\alpha_1,\,\alpha_2,\,\alpha_3$ so that
%
\begin{align*}
\alpha_1\vect{v}_1+\alpha_2\vect{v}_2+\alpha_3\vect{v}_3&=\vect{z}\\
%
\intertext{By \acronymref{theorem}{SLSLC} solutions to this vector equation are the solutions to the system of equations}
%
-7\alpha_1-6\alpha_2-12\alpha_3&=-33\\
5\alpha_1+5\alpha_2+7\alpha_3&=24\\
\alpha_1+4\alpha_3&=5
%
\intertext{Building the augmented matrix for this linear system, and row-reducing, gives}
%
\archetypepart{B}{augmentedreduced}
\end{align*}
%
This system has a unique solution,
\begin{align*}
\alpha_1 = -3&&\alpha_2 = 5&&\alpha_3 = 2
\end{align*}
%
telling us that
%
\begin{equation*}
(-3)\vect{v}_1+(5)\vect{v}_2+(2)\vect{v}_3=\vect{z}
\end{equation*}
%
so we are convinced that $\vect{z}$ really is in $\spn{R}$.  Notice that in this case we have only one way to answer the question affirmatively since the solution is unique.\par
%
Let's ask about another vector, say is $\vect{x}=\colvector{-7\\8\\-3}$ in $\spn{R}$?   Is $\vect{x}\in\spn{R}$?\par
%
We desire scalars $\alpha_1,\,\alpha_2,\,\alpha_3$ so that
%
\begin{align*}
\alpha_1\vect{v}_1+\alpha_2\vect{v}_2+\alpha_3\vect{v}_3&=\vect{x}\\
%
\intertext{By \acronymref{theorem}{SLSLC} solutions to this vector equation are the solutions to the system of equations}
%
-7\alpha_1-6\alpha_2-12\alpha_3&=-7\\
5\alpha_1+5\alpha_2+7\alpha_3&=8\\
\alpha_1+4\alpha_3&=-3
%
\intertext{Building the augmented matrix for this linear system, and row-reducing, gives}
%
\begin{bmatrix}
 \leading{1} & 0 & 0 & 1 \\
 0 & \leading{1} & 0 & 2 \\
 0 & 0 & \leading{1} & -1
\end{bmatrix}
%
\end{align*}
This system has a unique solution,
\begin{align*}
\alpha_1 = 1&&\alpha_2 = 2&&\alpha_3 = -1
\end{align*}
%
telling us that
%
\begin{equation*}
(1)\vect{v}_1+(2)\vect{v}_2+(-1)\vect{v}_3=\vect{x}
\end{equation*}
%
so we are convinced that $\vect{x}$ really is in $\spn{R}$.  Notice that in this case we again have only one way to answer the question affirmatively since the solution is again unique.\par
%
We could continue to test other vectors for membership in $\spn{R}$, but there is no point.   A question about membership in $\spn{R}$ inevitably leads to a system of three equations in the three variables $\alpha_1,\,\alpha_2,\,\alpha_3$ with a coefficient matrix whose columns are the vectors $\vect{v}_1,\,\vect{v}_2,\,\vect{v}_3$.  This particular coefficient matrix is nonsingular, so by \acronymref{theorem}{NMUS}, the system is guaranteed to have a solution.  (This solution is unique, but that's not critical here.)  So {\em no matter} which vector we might have chosen for $\vect{z}$, we would have been {\em certain} to discover that it was an element of $\spn{R}$.  Stated differently, every vector of size 3 is in $\spn{R}$, or $\spn{R}=\complex{3}$.\par
%
Compare this example with \acronymref{example}{SCAA}, and see if you can connect $\vect{z}$ with some aspects of the write-up for \acronymref{archetype}{B}.
%
\end{example}
%
\subsect{SSNS}{Spanning Sets of Null Spaces}
%
We saw in \acronymref{example}{VFSAL} that when a system of equations is homogeneous the solution set can be expressed in the form described by \acronymref{theorem}{VFSLS} where the vector $\vect{c}$ is the zero vector.  We can essentially ignore this vector, so that the remainder of the typical expression for a solution looks like an arbitrary linear combination, where the scalars are the free variables and the vectors are $\vectorlist{u}{n-r}$.  Which sounds a lot like a span.  This is the substance of the next theorem.
%
\begin{theorem}{SSNS}{Spanning Sets for Null Spaces}{null space!spanning set}
Suppose that $A$ is an $m\times n$ matrix, and $B$ is a row-equivalent matrix in reduced row-echelon form with $r$ nonzero rows.  Let $D=\{d_1,\,d_2,\,d_3,\,\ldots,\,d_r\}$ be the column indices where $B$ has leading 1's (pivot columns) and $F=\{f_1,\,f_2,\,f_3,\,\ldots,\,f_{n-r}\}$ be the set of column indices where $B$ does not have leading 1's.  Construct the $n-r$ vectors $\vect{z}_j$, $1\leq j\leq n-r$ of size $n$ as
%
\begin{equation*}
\vectorentry{\vect{z}_j}{i}=
\begin{cases}
1&\text{if $i\in F$, $i=f_j$}\\
0&\text{if $i\in F$, $i\neq f_j$}\\
-\matrixentry{B}{k,f_j}&\text{if $i\in D$, $i=d_k$}
\end{cases}
\end{equation*}
%
Then the null space of $A$ is given by
%
\begin{equation*}
\nsp{A}=\spn{\left\{\vectorlist{z}{n-r}\right\}}
\end{equation*}
%
\end{theorem}
%
\begin{proof}
Consider the homogeneous system with $A$ as a coefficient matrix, $\homosystem{A}$.  Its set of solutions, $S$, is by \acronymref{definition}{NSM}, the null space of $A$, $\nsp{A}$.  Let $B^{\prime}$ denote the result of row-reducing the augmented matrix of this homogeneous system.  Since the system is homogeneous, the final column of the augmented matrix will be all zeros, and after any number of row operations (\acronymref{definition}{RO}), the column will still be all zeros.  So $B^{\prime}$ has a final column that is totally zeros.\par
%
Now apply \acronymref{theorem}{VFSLS} to $B^{\prime}$, after noting that our homogeneous system must be consistent (\acronymref{theorem}{HSC}).  The vector $\vect{c}$ has zeros for each entry that corresponds to an index in $F$.  For entries that correspond to an index in $D$, the value is $-\matrixentry{B^{\prime}}{k,{n+1}}$, but for $B^{\prime}$ any entry in the final column (index $n+1$) is zero.  So $\vect{c}=\zerovector$.  The vectors $\vect{z}_j$, $1\leq j\leq n-r$ are identical to the vectors $\vect{u}_j$, $1\leq j\leq n-r$ described in \acronymref{theorem}{VFSLS}.  Putting it all together and applying \acronymref{definition}{SSCV} in the final step,
%
\begin{align*}
\nsp{A}
&=S\\
%
&=\setparts{
\vect{c}+\alpha_1\vect{u}_1+\alpha_2\vect{u}_2+\alpha_3\vect{u}_3+\cdots+\alpha_{n-r}\vect{u}_{n-r}
}{
\alpha_1,\,\alpha_2,\,\alpha_3,\,\ldots,\,\alpha_{n-r}\in\complex{\null}
}\\
%
&=\setparts{
\alpha_1\vect{u}_1+\alpha_2\vect{u}_2+\alpha_3\vect{u}_3+\cdots+\alpha_{n-r}\vect{u}_{n-r}
}{
\alpha_1,\,\alpha_2,\,\alpha_3,\,\ldots,\,\alpha_{n-r}\in\complex{\null}
}\\
%
&=\spn{\set{\vectorlist{z}{n-r}}}
%
\end{align*}
%
\end{proof}
%
%
\begin{example}{SSNS}{Spanning set of a null space}{null space!spanning set}
Find a set of vectors, $S$, so that the null space of the matrix $A$ below is the span of $S$, that is, $\spn{S}=\nsp{A}$.
%
\begin{equation*}
A=
\begin{bmatrix}
 1 & 3 & 3 & -1 & -5\\
 2 & 5 & 7 & 1 & 1\\
 1 & 1 & 5 & 1 & 5\\
 -1 & -4 & -2 & 0 & 4
\end{bmatrix}
\end{equation*}
%
The null space of $A$ is the set of all solutions to the homogeneous system $\homosystem{A}$.  If we find the vector form of the solutions to this homogeneous system (\acronymref{theorem}{VFSLS}) then the vectors $\vect{u}_j$, $1\leq j\leq n-r$ in the linear combination are exactly the vectors $\vect{z}_j$, $1\leq j\leq n-r$ described in \acronymref{theorem}{SSNS}.  So we can mimic \acronymref{example}{VFSAL} to arrive at these vectors (rather than being a slave to the formulas in the statement of the theorem).\par
%
Begin by row-reducing $A$.  The result is
%
\begin{equation*}
\begin{bmatrix}
\leading{1} & 0 & 6 & 0 & 4\\
0 & \leading{1} & -1 & 0 & -2\\
0 & 0 & 0 & \leading{1} & 3\\
0 & 0 & 0 & 0 & 0
\end{bmatrix}
\end{equation*}
%
With $D=\set{1,\,2,\,4}$ and $F=\set{3,\,5}$ we recognize that $x_3$ and $x_5$ are free variables and we can express each nonzero row as an expression for the dependent variables $x_1$, $x_2$, $x_4$ (respectively) in the free variables $x_3$ and $x_5$.  With this we can write the vector form of a solution vector as
%
\begin{equation*}
\colvector{x_1\\x_2\\x_3\\x_4\\x_5}=
\colvector{-6x_3-4x_5\\x_3+2x_5\\x_3\\-3x_5\\x_5}=
x_3\colvector{-6\\1\\1\\0\\0}+
x_5\colvector{-4\\2\\0\\-3\\1}
\end{equation*}
%
Then in the notation of \acronymref{theorem}{SSNS},
%
\begin{align*}
\vect{z}_1&=\colvector{-6\\1\\1\\0\\0}&
\vect{z}_2&=\colvector{-4\\2\\0\\-3\\1}
\end{align*}
%
and
%
\begin{equation*}
\nsp{A}
=\spn{\set{\vect{z}_1,\,\vect{z}_2}}
=\spn{\set{\colvector{-6\\1\\1\\0\\0},\,\colvector{-4\\2\\0\\-3\\1}}}
\end{equation*}
%
\end{example}
%
%
\begin{example}{NSDS}{Null space directly as a span}{Null space!as a span}
Let's express the null space of $A$ as the span of a set of vectors, applying \acronymref{theorem}{SSNS} as economically as possible, without reference to the underlying homogeneous system of equations (in contrast to \acronymref{example}{SSNS}).
%
\begin{equation*}
A=
\begin{bmatrix}
 2 & 1 & 5 & 1 & 5 & 1 \\
 1 & 1 & 3 & 1 & 6 & -1 \\
 -1 & 1 & -1 & 0 & 4 & -3 \\
 -3 & 2 & -4 & -4 & -7 & 0 \\
 3 & -1 & 5 & 2 & 2 & 3
\end{bmatrix}
\end{equation*}
%
\acronymref{theorem}{SSNS} creates vectors for the span by first row-reducing the matrix in question.  The row-reduced version of $A$ is
%
\begin{equation*}
B=
\begin{bmatrix}
 \leading{1} & 0 & 2 & 0 & -1 & 2 \\
 0 & \leading{1} & 1 & 0 & 3 & -1 \\
 0 & 0 & 0 & \leading{1} & 4 & -2 \\
 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0
\end{bmatrix}
\end{equation*}
%
We will mechanically follow the prescription of \acronymref{theorem}{SSNS}.  Here we go, in two big steps. \par
%
First, the non-pivot columns have indices $F=\set{3,\,5,\,6}$, so we will construct the $n-r=6-3=3$ vectors with a pattern of zeros and ones corresponding to the indices in $F$.  This is the realization of the first two lines of the three-case definition of the vectors $\vect{z}_j$, $1\leq j\leq n-r$.
%
\begin{align*}
\vect{z}_1&=\colvector{\\ \\ 1\\ \\0 \\ 0}
&
\vect{z}_2&=\colvector{\\ \\ 0\\ \\ 1\\ 0}
&
\vect{z}_3&=\colvector{\\ \\ 0\\ \\ 0\\ 1}
\end{align*}
%
Each of these vectors arises due to the presence of a column that is not a pivot column.  The remaining entries of each vector are the entries of the corresponding non-pivot column, negated, and distributed into the empty slots in order (these slots have indices in the set $D$ and correspond to pivot columns).  This is the realization of the third line of the three-case definition of the vectors $\vect{z}_j$, $1\leq j\leq n-r$.
%
\begin{align*}
\vect{z}_1&=\colvector{-2\\ -1\\ 1\\  0 \\0 \\ 0}
&
\vect{z}_2&=\colvector{1\\ -3\\ 0\\ -4\\ 1\\ 0}
&
\vect{z}_3&=\colvector{-2\\ 1\\ 0\\ 2\\ 0\\ 1}
\end{align*}
%
So, by \acronymref{theorem}{SSNS}, we have
%
\begin{equation*}
\nsp{A}
=
\spn{\set{\vect{z}_1,\,\vect{z}_2,\,\vect{z}_3}}
=
\spn{\set{
\colvector{-2\\ -1\\ 1\\  0 \\0 \\ 0},\,
\colvector{1\\ -3\\ 0\\ -4\\ 1\\ 0},\,
\colvector{-2\\ 1\\ 0\\ 2\\ 0\\ 1}
}}
\end{equation*}
%
We know that the null space of $A$ is the solution set of the homogeneous system $\homosystem{A}$, but nowhere in this application of \acronymref{theorem}{SSNS} have we found occasion to reference the variables or equations of this system.  These details are all buried in the proof of \acronymref{theorem}{SSNS}.
%
\end{example}
%
\computenote{\boolean{hasmathematica}}
{More advanced computational devices will compute the null space of a matrix.}
{
\ifthenelse{\boolean{hasmathematica}}{\computedevicetopic{MMA}{Mathematica}{mathematica}{NS}{Null Space}{null space}}{\relax}
}{
\ifthenelse{\boolean{hasmathematica}}{\quad\acronymref{computation}{NS.MMA}}{\relax}
}
%
Here's an example that will simultaneously exercise the span construction and \acronymref{theorem}{SSNS}, while also pointing the way to the next section.
%
\begin{example}{SCAD}{Span of the columns of Archetype D}{span of columns!Archetype D}
%
Begin with the set of four vectors of size $3$
%
\begin{equation*}
T=\set{\vect{w}_1,\,\vect{w}_2,\,\vect{w}_3,\,\vect{w}_4}
=\set{
\colvector{2\\-3\\1},\,
\colvector{1\\4\\1},\,
\colvector{7\\-5\\4},\,
\colvector{-7\\-6\\-5}
}
\end{equation*}
%
and consider the infinite set $W=\spn{T}$.  The vectors of $T$ have been chosen as the four columns of the coefficient matrix in \acronymref{archetype}{D}.  Check that the vector
%
\begin{equation*}
\vect{z}_2=\colvector{2\\3\\0\\1}
\end{equation*}
%
is a solution to the homogeneous system $\homosystem{D}$ (it is the  vector $\vect{z}_2$ provided by the description of the null space of the coefficient matrix $D$ from \acronymref{theorem}{SSNS}).  Applying \acronymref{theorem}{SLSLC}, we can write the linear combination,
%
\begin{equation*}
2\vect{w}_1+3\vect{w}_2+0\vect{w}_3+1\vect{w}_4=\zerovector
\end{equation*}
%
which we can solve for $\vect{w}_4$,
%
\begin{equation*}
\vect{w}_4=(-2)\vect{w}_1+(-3)\vect{w}_2.
\end{equation*}
%
This equation says that whenever we encounter the vector $\vect{w}_4$, we can replace it with a specific linear combination of the vectors $\vect{w}_1$ and $\vect{w}_2$.  So using $\vect{w}_4$ in the set $T$, along with $\vect{w}_1$ and $\vect{w}_2$, is excessive.  An example of what we mean here can be illustrated by the computation,
%
\begin{align*}
5\vect{w}_1+(-4)\vect{w}_2+6\vect{w}_3+(-3)\vect{w}_4&=
5\vect{w}_1+(-4)\vect{w}_2+6\vect{w}_3+(-3)\left((-2)\vect{w}_1+(-3)\vect{w}_2\right)\\
&=5\vect{w}_1+(-4)\vect{w}_2+6\vect{w}_3+\left(6\vect{w}_1+9\vect{w}_2\right)\\
&=11\vect{w}_1+5\vect{w}_2+6\vect{w}_3
\end{align*}
%
So what began as a linear combination of the vectors $\vect{w}_1,\,\vect{w}_2,\,\vect{w}_3,\,\vect{w}_4$ has been reduced to a linear combination of the vectors $\vect{w}_1,\,\vect{w}_2,\,\vect{w}_3$.  A careful proof using our definition of set equality (\acronymref{definition}{SE}) would now allow us to conclude that this reduction is possible for any vector in $W$, so
%
\begin{equation*}
W=\spn{\left\{\vect{w}_1,\,\vect{w}_2,\,\vect{w}_3\right\}}
\end{equation*}
%
So the span of our set of vectors, $W$, has not changed, but we have {\em described} it by the span of a set of {\em three} vectors, rather than {\em four}.  Furthermore, we can achieve yet another, similar, reduction.\par
%
Check that the vector
%
\begin{equation*}
\vect{z}_1=\colvector{-3\\-1\\1\\0}
\end{equation*}
%
is a solution to the homogeneous system $\linearsystem{D}{\zerovector}$ (it is the  vector $\vect{z}_1$ provided by the description of the null space of the coefficient matrix $D$ from \acronymref{theorem}{SSNS}).  Applying \acronymref{theorem}{SLSLC}, we can write the linear combination,
%
\begin{equation*}
(-3)\vect{w}_1+(-1)\vect{w}_2+1\vect{w}_3=\zerovector
\end{equation*}
%
which we can solve for $\vect{w}_3$,
%
\begin{equation*}
\vect{w}_3=3\vect{w}_1+1\vect{w}_2
\end{equation*}
%
This equation says that whenever we encounter the vector $\vect{w}_3$, we can replace it with a specific linear combination of the vectors $\vect{w}_1$ and $\vect{w}_2$.  So, as before, the vector $\vect{w}_3$ is not needed in the description of $W$, provided we have $\vect{w}_1$ and $\vect{w}_2$ available.  In particular, a careful proof (such as is done in \acronymref{example}{RSC5}) would show that
%
\begin{equation*}
W=\spn{\left\{\vect{w}_1,\,\vect{w}_2\right\}}
\end{equation*}
%
So $W$ began life as the span of a set of four vectors, and we have now shown (utilizing solutions to a homogeneous system) that $W$ can also be described as the span of a set of just two vectors.  Convince yourself that we cannot go any further.  In other words, it is not possible to dismiss either $\vect{w}_1$ or $\vect{w}_2$ in a similar fashion and winnow the set down to just one vector.\par
%
What was it about the original set of four vectors that allowed us to declare certain vectors as surplus?  And just which vectors were we able to dismiss?  And why did we have to stop once we had two vectors remaining?  The answers to these questions motivate ``linear independence,'' our next section and next definition, and so are worth considering carefully {\em now}.
%
\end{example}
%
\computenote{\boolean{hasmathematica}}
{It is possible to have your computational device crank out the vector form of the solution set to a linear system of equations.}
{
\ifthenelse{\boolean{hasmathematica}}{\computedevicetopic{MMA}{Mathematica}{mathematica}{VFSS}{Vector Form of Solution Set}{vector form of solutions}}{\relax}
}{
\ifthenelse{\boolean{hasmathematica}}{\quad\acronymref{computation}{VFSS.MMA}}{\relax}
}
%
%  End  ss.tex
