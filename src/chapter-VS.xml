<?xml version="1.0" encoding="UTF-8" ?>

<chapter acro="VS" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns="">
<title>Vector Spaces</title>

<introduction>
<p>We now have a computational toolkit in place and so we can begin our study of linear algebra in a more theoretical style.</p>

<p>Linear algebra is the study of two fundamental objects, vector spaces and linear transformations (see <acroref type="chapter" acro="LT" />).  This chapter will focus on the former.  The power of mathematics is often derived from generalizing many different situations into one abstract formulation, and that is exactly what we will be doing throughout this chapter.</p>

</introduction>

<xi:include href="./section-VS.xml" />
<xi:include href="./section-S.xml" />
<xi:include href="./section-LISS.xml" />
<xi:include href="./section-B.xml" />
<xi:include href="./section-D.xml" />
<xi:include href="./section-PD.xml" />
<annotatedacronyms>
<!-- %%%%%%%%%% -->
<!-- % -->
<!-- %  Annotated Acronyms VS -->
<!-- %  Vector Spaces -->
<!-- % -->
<!-- %%%%%%%%%% -->
<annoacro type="definition" acro="VS">
<p>
The most fundamental object in linear algebra is a vector space.  Or else the most fundamental object is a vector, and a vector space is important because it is a collection of vectors.  Either way, <acroref type="definition" acro="VS" /> is critical.  All of our remaining theorems that assume we are working with a vector space can trace their lineage back to this definition.
</p>
</annoacro>
<annoacro type="theorem" acro="TSS">
<p>
Checking all ten properties of a vector space (<acroref type="definition" acro="VS" />) can get tedious.  But if you have a subset of a <em>known</em> vector space, then <acroref type="theorem" acro="TSS" /> considerably shortens the verification.  Also, proofs of closure (the last two conditions in <acroref type="theorem" acro="TSS" />) are a good way to practice a common style of proof.
</p>
</annoacro>
<annoacro type="theorem" acro="VRRB">
<p>
The proof of uniqueness in this theorem is a very typical employment of the hypothesis of linear independence.  But that's not why we mention it here.  This theorem is critical to our first section about representations, <acroref type="section" acro="VR" />, via <acroref type="definition" acro="VR" />.
</p>
</annoacro>
<annoacro type="theorem" acro="CNMB">
<p>
Having just defined a basis (<acroref type="definition" acro="B" />) we discover that the columns of a nonsingular matrix form a basis of $\complex{m}$.  Much of what we know about nonsingular matrices is either contained in this statement, or much more evident because of it.
</p>
</annoacro>
<annoacro type="theorem" acro="SSLD">
<p>
This theorem is a key juncture in our development of linear algebra.  You have probably already realized how useful <acroref type="theorem" acro="G" /> is.  All four parts of <acroref type="theorem" acro="G" /> have proofs that finish with an application of <acroref type="theorem" acro="SSLD" />.
</p>
</annoacro>
<annoacro type="theorem" acro="RPNC">
<p>
This simple relationship between the rank, nullity and number of columns of a matrix might be surprising.  But in simplicity comes power, as this theorem can be very useful.  It will be generalized in the very last theorem of <acroref type="chapter" acro="LT" />, <acroref type="theorem" acro="RPNDD" />.
</p>
</annoacro>
<annoacro type="theorem" acro="G">
<p>
A whimsical title, but the intent is to make sure you don't miss this one.  Much of the interaction between bases, dimension, linear independence and spanning is captured in this theorem.
</p>
</annoacro>
<annoacro type="theorem" acro="RMRT">
<p>
This one is a real surprise.  Why should a matrix, and its transpose, both row-reduce to the same number of non-zero rows?
</p>
</annoacro>
<!--  End VS.tex annotated acronyms -->
</annotatedacronyms>
</chapter>