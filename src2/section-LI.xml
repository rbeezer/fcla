<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the book                -->
<!--                                              -->
<!--      A First Course in Linear Algebra        -->
<!--                                              -->
<!-- Copyright (C) 2004-2017  Robert A. Beezer    -->
<!-- See the file COPYING for copying conditions. -->

<section xml:id="section-LI" acro="LI">
    <title>Linear Independence</title>
    <introduction>
        <p><term>Linear independence</term> is one of the most fundamental conceptual ideas in linear algebra, along with the notion of a span.  So this section, and the subsequent <xref ref="section-LDS" acro="LDS"/>, will explore this new idea.</p>
    </introduction>
    <subsection xml:id="subsection-LI-LISV" acro="LISV">
        <title>Linearly Independent Sets of Vectors</title>
        <p><xref ref="theorem-SLSLC" acro="SLSLC"/> tells us that a solution to a homogeneous system of equations is a linear combination of the columns of the coefficient matrix that equals the zero vector.  We used just this situation to our advantage (twice!) in <xref ref="example-SCAD" acro="SCAD"/> where we reduced the set of vectors used in a span construction from four down to two, by declaring certain vectors as surplus.  The next two definitions will allow us to formalize this situation.</p>
        <!--   Any changes here should be carried forward to Definition RLD. -->
        <definition xml:id="definition-RLDCV" acro="RLDCV">
            <title>Relation of Linear Dependence for Column Vectors</title>
            <idx>relation of linear dependence</idx>
            <statement>
                <p>Given a set of vectors <m>S=\set{\vectorlist{u}{n}}</m>, a true statement of the form<me>\lincombo{\alpha}{u}{n}=\zerovector</me>is a <term>relation of linear dependence</term> on <m>S</m>.  If this statement is formed in a trivial fashion, <ie/> <m>\alpha_i=0</m>, <m>1\leq i\leq n</m>, then we say it is the <term>trivial relation of linear dependence</term> on <m>S</m>.</p>
            </statement>
        </definition>
        <!--   Any changes here should be carried forward to Definition LI. -->
        <definition xml:id="definition-LICV" acro="LICV">
            <title>Linear Independence of Column Vectors</title>
            <idx>linear independence</idx>
            <statement>
                <p>The set of vectors <m>S=\set{\vectorlist{u}{n}}</m> is <term>linearly dependent</term> if there is a relation of linear dependence on <m>S</m> that is not trivial.  In the case where the <em>only</em> relation of linear dependence on <m>S</m> is the trivial one, then <m>S</m> is a <term>linearly independent</term> set of vectors.</p>
            </statement>
        </definition>
        <p>Notice that a relation of linear dependence is an <em>equation</em>.  Though most of it is a linear combination, it is not a linear combination (that would be a vector).  Linear independence is a property of a <em>set</em> of vectors.  It is easy to take a set of vectors, and an equal number of scalars, <em>all zero</em>, and form a linear combination that equals the zero vector.  When the easy way is the <em>only</em> way, then we say the set is linearly independent.  Here are a couple of examples.</p>
        <example xml:id="example-LDS" acro="LDS">
            <idx>linearly dependent set</idx>
            <title>Linearly dependent set in <m>\complex{5}</m></title>
            <p>Consider the set of <m>n=4</m> vectors from <m>\complex{5}</m><me>S=\set{
            \colvector{2\\-1\\3\\1\\2},\,
            \colvector{1\\2\\-1\\5\\2},\,
            \colvector{2\\1\\-3\\6\\1},\,
            \colvector{-6\\7\\-1\\0\\1}
            }</me>.</p>
            <p>To determine linear independence we first form a relation of linear dependence,<me>\alpha_1\colvector{2\\-1\\3\\1\\2}+
            \alpha_2\colvector{1\\2\\-1\\5\\2}+
            \alpha_3\colvector{2\\1\\-3\\6\\1}+
            \alpha_4\colvector{-6\\7\\-1\\0\\1}
            =\zerovector</me>.</p>
            <p>We know that <m>\alpha_1=\alpha_2=\alpha_3=\alpha_4=0</m> is a solution to this equation, but that is of no interest whatsoever.  That is <em>always</em> the case, no matter what four vectors we might have chosen.  We are curious to know if there are other, nontrivial, solutions.  <xref ref="theorem-SLSLC" acro="SLSLC"/> tells us that we can find such solutions as solutions to the homogeneous system <m>\homosystem{A}</m> where the coefficient matrix has these four vectors as columns, which we then row-reduce<me>A=
            \begin{bmatrix}
            2&amp;1&amp;2&amp;-6\\
            -1&amp;2&amp;1&amp;7\\
            3&amp;-1&amp;-3&amp;-1\\
            1&amp;5&amp;6&amp;0\\
            2&amp;2&amp;1&amp;1
            \end{bmatrix}
            \rref
            \begin{bmatrix}
            \leading{1}&amp;0&amp;0&amp;-2\\
            0&amp;\leading{1}&amp;0&amp;4\\
            0&amp;0&amp;\leading{1}&amp;-3\\
            0&amp;0&amp;0&amp;0\\
            0&amp;0&amp;0&amp;0
            \end{bmatrix}</me>.</p>
            <p>We could solve this homogeneous system completely, but for this example all we need is one nontrivial solution.  Setting the lone free variable to any nonzero value, such as <m>x_4=1</m>, yields the nontrivial solution <m>\vect{x}</m> which we use to complete our application of <xref ref="theorem-SLSLC" acro="SLSLC"/><md>
                <mrow>\vect{x}&amp;=\colvector{2\\-4\\3\\1}
                &amp;
                2\colvector{2\\-1\\3\\1\\2}+
                (-4)\colvector{1\\2\\-1\\5\\2}+
                3\colvector{2\\1\\-3\\6\\1}+
                1\colvector{-6\\7\\-1\\0\\1}
                &amp;=\zerovector</mrow>
            </md>.</p>
            <p>This is a relation of linear dependence on <m>S</m> that is not trivial, so we conclude that <m>S</m> is linearly dependent.</p>
        </example>
        <example xml:id="example-LIS" acro="LIS">
            <idx>linearly independent set</idx>
            <title>Linearly independent set in <m>\complex{5}</m></title>
            <p>Consider the set of <m>n=4</m> vectors from <m>\complex{5}</m>,<me>T=\set{
            \colvector{2\\-1\\3\\1\\2},\,
            \colvector{1\\2\\-1\\5\\2},\,
            \colvector{2\\1\\-3\\6\\1},\,
            \colvector{-6\\7\\-1\\1\\1}
            }</me>.</p>
            <p>To determine linear independence we first form a relation of linear dependence,
            <me>\alpha_1\colvector{2\\-1\\3\\1\\2}+
            \alpha_2\colvector{1\\2\\-1\\5\\2}+
            \alpha_3\colvector{2\\1\\-3\\6\\1}+
            \alpha_4\colvector{-6\\7\\-1\\1\\1}
            =\zerovector</me>.</p>
            <p>We know that <m>\alpha_1=\alpha_2=\alpha_3=\alpha_4=0</m> is a solution to this equation, but that is of no interest whatsoever.  That is <em>always</em> the case, no matter what four vectors we might have chosen.  We are curious to know if there are other, nontrivial, solutions.  <xref ref="theorem-SLSLC" acro="SLSLC"/> tells us that we can find such solutions as solution to the homogeneous system <m>\linearsystem{B}{\zerovector}</m> where the coefficient matrix has these four vectors as columns.  Row-reducing this coefficient matrix yields<md>
                <mrow>B=
                \begin{bmatrix}
                2&amp;1&amp;2&amp;-6\\
                -1&amp;2&amp;1&amp;7\\
                3&amp;-1&amp;-3&amp;-1\\
                1&amp;5&amp;6&amp;1\\
                2&amp;2&amp;1&amp;1
                \end{bmatrix}
                &amp;\rref
                \begin{bmatrix}
                \leading{1}&amp;0&amp;0&amp;0\\
                0&amp;\leading{1}&amp;0&amp;0\\
                0&amp;0&amp;\leading{1}&amp;0\\
                0&amp;0&amp;0&amp;\leading{1}\\
                0&amp;0&amp;0&amp;0
                \end{bmatrix}</mrow>
            </md>.</p>
            <p>From the form of this matrix, we see that there are no free variables, so the solution is unique, and because the system is homogeneous, this unique solution is the trivial solution.  So we now know that there is but one way to combine the four vectors of <m>T</m> into a relation of linear dependence, and that one way is the easy and obvious way.  In this situation we say that the set, <m>T</m>, is linearly independent.</p>
        </example>
        <p><xref ref="example-LDS" acro="LDS"/> and <xref ref="example-LIS" acro="LIS"/> relied on solving a homogeneous system of equations to determine linear independence.  We can codify this process in a time-saving theorem.</p>
        <theorem xml:id="theorem-LIVHS" acro="LIVHS">
            <title>Linearly Independent Vectors and Homogeneous Systems</title>
            <idx><h>homogeneous systems</h><h>linear independence</h></idx>
            <idx><h>linear independence</h><h>homogeneous systems</h></idx>
            <statement>
                <p>Suppose that <m>S=\set{\vectorlist{v}{n}}\subseteq\complex{m}</m> is a set of vectors and <m>A</m> is the <m>m\times n</m> matrix whose columns are the vectors in <m>S</m>.  Then <m>S</m> is a linearly independent set if and only if the homogeneous system <m>\homosystem{A}</m> has a unique solution.</p>
            </statement>
            <proof>
                <case direction="backward">
                    <p>Suppose that <m>\homosystem{A}</m> has a unique solution.  Since it is a homogeneous system, this solution must be the trivial solution <m>\vect{x}=\zerovector</m>.  By <xref ref="theorem-SLSLC" acro="SLSLC"/>, this means that the only relation of linear dependence on <m>S</m> is the trivial one.  So <m>S</m> is linearly independent.</p>
                </case>
                <case direction="forward">
                    <p>We will prove the contrapositive.  Suppose that <m>\linearsystem{A}{\zerovector}</m> does not have a unique solution.  Since it is a homogeneous system, it is consistent (<xref ref="theorem-HSC" acro="HSC"/>), and so must have infinitely many solutions (<xref ref="theorem-PSSLS" acro="PSSLS"/>).  One of these infinitely many solutions must be nontrivial (in fact, almost all of them are), so choose one.  By <xref ref="theorem-SLSLC" acro="SLSLC"/> this nontrivial solution will give a nontrivial relation of linear dependence on <m>S</m>, so we can conclude that <m>S</m> is a linearly dependent set.</p>
                </case>
            </proof>
        </theorem>
        <p>Since <xref ref="theorem-LIVHS" acro="LIVHS"/> is an equivalence, we can use it to determine the linear independence or dependence of any set of column vectors, just by creating a matrix and analyzing the row-reduced form.  Let us illustrate this with two more examples.</p>
        <example xml:id="example-LIHS" acro="LIHS">
            <title>Linearly independent, homogeneous system</title>
            <idx><h>linearly independent</h><h>via homogeneous system</h></idx>
            <p>Is the set of vectors<me>S=\set{
            \colvector{2\\-1\\3\\4\\2},\,
            \colvector{6\\2\\-1\\3\\4},\,
            \colvector{4\\3\\-4\\5\\1}
            }</me>linearly independent or linearly dependent?</p>
            <p><xref ref="theorem-LIVHS" acro="LIVHS"/> suggests we study the matrix, <m>A</m>, whose columns are the vectors in <m>S</m>.  Specifically, we are interested in the size of the solution set for the homogeneous system <m>\homosystem{A}</m>, so we row-reduce <m>A</m>.<me>A=
            \begin{bmatrix}
            2 &amp; 6 &amp; 4\\
            -1 &amp; 2 &amp; 3\\
            3 &amp; -1 &amp; -4\\
            4 &amp; 3 &amp; 5\\
            2 &amp; 4 &amp; 1
            \end{bmatrix}
            \rref
            \begin{bmatrix}
            \leading{1} &amp; 0 &amp; 0\\
            0 &amp; \leading{1} &amp; 0\\
            0 &amp; 0 &amp; \leading{1}\\
            0 &amp; 0 &amp; 0\\
            0 &amp; 0 &amp; 0
            \end{bmatrix}</me></p>
            <p>Now, <m>r=3</m>, so there are <m>n-r=3-3=0</m> free variables and we see that <m>\homosystem{A}</m> has a unique solution  (<xref ref="theorem-HSC" acro="HSC"/>, <xref ref="theorem-FVCS" acro="FVCS"/>).  By <xref ref="theorem-LIVHS" acro="LIVHS"/>, the set <m>S</m> is linearly independent.</p>
        </example>
        <example xml:id="example-LDHS" acro="LDHS">
            <title>Linearly dependent, homogeneous system</title>
            <idx><h>linearly dependent</h><h>via homogeneous system</h></idx>
            <p>Is the set of vectors<me>S=\set{
            \colvector{2\\-1\\3\\4\\2},\,
            \colvector{6\\2\\-1\\3\\4},\,
            \colvector{4\\3\\-4\\-1\\2}
            }</me>linearly independent or linearly dependent?</p>
            <p><xref ref="theorem-LIVHS" acro="LIVHS"/> suggests we study the matrix, <m>A</m>, whose columns are the vectors in <m>S</m>.  Specifically, we are interested in the size of the solution set for the homogeneous system <m>\homosystem{A}</m>, so we row-reduce <m>A</m>.<me>A=
            \begin{bmatrix}
            2 &amp; 6 &amp; 4\\
            -1 &amp; 2 &amp; 3\\
            3 &amp; -1 &amp; -4\\
            4 &amp; 3 &amp; -1\\
            2 &amp; 4 &amp; 2
            \end{bmatrix}
            \rref
            \begin{bmatrix}
            \leading{1} &amp; 0 &amp; -1\\
            0 &amp; \leading{1} &amp; 1\\
            0 &amp; 0 &amp; 0\\
            0 &amp; 0 &amp; 0\\
            0 &amp; 0 &amp; 0
            \end{bmatrix}</me></p>
            <p>Now, <m>r=2</m>, so there are <m>n-r=3-2=1</m> free variables and we see that <m>\homosystem{A}</m> has infinitely many solutions (<xref ref="theorem-HSC" acro="HSC"/>, <xref ref="theorem-FVCS" acro="FVCS"/>).  By <xref ref="theorem-LIVHS" acro="LIVHS"/>, the set <m>S</m> is linearly dependent.</p>
        </example>
        <p>As an equivalence, <xref ref="theorem-LIVHS" acro="LIVHS"/> gives us a straightforward way to determine if a set of vectors is linearly independent or dependent.</p>
        <p>Review <xref ref="example-LIHS" acro="LIHS"/> and <xref ref="example-LDHS" acro="LDHS"/>.  They are very similar, differing only in the last two slots of the third vector.  This resulted in slightly different matrices when row-reduced, and slightly different values of <m>r</m>, the number of nonzero rows.  Notice, too, that we are less interested in the actual solution set, and more interested in its form or size.  These observations allow us to make a slight improvement in <xref ref="theorem-LIVHS" acro="LIVHS"/>.</p>
        <theorem xml:id="theorem-LIVRN" acro="LIVRN">
            <title>Linearly Independent Vectors, <m>r</m> and <m>n</m></title>
            <idx><h>linear independence</h><h><m>r</m> and <m>n</m></h></idx>
            <statement>
                <p>Suppose that <m>S=\set{\vectorlist{v}{n}}\subseteq\complex{m}</m> is a set of vectors and <m>A</m> is the <m>m\times n</m> matrix whose columns are the vectors in <m>S</m>.   Let <m>B</m> be a matrix in reduced row-echelon form that is row-equivalent to <m>A</m> and let <m>r</m> denote the number of pivot columns in <m>B</m>.  Then <m>S</m> is linearly independent if and only if <m>n=r</m>.</p>
            </statement>
            <proof>
                <p><xref ref="theorem-LIVHS" acro="LIVHS"/> says the linear independence of <m>S</m> is equivalent to the homogeneous linear system <m>\homosystem{A}</m> having a unique solution.  Since <m>\homosystem{A}</m> is consistent (<xref ref="theorem-HSC" acro="HSC"/>) we can apply <xref ref="theorem-CSRN" acro="CSRN"/> to see that the solution is unique exactly when <m>n=r</m>.</p>
            </proof>
        </theorem>
        <p>So now here is an example of the most straightforward way to determine if a set of column vectors is linearly independent or linearly dependent.  While this method can be quick and easy, do not forget the logical progression from the definition of linear independence through homogeneous system of equations which makes it possible.</p>
        <example xml:id="example-LDRN" acro="LDRN">
            <idx>
                <h>linearly dependent</h>
                <h><m>r</m> and <m>n</m></h>
            </idx>
            <title>Linearly dependent, <m>r</m> and <m>n</m></title>
            <p>Is the set of vectors<me>S=\set{
            \colvector{2\\-1\\3\\1\\0\\3},\,
            \colvector{9\\-6\\-2\\3\\2\\1},\,
            \colvector{1\\1\\1\\0\\0\\1},\,
            \colvector{-3\\1\\4\\2\\1\\2},\,
            \colvector{6\\-2\\1\\4\\3\\2}
            }</me>linearly independent or linearly dependent?</p>
            <p><xref ref="theorem-LIVRN" acro="LIVRN"/> suggests we place these vectors into a matrix as columns and analyze the row-reduced version of the matrix<me>\begin{bmatrix}
            2 &amp; 9 &amp; 1 &amp; -3 &amp; 6\\
            -1 &amp; -6 &amp; 1 &amp; 1 &amp; -2\\
            3 &amp; -2 &amp; 1 &amp; 4 &amp; 1\\
            1 &amp; 3 &amp; 0 &amp; 2 &amp; 4\\
            0 &amp; 2 &amp; 0 &amp; 1 &amp; 3\\
            3 &amp; 1 &amp; 1 &amp; 2 &amp; 2
            \end{bmatrix}
            \rref
            \begin{bmatrix}
            \leading{1} &amp; 0 &amp; 0 &amp; 0 &amp; -1\\
            0 &amp; \leading{1} &amp; 0 &amp; 0 &amp; 1\\
            0 &amp; 0 &amp; \leading{1} &amp; 0 &amp; 2\\
            0 &amp; 0 &amp; 0 &amp; \leading{1} &amp; 1\\
            0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
            0 &amp; 0 &amp; 0 &amp; 0 &amp; 0
            \end{bmatrix}</me>.</p>
            <p>Now we need only compute that <m>r=4\lt 5=n</m> to recognize, via <xref ref="theorem-LIVRN" acro="LIVRN"/> that <m>S</m> is a linearly dependent set.  Boom!</p>
        </example>
        <example xml:id="example-LLDS" acro="LLDS">
            <title>Large linearly dependent set in <m>\complex{4}</m></title>
            <idx>linearly independent set</idx>
            <p>Consider the set of <m>n=9</m> vectors from <m>\complex{4}</m>,<me>R=\set{
            \colvector{-1\\3\\1\\2},\,
            \colvector{7\\1\\-3\\6},\,
            \colvector{1\\2\\-1\\-2},\,
            \colvector{0\\4\\2\\9},\,
            \colvector{5\\-2\\4\\3},\,
            \colvector{2\\1\\-6\\4},\,
            \colvector{3\\0\\-3\\1},\,
            \colvector{1\\1\\5\\3},\,
            \colvector{-6\\-1\\1\\1}
            }</me>.</p>
            <p>To employ <xref ref="theorem-LIVHS" acro="LIVHS"/>, we form a <m>4\times 9</m> matrix, <m>C</m>, whose columns are the vectors in <m>R</m><me>C=
            \begin{bmatrix}
            -1&amp;7&amp;1&amp;0&amp;5&amp;2&amp;3&amp;1&amp;-6\\
            3&amp;1&amp;2&amp;4&amp;-2&amp;1&amp;0&amp;1&amp;-1\\
            1&amp;-3&amp;-1&amp;2&amp;4&amp;-6&amp;-3&amp;5&amp;1\\
            2&amp;6&amp;-2&amp;9&amp;3&amp;4&amp;1&amp;3&amp;1
            \end{bmatrix}</me>.</p>
            <!-- Can do quicker with LIVRN and counting, but want to use HMVEI -->
            <p>To determine if the homogeneous system <m>\homosystem{C}</m> has a unique solution or not, we would normally row-reduce this matrix.  But in this particular example, we can do better.  <xref ref="theorem-HMVEI" acro="HMVEI"/> tells us that since the system is homogeneous with <m>n=9</m> variables in <m>m=4</m> equations, and <m>n\gt m</m>, there must be infinitely many solutions.  Since there is not a unique solution, <xref ref="theorem-LIVHS" acro="LIVHS"/> says the set is linearly dependent.</p>
        </example>
        <p>The situation in <xref ref="example-LLDS" acro="LLDS"/> is slick enough to warrant formulating as a theorem.</p>
        <!-- See note on Example above -->
        <theorem xml:id="theorem-MVSLD" acro="MVSLD">
            <title>More Vectors than Size implies Linear Dependence</title>
            <idx><h>linear dependence</h><h>more vectors than size</h></idx>
            <statement>
                <p>Suppose that <m>S=\set{\vectorlist{u}{n}}\subseteq\complex{m}</m> and <m>n\gt m</m>. Then <m>S</m> is a linearly dependent set.</p>
            </statement>
            <proof>
                <p>Form the <m>m\times n</m> matrix <m>A</m> whose columns are <m>\vect{u}_i</m>, <m>1\leq i\leq n</m>.  Consider the homogeneous system <m>\homosystem{A}</m>.  By <xref ref="theorem-HMVEI" acro="HMVEI"/> this system has infinitely many solutions.  Since the system does not have a unique solution, <xref ref="theorem-LIVHS" acro="LIVHS"/> says the columns of <m>A</m> form a linearly dependent set, as desired.</p>
            </proof>
        </theorem>
        <computation xml:id="sage-LI" acro="LI">
            <title>Linear Independence</title>
            <idx>linear independence</idx>
            <p>We can use the Sage tools we already have, along with <xref ref="theorem-LIVRN" acro="LIVRN"/>, to determine if sets are linearly independent.  There is just one hitch <mdash/> Sage has a preference for placing vectors into matrices as <em>rows</em>, rather than as columns.  When printing vectors on the screen, writing them <em>across</em> the screen makes good sense, and there are more mathematical reasons for this choice.  But we have chosen to present introductory linear algebra with an emphasis on the <em>columns</em> of matrices <mdash/> again, for good mathematical reasons.  Fortunately, Sage allows us to build matrices from columns as well as rows.</p>
            <p>Let us redo <xref ref="example-LDHS" acro="LDHS"/>, the determination that a set of three vectors is linearly dependent.  We will enter the vectors, construct a matrix with the vectors as <em>columns</em> via the <c>column_matrix()</c> constructor, and analyze.  Here we go.</p>
            <sage xml:id="sagecell-LI-1">
                <input>
                v1 = vector(QQ, [2, -1,  3,  4, 2])
                v2 = vector(QQ, [6,  2, -1,  3, 4])
                v3 = vector(QQ, [4,  3, -4, -1, 2])
                A = column_matrix([v1, v2, v3])
                A
                </input>
                <output>
                [ 2  6  4]
                [-1  2  3]
                [ 3 -1 -4]
                [ 4  3 -1]
                [ 2  4  2]
                </output>
            </sage>
            <sage xml:id="sagecell-LI-2">
                <input>
                A.ncols() == len(A.pivots())
                </input>
                <output>
                False
                </output>
            </sage>
            <p>Notice that we never explicitly row-reduce <c>A</c>, though this computation must happen behind the scenes when we compute the list of pivot columns.  We do not really care <em>where</em> the pivots are (the actual list), but rather we want to know <em>how many</em> there are, thus we ask about the <em>length</em> of the list with the function <c>len()</c>.  Once we construct the matrix, the analysis is quick.  With <m>n\neq r</m>, <xref ref="theorem-LIVRN" acro="LIVRN"/> tells us the set is linearly dependent.</p>
            <p>Reprising <xref ref="example-LIS" acro="LIS"/> with Sage would be good practice at this point.  Here is an empty compute cell to use.</p>
            <sage xml:id="sagecell-LI-3"/>
            <p>While it is extremely important to understand the approach outlined above, Sage has a convenient tool for working with linear independence. <c>.linear_dependence()</c>  is a method for vector spaces, which we feed in a list of vectors.  The output is again a list of vectors, each one containing the scalars that yield a nontrivial relation of linear dependence on the input vectors.  We will give this method a workout in the next section, but for now we are interested in the case of a linearly independent set.  In this instance, the method will return nothing (an empty list, really).  Not even the all-zero vector is produced, since it is not interesting and definitely is not surprising.</p>
            <p>Again, we will not say anymore about the output of this method until the next section, and do not let its use replace a good conceptual understanding of this section.  We will redo <xref ref="example-LDHS" acro="LDHS"/> again, you try <xref ref="example-LIS" acro="LIS"/> again.  If you are playing along, be sure <c>v1, v2, v3</c> are defined from the code above.</p>
            <sage xml:id="sagecell-LI-4">
                <input>
                L = [v1, v2, v3]
                V = QQ^5
                V.linear_dependence(L) == []
                </input>
                <output>
                False
                </output>
            </sage>
            <p>The only comment to make here is that we need to create the vector space <c>QQ^5</c> since <c>.linear_dependence()</c> is a method of vector spaces.  <xref ref="example-LIS" acro="LIS"/> should proceed similarly, though being a linearly independent set, the comparison with the empty list should yield <c>True</c>.</p>
        </computation>
    </subsection>
    <subsection xml:id="subsection-LI-LINM" acro="LINM">
        <title>Linear Independence and Nonsingular Matrices</title>
        <p>We will now specialize to sets of <m>n</m> vectors from <m>\complex{n}</m>.  This will put <xref ref="theorem-MVSLD" acro="MVSLD"/> off-limits, while <xref ref="theorem-LIVHS" acro="LIVHS"/> will involve square matrices.  Let us begin by contrasting Archetype<nbsp/><xref ref="archetype-A" acro="A" text="global"/> and Archetype<nbsp/><xref ref="archetype-B" acro="B" text="global"/>.</p>
        <example xml:id="example-LDCAA" acro="LDCAA">
            <title>Linearly dependent columns in Archetype A</title>
            <idx><h>linearly dependent columns</h><h>Archetype A</h></idx>
            <idx><h>Archetype A</h><h>linearly dependent columns</h></idx>
            <!-- Archetype A, Part purematrix -->
            <p>Archetype<nbsp/><xref ref="archetype-A" acro="A" text="global"/> is a system of linear equations with coefficient matrix,<me>A=\begin{bmatrix}
            1 &amp; -1 &amp; 2\\
            2 &amp; 1 &amp; 1\\
            1 &amp; 1 &amp; 0
            \end{bmatrix}
            </me>.</p>
            <p>Do the columns of this matrix form a linearly independent or dependent set?  By <xref ref="example-S" acro="S"/> we know that <m>A</m> is singular.  According to the definition of nonsingular matrices, <xref ref="definition-NM" acro="NM"/>, the homogeneous system <m>\homosystem{A}</m> has infinitely many solutions.  So by <xref ref="theorem-LIVHS" acro="LIVHS"/>, the columns of <m>A</m> form a linearly dependent set.</p>
        </example>
        <example xml:id="example-LICAB" acro="LICAB">
            <title>Linearly independent columns in Archetype B</title>
            <idx><h>linearly independent columns</h> <h>Archetype B</h></idx>
            <idx><h>Archetype B</h><h>linearly independent columns</h></idx>
            <!-- Archetype B, Part purematrix -->
            <p>Archetype<nbsp/><xref ref="archetype-B" acro="B" text="global"/> is a system of linear equations with coefficient matrix,<me>B=\begin{bmatrix}
            -7&amp;-6&amp;-12\\
             5&amp;5&amp;7\\
             1&amp;0&amp;4
            \end{bmatrix}
            </me>.</p>
            <p>Do the columns of this matrix form a linearly independent or dependent set?  By <xref ref="example-NM" acro="NM"/> we know that <m>B</m> is nonsingular.  According to the definition of nonsingular matrices, <xref ref="definition-NM" acro="NM"/>, the homogeneous system <m>\homosystem{A}</m> has a unique solution.  So by <xref ref="theorem-LIVHS" acro="LIVHS"/>, the columns of <m>B</m> form a linearly independent set.</p>
        </example>
        <p>That Archetype<nbsp/><xref ref="archetype-A" acro="A" text="global"/> and Archetype<nbsp/><xref ref="archetype-B" acro="B" text="global"/> have opposite properties for the columns of their coefficient matrices is no accident.  Here is the theorem, and then we will update our equivalences for nonsingular matrices, <xref ref="theorem-NME1" acro="NME1"/>.</p>
        <theorem xml:id="theorem-NMLIC" acro="NMLIC">
            <title>Nonsingular Matrices have Linearly Independent Columns</title>
            <idx><h>nonsingular matrices</h><h>linearly independent columns</h></idx>
            <statement>
                <p>Suppose that <m>A</m> is a square matrix.  Then <m>A</m> is nonsingular if and only if the columns of <m>A</m> form a linearly independent set.</p>
            </statement>
            <proof>
                <p>This is a proof where we can chain together equivalences, rather than proving the two halves separately.</p>
                <p><md>
                    <mrow>A\text{ nonsingular}&amp;\iff\homosystem{A}\text{ has a unique solution}&amp;&amp;
                        <xref ref="definition-NM" acro="NM"/></mrow>
                    <mrow>&amp;\iff\text{columns of }A\text{ are linearly independent}&amp;&amp;
                        <xref ref="theorem-LIVHS" acro="LIVHS"/></mrow>
                </md></p>
            </proof>
        </theorem>
        <p>Here is the update to <xref ref="theorem-NME1" acro="NME1"/>.</p>
        <theorem xml:id="theorem-NME2" acro="NME2">
            <title>Nonsingular Matrix Equivalences, Round 2</title>
            <idx><h>nonsingular matrix</h><h>equivalences</h></idx>
            <statement>
                <p>Suppose that <m>A</m> is a square matrix.  The following are equivalent.<ol>
                    <li><m>A</m> is nonsingular.</li>
                    <li><m>A</m> row-reduces to the identity matrix.</li>
                    <li>The null space of <m>A</m> contains only the zero vector, <m>\nsp{A}=\set{\zerovector}</m>.</li>
                    <li>The linear system <m>\linearsystem{A}{\vect{b}}</m> has a unique solution for every possible choice of <m>\vect{b}</m>.</li>
                    <li>The columns of <m>A</m> form a linearly independent set.</li>
                </ol></p>
            </statement>
            <proof>
                <p><xref ref="theorem-NMLIC" acro="NMLIC"/> is yet another equivalence for a nonsingular matrix, so we can add it to the list in <xref ref="theorem-NME1" acro="NME1"/>.</p>
            </proof>
        </theorem>
        <computation xml:id="sage-NME2" acro="NME2">
            <title>Nonsingular Matrix Equivalences, Round 2</title>
            <idx>nonsingular matrices, round 2</idx>
            <p>As will be our habit, we can illustrate properties of nonsingular matrices with random square matrices.  Review Sage<nbsp/><xref ref="sage-NME1" acro="NME1" text="global"/> for a refresher on generating these matrices.  Here we will illustrate the fifth condition, half of <xref ref="theorem-NMLIC" acro="NMLIC"/>.</p>
            <sage xml:id="sagecell-NME2-1">
                <input>
                n = 6
                A = random_matrix(QQ, n, algorithm='unimodular')
                A                                  # random
                </input>
                                <output>
                [   2    7   37  -79  268   44]
                [  -1   -3  -16   33 -110  -16]
                [   1    1    7  -14   44    5]
                [   0   -3  -15   34 -118  -23]
                [   2    6   33  -68  227   34]
                [  -1   -3  -16   35 -120  -21]
                </output>
            </sage>
            <sage xml:id="sagecell-NME2-2">
                <input>
                V = QQ^n
                V.linear_dependence(A.columns()) == []
                </input>
                <output>
                True
                </output>
            </sage>
            <p>You should always get an empty list (<c>[]</c>) as the result of the second compute cell, no matter which random (unimodular) matrix you produce prior.  Note that the list of vectors created using <c>.columns()</c> is exactly the list we want to feed to <c>.linear_dependence()</c>.</p>
        </computation>
    </subsection>
    <subsection xml:id="subsection-LI-NSSLI" acro="NSSLI">
        <title>Null Spaces, Spans, Linear Independence</title>
        <p>In <xref ref="subsection-SS-SSNS" acro="SS.SSNS"/> we proved <xref ref="theorem-SSNS" acro="SSNS"/> which provided <m>n-r</m> vectors that could be used with the span construction to build the entire null space of a matrix.  As we have hinted in <xref ref="example-SCAD" acro="SCAD"/>, and as we will see again going forward, linearly dependent sets carry redundant vectors with them when used in building a set as a span.  Our aim now is to show that the vectors provided by <xref ref="theorem-SSNS" acro="SSNS"/> form a linearly independent set, so in one sense they are as efficient as possible a way to describe the null space.  Notice that the vectors <m>\vect{z}_j</m>, <m>1\leq j\leq n-r</m> first appear in the vector form of solutions to arbitrary linear systems (<xref ref="theorem-VFSLS" acro="VFSLS"/>).  The exact same vectors appear again in the span construction in the conclusion of <xref ref="theorem-SSNS" acro="SSNS"/>.  Since this second theorem specializes to homogeneous systems the only real difference is that the vector <m>\vect{c}</m> in <xref ref="theorem-VFSLS" acro="VFSLS"/> is the zero vector for a homogeneous system.  Finally, <xref ref="theorem-BNS" acro="BNS"/> will now show that these same vectors are a linearly independent set.  We will set the stage for the proof of this theorem with a moderately large example.  Study the example carefully, as it will make it easier to understand the proof.</p>
        <example xml:id="example-LINSB" acro="LINSB">
            <title>Linear independence of null space basis</title>
            <idx><h>null space</h><h>linearly independent basis</h></idx>
            <p>Suppose that we are interested in the null space of a <m>3\times 7</m> matrix, <m>A</m>, which row-reduces to<me>B=
            \begin{bmatrix}
            \leading{1} &amp; 0 &amp; -2 &amp; 4 &amp; 0 &amp; 3 &amp; 9\\
            0 &amp; \leading{1} &amp; 5 &amp; 6 &amp; 0 &amp; 7 &amp; 1\\
            0 &amp; 0 &amp; 0 &amp; 0 &amp; \leading{1} &amp; 8 &amp; -5
            \end{bmatrix}</me>.</p>
            <p>The set <m>F=\set{3,\,4,\,6,\,7}</m> is the set of indices for our four free variables that would be used in a description of the solution set for the homogeneous system <m>\homosystem{A}</m>.  Applying <xref ref="theorem-SSNS" acro="SSNS"/> we can begin to construct a set of four vectors whose span is the null space of <m>A</m>, a set of vectors we will reference as <m>T</m>.<me>\nsp{A}=\spn{T}=\spn{\set{\vect{z}_1,\,\vect{z}_2,\,\vect{z}_3,\,\vect{z}_4}}=\spn{\set{
            \colvector{ \\ \\1\\0\\ \\0\\0},\,
            \colvector{ \\ \\0\\1\\ \\0\\0},\,
            \colvector{ \\ \\0\\0\\ \\1\\0},\,
            \colvector{ \\ \\0\\0\\ \\0\\1}
            }}</me></p>
            <p>So far, we have constructed as much of these individual vectors as we can, based just on the knowledge of the contents of the set <m>F</m>.  This has allowed us to determine the entries in slots 3, 4, 6 and 7, while we have left slots 1, 2 and 5 blank.  Without doing any more, let us ask if <m>T</m>  is linearly independent?  Begin with a relation of linear dependence on <m>T</m>, and see what we can learn about the scalars,<md>
                <mrow>\zerovector&amp;=\alpha_1\vect{z}_1+\alpha_2\vect{z}_2+\alpha_3\vect{z}_3+\alpha_4\vect{z}_4</mrow><mrow>\colvector{0\\0\\0\\0\\0\\0\\0}
                &amp;=
                \alpha_1\colvector{ \\ \\1\\0\\ \\0\\0}+
                \alpha_2\colvector{ \\ \\0\\1\\ \\0\\0}+
                \alpha_3\colvector{ \\ \\0\\0\\ \\1\\0}+
                \alpha_4\colvector{ \\ \\0\\0\\ \\0\\1}</mrow><mrow>&amp;=
                \colvector{ \\ \\\alpha_1\\0\\ \\0\\0}+
                \colvector{ \\ \\0\\\alpha_2\\ \\0\\0}+
                \colvector{ \\ \\0\\0\\ \\\alpha_3\\0}+
                \colvector{ \\ \\0\\0\\ \\0\\\alpha_4}
                =
                \colvector{ \\ \\\alpha_1\\\alpha_2\\ \\\alpha_3\\\alpha_4}</mrow>
            </md></p>
            <p>Applying <xref ref="definition-CVE" acro="CVE"/> to the two ends of this chain of equalities, we see that <m>\alpha_1=\alpha_2=\alpha_3=\alpha_4=0</m>.  So the only relation of linear dependence on the set <m>T</m> is a trivial one.  By <xref ref="definition-LICV" acro="LICV"/> the set <m>T</m> is linearly independent.  The important feature of this example is how the <q>pattern of zeros and ones</q> in the four vectors led to the conclusion of linear independence.</p>
        </example>
        <p>The proof of <xref ref="theorem-BNS" acro="BNS"/> is really quite straightforward, and relies on the <q>pattern of zeros and ones</q> that arise in the vectors <m>\vect{z}_i</m>, <m>1\leq i\leq n-r</m> in the entries that arise with the locations of the non-pivot columns.  Play along with <xref ref="example-LINSB" acro="LINSB"/> as you study the proof.   Also, take a look at <xref ref="example-VFSAD" acro="VFSAD"/>, <xref ref="example-VFSAI" acro="VFSAI"/> and <xref ref="example-VFSAL" acro="VFSAL"/>, especially at the conclusion of Step 2 (temporarily ignore the construction of the constant vector, <m>\vect{c}</m>).  This proof is also a good first example of how to prove a conclusion that states a set is linearly independent.</p>
        <theorem xml:id="theorem-BNS" acro="BNS">
            <title>Basis for Null Spaces</title>
            <idx><h>null space</h><h>basis</h></idx>
            <statement>
                <p>Suppose that <m>A</m> is an <m>m\times n</m> matrix, and <m>B</m> is a row-equivalent matrix in reduced row-echelon form with <m>r</m> pivot columns.  Let <m>D=\{d_1,\,d_2,\,d_3,\,\ldots,\,d_r\}</m> and <m>F=\{f_1,\,f_2,\,f_3,\,\ldots,\,f_{n-r}\}</m> be the sets of column indices where <m>B</m> does and does not (respectively) have pivot columns.  Construct the <m>n-r</m> vectors <m>\vect{z}_j</m>, <m>1\leq j\leq n-r</m> of size <m>n</m> as<me>\vectorentry{\vect{z}_j}{i}=
                \begin{cases}
                1&amp;\text{if }i\in F, i=f_j\\
                0&amp;\text{if }i\in F, i\neq f_j\\
                -\matrixentry{B}{k,f_j}&amp;\text{if }i\in D, i=d_k
                \end{cases}</me>.</p>
                <p>Define the set <m>S=\set{\vectorlist{z}{n-r}}</m>.  Then<ol>
                    <li><m>\nsp{A}=\spn{S}</m>.</li>
                    <li><m>S</m> is a linearly independent set.</li>
                </ol></p>
            </statement>
            <proof>
                <p>First, note that when <m>n=r</m>, the set <m>S</m> is empty, and both conclusions are true.  So we assume for the remainder that <m>n\gt r</m>.  (You might review the discussion in <xref ref="solution-SS-T20" />.)</p>
                <p>Notice first that the vectors <m>\vect{z}_j</m>, <m>1\leq j\leq n-r</m> are exactly the same as the <m>n-r</m> vectors defined in <xref ref="theorem-SSNS" acro="SSNS"/>.  Also, the hypotheses of <xref ref="theorem-SSNS" acro="SSNS"/> are the same as the hypotheses of the theorem we are currently proving.  So it is then simply the conclusion of <xref ref="theorem-SSNS" acro="SSNS"/> that tells us that <m>\nsp{A}=\spn{S}</m>.  That was the easy half, but the second part is not much harder.  What is new here is the claim that <m>S</m> is a linearly independent set.</p>
                <p>To prove the linear independence of a set, we need to start with a relation of linear dependence and somehow conclude that the scalars involved <em>must all be zero</em>, <ie/> that the relation of linear dependence only happens in the trivial fashion.  So to establish the linear independence of <m>S</m>, we start with<me>\lincombo{\alpha}{z}{n-r}=\zerovector</me>.</p>
                <p>For each <m>j</m>, <m>1\leq j\leq n-r</m>, consider the equality of the individual entries of the vectors on both sides of this equality in position <m>f_j</m><md>
                    <mrow>0
                    &amp;=\vectorentry{\zerovector}{f_j}</mrow>
                    <mrow>&amp;=\vectorentry{\lincombo{\alpha}{z}{n-r}}{f_j}&amp;&amp;
                        <xref ref="definition-CVE" acro="CVE"/></mrow>
                    <mrow>&amp;=
                    \vectorentry{\alpha_1\vect{z}_1}{f_j}+
                    \vectorentry{\alpha_2\vect{z}_2}{f_j}+
                    \vectorentry{\alpha_3\vect{z}_3}{f_j}+
                    \cdots+
                    \vectorentry{\alpha_{n-r}\vect{z}_{n-r}}{f_j}&amp;&amp;
                        <xref ref="definition-CVA" acro="CVA"/></mrow>
                    <mrow>&amp;=
                    \alpha_1\vectorentry{\vect{z}_1}{f_j}+
                    \alpha_2\vectorentry{\vect{z}_2}{f_j}+
                    \alpha_3\vectorentry{\vect{z}_3}{f_j}+
                    \cdots+</mrow>
                    <mrow>&amp;\quad\quad
                    \alpha_{j-1}\vectorentry{\vect{z}_{j-1}}{f_j}+
                    \alpha_{j}\vectorentry{\vect{z}_j}{f_j}+
                    \alpha_{j+1}\vectorentry{\vect{z}_{j+1}}{f_j}+
                    \cdots+</mrow>
                    <mrow>&amp;\quad\quad
                    \alpha_{n-r}\vectorentry{\vect{z}_{n-r}}{f_j}&amp;&amp;
                        <xref ref="definition-CVSM" acro="CVSM"/></mrow>
                    <mrow>&amp;=\alpha_1(0)+
                    \alpha_2(0)+
                    \alpha_3(0)+
                    \cdots+</mrow>
                    <mrow>&amp;\quad\quad
                    \alpha_{j-1}(0)+
                    \alpha_{j}(1)+
                    \alpha_{j+1}(0)+
                    \cdots+\alpha_{n-r}(0)&amp;&amp;
                        \text{Definition of }\vect{z}_j</mrow>
                    <mrow>&amp;=\alpha_{j}</mrow>
                </md>.</p>
            <p>So for all <m>j</m>, <m>1\leq j\leq n-r</m>, we have <m>\alpha_j=0</m>, which is the conclusion that tells us that the <em>only</em> relation of linear dependence on <m>S=\set{\vectorlist{z}{n-r}}</m> is the trivial one.  Hence, by <xref ref="definition-LICV" acro="LICV"/> the set is linearly independent, as desired.</p>
            </proof>
        </theorem>
        <example xml:id="example-NSLIL" acro="NSLIL">
            <title>Null space spanned by linearly independent set,  Archetype L</title>
            <idx><h>null space span, linearly independent</h><h>Archetype L</h></idx>
            <idx><h>Archetype L</h><h>null space span, linearly independent</h></idx>
            <!-- Archetype L, Part nullspacebasis -->
            <p>In <xref ref="example-VFSAL" acro="VFSAL"/> we previewed <xref ref="theorem-SSNS" acro="SSNS"/> by finding a set of two vectors such that their span was the null space for the matrix in Archetype<nbsp/><xref ref="archetype-L" acro="L" text="global"/>.  Writing the matrix as <m>L</m>, we have<me>\nsp{L}=
            \spn{\set{\colvector{-1\\2\\-2\\1\\0},\,\colvector{2\\-2\\1\\0\\1}}
            }</me>.  Solving the homogeneous system <m>\homosystem{L}</m> resulted in recognizing <m>x_4</m> and <m>x_5</m> as the free variables.  So look in entries 4 and 5 of the two vectors above and notice the pattern of zeros and ones that provides the linear independence of the set.</p>
        </example>
        <computation xml:id="sage-LISS" acro="LISS">
            <title>Linearly Independent Spanning Sets</title>
            <idx>linearly independent spanning sets</idx>
            <p>No new commands here, but instead we can now reveal and explore one of our Sage mysteries.  When we create a null space with <c>.right_kernel(basis='pivot')</c>, or a span with <c>.span()</c>, Sage internally creates a new spanning set for the null space or span.  We see the vectors of these spanning sets as the rows of the <c>Basis matrix</c> or <c>User basis matrix</c> when we print one of these sets.</p>
            <p>But more than a spanning set, these vectors are always a linearly independent set.  The advantages of this will have to wait for more theory, but for now, this may explain why the spanning set changes (and sometimes shrinks).  We can also demonstrate this behavior, by creating the span of a large set of (linearly dependent) vectors.</p>
            <sage xml:id="sagecell-LISS-1">
                <input>
                V = QQ^5
                v1 = vector(QQ, [ 5,   2, -11, -24, -17])
                v2 = vector(QQ, [ 4,  13,  -5,  -4,  13])
                v3 = vector(QQ, [ 8,  29,  -9,  -4,  33])
                v4 = vector(QQ, [ 1,   1,  -2,  -4,  -2])
                v5 = vector(QQ, [-7, -25,   8,   4, -28])
                v6 = vector(QQ, [ 0,  -3,  -1,  -4,  -7])
                v7 = vector(QQ, [-1,   2,   3,   8,   9])
                L = [v1, v2, v3, v4, v5, v6, v7]
                V.linear_dependence(L) == []
                </input>
                <output>
                False
                </output>
            </sage>
            <sage xml:id="sagecell-LISS-2">
                <input>
                W = V.span(L)
                W
                </input>
                <output>
                Vector space of degree 5 and dimension 2 over Rational Field
                Basis matrix:
                [    1     0  -7/3 -16/3 -13/3]
                [    0     1   1/3   4/3   7/3]
                </output>
            </sage>
            <sage xml:id="sagecell-LISS-3">
                <input>
                S = W.basis()
                S
                </input>
                <output>
                [
                (1, 0, -7/3, -16/3, -13/3),
                (0, 1, 1/3, 4/3, 7/3)
                ]
                </output>
            </sage>
            <sage xml:id="sagecell-LISS-4">
                <input>
                X = V.span(S)
                X == W
                </input>
                <output>
                True
                </output>
            </sage>
            <sage xml:id="sagecell-LISS-5">
                <input>
                V.linear_dependence(S) == []
                </input>
                <output>
                True
                </output>
            </sage>
            <p>So the above examines properties of the two vectors, <c>S</c>, that Sage computes for the span <c>W</c> built from the seven vectors in the set <c>L</c>.  We see that the span of <c>S</c> is equal to the span of <c>L</c> via the comparison <c>X == W</c>.  And the smaller set, <c>S</c>, is linearly independent, as promised.  Notice that we do not need to use Sage to know that <c>L</c> is linearly dependent, this is guaranteed by <xref ref="theorem-MVSLD" acro="MVSLD"/>.</p>
        </computation>
    </subsection>
    <exercises xml:id="readingquestions-LI">
        <title>Reading Questions</title>
        <exercise xml:id="reading-LI-1">
            <statement>
                <p>Let  <m>S</m>  be the set of three vectors below.<me>S=\set{\colvector{1\\2\\-1},\,\colvector{3\\-4\\2},\,\colvector{4\\-2\\1}}</me>Is <m>S</m> linearly independent or linearly dependent?  Explain why.</p>
            </statement>
        </exercise>
        <exercise xml:id="reading-LI-2">
            <statement>
                <p> Let <m>S</m> be the set of three vectors below.<me>S=\set{\colvector{1\\-1\\0},\,\colvector{3\\2\\2},\,\colvector{4\\3\\-4}}</me>Is <m>S</m> linearly independent or linearly dependent?  Explain why.</p>
            </statement>
        </exercise>
        <exercise xml:id="reading-LI-3">
            <statement>
                <p>Is the matrix below singular or nonsingular?  Explain your answer using only the final conclusion you reached in the previous question, along with one new theorem.<me>\begin{bmatrix}
                 1  &amp;3  &amp;4\\
                -1  &amp;2  &amp;3\\
                 0  &amp;2 &amp;-4
                 \end{bmatrix}</me></p>
            </statement>
        </exercise>
    </exercises>
    <exercises xml:id="exercises-LI">
        <title>Exercises</title>
        <exercisegroup>
            <introduction>
                <p>Determine if the sets of vectors in Exercises C20<ndash/>C25 are linearly independent or linearly dependent.  When the set is linearly dependent, exhibit a nontrivial relation of linear dependence.</p>
            </introduction>
            <exercise number="C20" xml:id="exercise-LI-C20">
                <statement>
                    <p>
                        <m>\set{\colvector{1\\-2\\1},\,\colvector{2\\-1\\3},\,\colvector{1\\5\\0}}</m>
                    </p>
                </statement>
                <solution xml:id="solution-LI-C20">
                    <p>With three vectors from <m>\complex{3}</m>, we can form a square matrix by making these three vectors the columns of a matrix.  We do so, and row-reduce to obtain<me>\begin{bmatrix}
                    \leading{1} &amp; 0 &amp; 0\\
                    0 &amp; \leading{1} &amp; 0\\
                    0 &amp; 0 &amp; \leading{1}
                    \end{bmatrix}</me>the <m>3\times 3</m> identity matrix.  So by <xref ref="theorem-NME2" acro="NME2"/> the original matrix is nonsingular and its columns are therefore a linearly independent set.</p>
                </solution>
            </exercise>
            <exercise number="C21" xml:id="exercise-LI-C21">
                <statement>
                    <p>
                        <m>\set{\colvector{-1\\2\\4\\2},\,\colvector{3\\3\\-1\\3},\,\colvector{7\\3\\-6\\4}}</m>
                    </p>
                </statement>
                <solution xml:id="solution-LI-C21">
                    <p><xref ref="theorem-LIVRN" acro="LIVRN"/> says we can answer this question by putting theses vectors into a matrix as columns and row-reducing.  Doing this we obtain<me>\begin{bmatrix}
                    \leading{1} &amp; 0 &amp; 0\\
                    0 &amp; \leading{1} &amp; 0\\
                    0 &amp; 0 &amp; \leading{1}\\
                    0 &amp; 0 &amp; 0
                    \end{bmatrix}</me>.  With <m>n=3</m> (3 vectors, 3 columns) and <m>r=3</m> (3 pivot columns) we have <m>n=r</m> and the theorem says the vectors are linearly independent.</p>
                </solution>
            </exercise>
            <exercise number="C22" xml:id="exercise-LI-C22">
                <statement>
                    <p><m>\set{
                    \colvector{-2\\-1\\-1},\,
                    \colvector{1\\0\\-1},\,
                    \colvector{3\\3\\6},\,
                    \colvector{-5\\-4\\-6},\,
                    \colvector{4\\4\\7}
                    }</m></p>
                </statement>
                <solution xml:id="solution-LI-C22">
                    <p>Five vectors from <m>\complex{3}</m>.  <xref ref="theorem-MVSLD" acro="MVSLD"/> says the set is linearly dependent.  Boom.</p>
                </solution>
            </exercise>
            <exercise number="C23" xml:id="exercise-LI-C23">
                <statement>
                    <p><m>\set{
                    \colvector{1\\-2\\2\\5\\3},\,
                    \colvector{3\\3\\1\\2\\-4},\,
                    \colvector{2\\1\\2\\-1\\1},\,
                    \colvector{1\\0\\1\\2\\2}
                    }</m></p>
                </statement>
                <solution xml:id="solution-LI-C23">
                    <p><xref ref="theorem-LIVRN" acro="LIVRN"/> suggests we analyze a matrix whose columns are the vectors of <m>S</m><me>A=\begin{bmatrix}
                    1 &amp; 3 &amp; 2 &amp; 1\\
                    -2 &amp; 3 &amp; 1 &amp; 0\\
                    2 &amp; 1 &amp; 2 &amp; 1\\
                    5 &amp; 2 &amp; -1 &amp; 2\\
                    3 &amp; -4 &amp; 1 &amp; 2
                    \end{bmatrix}</me>.  Row-reducing the matrix <m>A</m> yields<me>\begin{bmatrix}
                    \leading{1} &amp; 0 &amp; 0 &amp; 0\\
                    0 &amp; \leading{1} &amp; 0 &amp; 0\\
                    0 &amp; 0 &amp; \leading{1} &amp; 0\\
                    0 &amp; 0 &amp; 0 &amp; \leading{1}\\
                    0 &amp; 0 &amp; 0 &amp; 0
                    \end{bmatrix}</me>.  We see that <m>r=4=n</m>, where <m>r</m> is the number of nonzero rows and <m>n</m> is the number of columns.  By <xref ref="theorem-LIVRN" acro="LIVRN"/>, the set <m>S</m> is linearly independent.</p>
                </solution>
            </exercise>
            <exercise number="C24" xml:id="exercise-LI-C24">
                <statement>
                    <p><m>\set{
                    \colvector{1\\2\\-1\\0\\1},\,
                    \colvector{3\\2\\-1\\2\\2},\,
                    \colvector{4\\4\\-2\\2\\3},\,
                    \colvector{-1\\2\\-1\\-2\\0}
                    }</m></p>
                </statement>
                <solution xml:id="solution-LI-C24">
                    <p><xref ref="theorem-LIVRN" acro="LIVRN"/> suggests we analyze a matrix whose columns are the vectors from the set<me>A=\begin{bmatrix}
                    1 &amp; 3 &amp; 4 &amp; -1\\
                    2 &amp; 2 &amp; 4 &amp; 2\\
                    -1 &amp; -1 &amp; -2 &amp; -1\\
                    0 &amp; 2 &amp; 2 &amp; -2\\
                    1 &amp; 2 &amp; 3 &amp; 0
                    \end{bmatrix}</me>.  Row-reducing the matrix <m>A</m> yields<me>\begin{bmatrix}
                    \leading{1} &amp; 0 &amp; 1 &amp; 2\\
                    0 &amp; \leading{1} &amp; 1 &amp; -1\\
                    0 &amp; 0 &amp; 0 &amp; 0\\
                    0 &amp; 0 &amp; 0 &amp; 0\\
                    0 &amp; 0 &amp; 0 &amp; 0
                    \end{bmatrix}</me>.  We see that <m>r=2\neq 4=n</m>, where <m>r</m> is the number of nonzero rows and <m>n</m> is the number of columns.  By <xref ref="theorem-LIVRN" acro="LIVRN"/>, the set <m>S</m> is  linearly dependent.</p>
                </solution>
            </exercise>
            <exercise number="C25" xml:id="exercise-LI-C25">
                <statement>
                    <p><m>\set{
                    \colvector{2\\1\\3\\-1\\2},\,
                    \colvector{4\\-2\\1\\3\\2},\,
                    \colvector{10\\-7\\0\\10\\4}
                    }</m></p>
                </statement>
                <solution xml:id="solution-LI-C25">
                    <p><xref ref="theorem-LIVRN" acro="LIVRN"/> suggests we analyze a matrix whose columns are the vectors from the set<me>A=\begin{bmatrix}
                     2 &amp; 4 &amp; 10 \\
                     1 &amp; -2 &amp; -7 \\
                     3 &amp; 1 &amp; 0 \\
                     -1 &amp; 3 &amp; 10 \\
                     2 &amp; 2 &amp; 4
                    \end{bmatrix}</me>.  Row-reducing the matrix <m>A</m> yields<me>\begin{bmatrix}
                     \leading{1} &amp; 0 &amp; -1 \\
                     0 &amp; \leading{1} &amp; 3 \\
                     0 &amp; 0 &amp; 0 \\
                     0 &amp; 0 &amp; 0 \\
                     0 &amp; 0 &amp; 0
                    \end{bmatrix}</me>.  We see that <m>r=2\neq 3=n</m>, where <m>r</m> is the number of nonzero rows and <m>n</m> is the number of columns.  By <xref ref="theorem-LIVRN" acro="LIVRN"/>, the set <m>S</m> is  linearly dependent.</p>
                </solution>
            </exercise>
        </exercisegroup>
        <!-- %  No more computational linear independence -->
        <exercise number="C30" xml:id="exercise-LI-C30">
            <statement>
                <p>For the matrix <m>B</m> below, find a set <m>S</m> that is linearly independent and spans the null space of <m>B</m>, that is, <m>\nsp{B}=\spn{S}</m>.<me>B=
                \begin{bmatrix}
                -3 &amp; 1 &amp; -2 &amp; 7\\
                -1 &amp; 2 &amp; 1 &amp; 4\\
                1 &amp; 1 &amp; 2 &amp; -1
                \end{bmatrix}</me></p>
            </statement>
            <solution xml:id="solution-LI-C30">
                <p>The requested set is described by <xref ref="theorem-BNS" acro="BNS"/>.  It is easiest to find by using the procedure of <xref ref="example-VFSAL" acro="VFSAL"/>.  Begin by row-reducing the matrix, viewing it as the coefficient matrix of a homogeneous system of equations.  We obtain<me>\begin{bmatrix}
                \leading{1} &amp; 0 &amp; 1 &amp; -2\\
                0 &amp; \leading{1} &amp; 1 &amp; 1\\
                0 &amp; 0 &amp; 0 &amp; 0
                \end{bmatrix}</me>.  Now build the vector form of the solutions to this homogeneous system (<xref ref="theorem-VFSLS" acro="VFSLS"/>).  The free variables are <m>x_3</m> and <m>x_4</m>, due to the locations of the non-pivot columns<me>\colvector{x_1\\x_2\\x_3\\x_4}=
                x_3\colvector{-1\\-1\\1\\0}+
                x_4\colvector{2\\-1\\0\\1}</me>.  The desired set <m>S</m> is simply the constant vectors in this expression, and these are the vectors <m>\vect{z}_1</m> and <m>\vect{z}_2</m> described by <xref ref="theorem-BNS" acro="BNS"/>.<me>S=\set{
                \colvector{-1\\-1\\1\\0},\,
                \colvector{2\\-1\\0\\1}
                }</me></p>
            </solution>
        </exercise>
        <exercise number="C31" xml:id="exercise-LI-C31">
            <statement>
                <p>For the matrix <m>A</m> below, find a linearly independent set <m>S</m> so that the null space of <m>A</m> is spanned by <m>S</m>, that is, <m>\nsp{A}=\spn{S}</m>.<me>A=
                \begin{bmatrix}
                 -1 &amp; -2 &amp; 2 &amp; 1 &amp; 5 \\
                 1 &amp; 2 &amp; 1 &amp; 1 &amp; 5 \\
                 3 &amp; 6 &amp; 1 &amp; 2 &amp; 7 \\
                 2 &amp; 4 &amp; 0 &amp; 1 &amp; 2
                \end{bmatrix}</me></p>
            </statement>
            <solution xml:id="solution-LI-C31">
                <p><xref ref="theorem-BNS" acro="BNS"/> provides formulas for <m>n-r</m> vectors that will meet the requirements of this question.  These vectors are the same ones listed in <xref ref="theorem-VFSLS" acro="VFSLS"/> when we solve the homogeneous system <m>\homosystem{A}</m>, whose solution set is the null space (<xref ref="definition-NSM" acro="NSM"/>).</p>
                <p>To apply <xref ref="theorem-BNS" acro="BNS"/> or <xref ref="theorem-VFSLS" acro="VFSLS"/> we first row-reduce the matrix, resulting in<me>B=
                \begin{bmatrix}
                 \leading{1} &amp; 2 &amp; 0 &amp; 0 &amp; 3 \\
                 0 &amp; 0 &amp; \leading{1} &amp; 0 &amp; 6 \\
                 0 &amp; 0 &amp; 0 &amp; \leading{1} &amp; -4 \\
                 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0
                \end{bmatrix}</me>  So we see that <m>n-r=5-3=2</m> and <m>F=\set{2,5}</m>, so the vector form of a generic solution vector is<me>\colvector{x_1\\x_2\\x_3\\x_4\\x_5}
                =
                x_2\colvector{-2\\1\\ 0\\ 0\\0}
                +
                x_5\colvector{-3\\0\\-6 \\4 \\1}</me>.  So we have
                <me>\nsp{A}=\spn{\set{
                \colvector{-2\\1\\ 0\\ 0\\0},\,
                \colvector{-3\\0\\-6 \\4 \\1}
                }}</me>.</p>
            </solution>
        </exercise>
        <exercise number="C32" xml:id="exercise-LI-C32">
            <statement>
                <p>Find a set of column vectors, <m>T</m>, such that (1) the span of <m>T</m> is the null space of <m>B</m>, <m>\spn{T}=\nsp{B}</m> and (2) <m>T</m> is a linearly independent set.<md>
                    <mrow>B&amp;=
                    \begin{bmatrix}
                     2 &amp; 1 &amp; 1 &amp; 1 \\
                     -4 &amp; -3 &amp; 1 &amp; -7 \\
                     1 &amp; 1 &amp; -1 &amp; 3
                    \end{bmatrix}</mrow>
                </md></p>
            </statement>
            <solution xml:id="solution-LI-C32">
                <p>The conclusion of <xref ref="theorem-BNS" acro="BNS"/> gives us everything this question asks for.  We need the reduced row-echelon form of the matrix so we can determine the number of vectors in <m>T</m>, and their entries.<md>
                    <mrow>\begin{bmatrix}
                     2 &amp; 1 &amp; 1 &amp; 1 \\
                     -4 &amp; -3 &amp; 1 &amp; -7 \\
                     1 &amp; 1 &amp; -1 &amp; 3
                    \end{bmatrix}
                    &amp;\rref
                    \begin{bmatrix}
                     \leading{1} &amp; 0 &amp; 2 &amp; -2 \\
                     0 &amp; \leading{1} &amp; -3 &amp; 5 \\
                     0 &amp; 0 &amp; 0 &amp; 0
                    \end{bmatrix}</mrow>
                </md>We can build the set <m>T</m> in immediately via <xref ref="theorem-BNS" acro="BNS"/>, but we will illustrate its construction in two steps.  Since <m>F=\set{3,\,4}</m>, we will have two vectors and can distribute strategically placed ones, and many zeros.  Then we distribute the negatives of the appropriate entries of the non-pivot columns of the reduced row-echelon matrix.<md>
                    <mrow>T&amp;=\set{
                    \colvector{\\\\1\\0},\,
                    \colvector{\\\\0\\1}
                    }
                    &amp;
                    T&amp;=\set{
                    \colvector{-2\\3\\1\\0},\,
                    \colvector{2\\-5\\0\\1}
                    }</mrow>
                </md></p>
            </solution>
        </exercise>
        <exercise number="C33" xml:id="exercise-LI-C33">
            <statement>
                <p>Find  a set <m>S</m> so that <m>S</m> is linearly independent and <m>\nsp{A}=\spn{S}</m>, where <m>\nsp{A}</m> is the null space of the matrix <m>A</m> below.<md>
                    <mrow>A&amp;=
                    \begin{bmatrix}
                    2 &amp; 3 &amp; 3 &amp; 1 &amp; 4 \\
                    1 &amp; 1 &amp; -1 &amp; -1 &amp; -3 \\
                    3 &amp; 2 &amp; -8 &amp; -1 &amp; 1
                    \end{bmatrix}</mrow>
                </md></p>
            </statement>
            <solution xml:id="solution-LI-C33">
                <p>A direct application of <xref ref="theorem-BNS" acro="BNS"/> will provide the desired set.  We require the reduced row-echelon form of <m>A</m>.<md>
                    <mrow>\begin{bmatrix}
                    2 &amp; 3 &amp; 3 &amp; 1 &amp; 4 \\
                    1 &amp; 1 &amp; -1 &amp; -1 &amp; -3 \\
                    3 &amp; 2 &amp; -8 &amp; -1 &amp; 1
                    \end{bmatrix}
                    &amp;\rref
                    \begin{bmatrix}
                    \leading{1} &amp; 0 &amp; -6 &amp; 0 &amp; 3 \\
                    0 &amp; \leading{1} &amp; 5 &amp; 0 &amp; -2 \\
                    0 &amp; 0 &amp; 0 &amp; \leading{1} &amp; 4\end{bmatrix}</mrow>
                </md>  The non-pivot columns have indices <m>F=\set{3,\,5}</m>.  We build the desired set in two steps, first placing the requisite zeros and ones in locations based on <m>F</m>, then placing the negatives of the entries of columns 3 and 5 in the proper locations.  This is all specified in <xref ref="theorem-BNS" acro="BNS"/>.<md>
                    <mrow>S&amp;=
                    \set{
                    \colvector{ \\ \\ 1\\ \\ 0},\,
                    \colvector{ \\ \\ 0\\ \\ 1}
                    }
                    =
                    \set{
                    \colvector{ 6\\ -5\\ 1\\ 0\\ 0},\,
                    \colvector{ -3\\ 2\\ 0\\ -4\\ 1}
                    }</mrow>
                </md></p>
            </solution>
        </exercise>
        <exercise number="C50" xml:id="exercise-LI-C50">
            <statement>
                <p>Consider each archetype that is a system of equations and consider the solutions listed for the homogeneous version of the archetype.  (If only the trivial solution is listed, then assume this is the only solution to the system.)  From the solution set, determine if the columns of the coefficient matrix form a linearly independent or linearly dependent set.  In the case of a linearly dependent set, use one of the sample solutions to provide a nontrivial relation of linear dependence on the set of columns of the coefficient matrix (<xref ref="definition-RLD" acro="RLD"/>).  Indicate when <xref ref="theorem-MVSLD" acro="MVSLD"/> applies and connect this with the number of variables and equations in the system of equations.</p>
                <p>
                Archetype<nbsp/><xref ref="archetype-A" acro="A" text="global"/>,
                Archetype<nbsp/><xref ref="archetype-B" acro="B" text="global"/>,
                Archetype<nbsp/><xref ref="archetype-C" acro="C" text="global"/>,
                Archetype<nbsp/><xref ref="archetype-D" acro="D" text="global"/>/Archetype<nbsp/><xref ref="archetype-E" acro="E" text="global"/>,
                Archetype<nbsp/><xref ref="archetype-F" acro="F" text="global"/>,
                Archetype<nbsp/><xref ref="archetype-G" acro="G" text="global"/>/Archetype<nbsp/><xref ref="archetype-H" acro="H" text="global"/>,
                Archetype<nbsp/><xref ref="archetype-I" acro="I" text="global"/>,
                Archetype<nbsp/><xref ref="archetype-J" acro="J" text="global"/>
                </p>
            </statement>
        </exercise>
        <exercise number="C51" xml:id="exercise-LI-C51">
            <statement>
                <p>For each archetype that is a system of equations consider the homogeneous version. Write elements of the solution set in vector form (<xref ref="theorem-VFSLS" acro="VFSLS"/>) and from this extract the vectors <m>\vect{z}_j</m> described in <xref ref="theorem-BNS" acro="BNS"/>.  These vectors are used in a span construction to describe the null space of the coefficient matrix for each archetype.  What does it mean when we write a null space as <m>\spn{\set{\ }}</m>?</p>
                <p>
                Archetype<nbsp/><xref ref="archetype-A" acro="A" text="global"/>,
                Archetype<nbsp/><xref ref="archetype-B" acro="B" text="global"/>,
                Archetype<nbsp/><xref ref="archetype-C" acro="C" text="global"/>,
                Archetype<nbsp/><xref ref="archetype-D" acro="D" text="global"/>/Archetype<nbsp/><xref ref="archetype-E" acro="E" text="global"/>,
                Archetype<nbsp/><xref ref="archetype-F" acro="F" text="global"/>,
                Archetype<nbsp/><xref ref="archetype-G" acro="G" text="global"/>/Archetype<nbsp/><xref ref="archetype-H" acro="H" text="global"/>,
                Archetype<nbsp/><xref ref="archetype-I" acro="I" text="global"/>,
                Archetype<nbsp/><xref ref="archetype-J" acro="J" text="global"/>
                </p>
            </statement>
        </exercise>
        <exercise number="C52" xml:id="exercise-LI-C52">
            <statement>
                <p>For each archetype that is a system of equations consider the homogeneous version.  Sample solutions are given and a linearly independent spanning set is given for the null space of the coefficient matrix.  Write each of the sample solutions individually as a linear combination of the vectors in the spanning set for the null space of the coefficient matrix.</p>
                <p>
                Archetype<nbsp/><xref ref="archetype-A" acro="A" text="global"/>,
                Archetype<nbsp/><xref ref="archetype-B" acro="B" text="global"/>,
                Archetype<nbsp/><xref ref="archetype-C" acro="C" text="global"/>,
                Archetype<nbsp/><xref ref="archetype-D" acro="D" text="global"/>/Archetype<nbsp/><xref ref="archetype-E" acro="E" text="global"/>,
                Archetype<nbsp/><xref ref="archetype-F" acro="F" text="global"/>,
                Archetype<nbsp/><xref ref="archetype-G" acro="G" text="global"/>/Archetype<nbsp/><xref ref="archetype-H" acro="H" text="global"/>,
                Archetype<nbsp/><xref ref="archetype-I" acro="I" text="global"/>,
                Archetype<nbsp/><xref ref="archetype-J" acro="J" text="global"/>
                </p>
            </statement>
        </exercise>
        <exercise number="C60" xml:id="exercise-LI-C60">
            <statement>
                <p>For the matrix <m>A</m> below, find a set of vectors <m>S</m> so that (1) <m>S</m> is linearly independent, and (2) the span of <m>S</m> equals the null space of <m>A</m>, <m>\spn{S}=\nsp{A}</m>.  (See <xref ref="exercise-SS-C60" acro="SS.C60"/>.)<me>A=
                \begin{bmatrix}
                1 &amp; 1 &amp; 6 &amp; -8\\
                1 &amp; -2 &amp; 0 &amp; 1\\
                -2 &amp; 1 &amp; -6 &amp; 7
                \end{bmatrix}</me></p>
            </statement>
            <solution xml:id="solution-LI-C60">
                <p><xref ref="theorem-BNS" acro="BNS"/> says that if we find the vector form of the solutions to the homogeneous system <m>\homosystem{A}</m>, then the fixed vectors (one per free variable) will have the desired properties.  Row-reduce <m>A</m>, viewing it as the augmented matrix of a homogeneous system with an invisible columns of zeros as the last column.<me>\begin{bmatrix}
                \leading{1} &amp; 0 &amp; 4 &amp; -5\\
                0 &amp; \leading{1} &amp; 2 &amp; -3\\
                0 &amp; 0 &amp; 0 &amp; 0
                \end{bmatrix}</me>  Moving to the vector form of the solutions (<xref ref="theorem-VFSLS" acro="VFSLS"/>), with free variables <m>x_3</m> and <m>x_4</m>, solutions to the consistent system (it is homogeneous, <xref ref="theorem-HSC" acro="HSC"/>) can be expressed as<me>\colvector{x_1\\x_2\\x_3\\x_4}
                =
                x_3\colvector{-4\\-2\\1\\0}+
                x_4\colvector{5\\3\\0\\1}</me>.  Then with <m>S</m> given by<me>S=\set{\colvector{-4\\-2\\1\\0},\,\colvector{5\\3\\0\\1}}</me><xref ref="theorem-BNS" acro="BNS"/> guarantees the set has the desired properties.</p>
            </solution>
        </exercise>
        <exercise number="M20" xml:id="exercise-LI-M20">
            <statement>
                <p>Suppose that <m>S=\set{\vect{v}_1,\,\vect{v}_2,\,\vect{v}_3}</m> is a set of three vectors from <m>\complex{873}</m>.  Prove that the set<md>
                    <mrow>T&amp;=
                    \set{
                    2\vect{v}_1+3\vect{v}_2+\vect{v}_3,\,
                    \vect{v}_1-\vect{v}_2-2\vect{v}_3,\,
                    2\vect{v}_1+\vect{v}_2-\vect{v}_3
                    }</mrow>
                </md>is linearly dependent.</p>
            </statement>
            <solution xml:id="solution-LI-M20">
                <p>By <xref ref="definition-LICV" acro="LICV"/>, we can complete this problem by finding scalars, <m>\alpha_1,\,\alpha_2,\,\alpha_3</m>, not all zero, such that<md>
                    <mrow>\alpha_1\left(2\vect{v}_1+3\vect{v}_2+\vect{v}_3\right)+
                    \alpha_2\left(\vect{v}_1-\vect{v}_2-2\vect{v}_3\right)+
                    \alpha_3\left(2\vect{v}_1+\vect{v}_2-\vect{v}_3\right)
                    &amp;=
                    \zerovector</mrow>
                </md>.  Using various properties in <xref ref="theorem-VSPCV" acro="VSPCV"/>, we can rearrange this vector equation to<md>
                    <mrow>\left(2\alpha_1+\alpha_2+2\alpha_3\right)\vect{v}_1+
                    \left(3\alpha_1-\alpha_2+\alpha_3\right)\vect{v}_2+
                    \left(\alpha_1-2\alpha_2-\alpha_3\right)\vect{v}_3
                    &amp;=
                    \zerovector</mrow>
                </md>.  We can certainly make this vector equation true if we can determine values for the <m>\alpha</m>'s such that<md>
                    <mrow>2\alpha_1+\alpha_2+2\alpha_3&amp;=0</mrow>
                    <mrow>3\alpha_1-\alpha_2+\alpha_3&amp;=0</mrow>
                    <mrow>\alpha_1-2\alpha_2-\alpha_3&amp;=0</mrow>
                </md>Aah, a homogeneous system of equations.  And it has infinitely many nonzero solutions.  By the now familiar techniques, one such solution is <m>\alpha_1=3</m>, <m>\alpha_2=4</m>, <m>\alpha_3=-5</m>, which you can check in the original relation of linear dependence on <m>T</m> above.</p>
                <p>Note that simply writing down the three scalars, and demonstrating that they provide a nontrivial relation of linear dependence on <m>T</m>, could be considered an ironclad solution.  But it would not have been very informative for you if we had only done just that here.  Compare this solution very carefully with <xref ref="solution-LI-M21" acro="LI.M21"/>.</p>
            </solution>
        </exercise>
        <exercise number="M21" xml:id="exercise-LI-M21">
            <statement>
                <p>Suppose that <m>S=\set{\vect{v}_1,\,\vect{v}_2,\,\vect{v}_3}</m> is a linearly independent set of three vectors from <m>\complex{873}</m>.  Prove that the set<md>
                    <mrow>T&amp;=
                    \set{
                    2\vect{v}_1+3\vect{v}_2+\vect{v}_3,\,
                    \vect{v}_1-\vect{v}_2+2\vect{v}_3,\,
                    2\vect{v}_1+\vect{v}_2-\vect{v}_3
                    }</mrow>
                </md>is linearly independent.</p>
            </statement>
            <solution xml:id="solution-LI-M21">
                <p>By <xref ref="definition-LICV" acro="LICV"/> we can complete this problem by proving that if we assume that<md>
                    <mrow>\alpha_1\left(2\vect{v}_1+3\vect{v}_2+\vect{v}_3\right)+
                    \alpha_2\left(\vect{v}_1-\vect{v}_2+2\vect{v}_3\right)+
                    \alpha_3\left(2\vect{v}_1+\vect{v}_2-\vect{v}_3\right)
                    &amp;=
                    \zerovector</mrow>
                </md>then we <em>must</em> conclude that <m>\alpha_1=\alpha_2=\alpha_3=0</m>.  Using various properties in <xref ref="theorem-VSPCV" acro="VSPCV"/>, we can rearrange this vector equation to<md>
                    <mrow>\left(2\alpha_1+\alpha_2+2\alpha_3\right)\vect{v}_1+
                    \left(3\alpha_1-\alpha_2+\alpha_3\right)\vect{v}_2+
                    \left(\alpha_1+2\alpha_2-\alpha_3\right)\vect{v}_3
                    &amp;=
                    \zerovector</mrow>
                </md>.Because the set <m>S=\set{\vect{v}_1,\,\vect{v}_2,\,\vect{v}_3}</m> was assumed to be linearly independent, by <xref ref="definition-LICV" acro="LICV"/> we <em>must</em> conclude that<md>
                    <mrow>2\alpha_1+\alpha_2+2\alpha_3&amp;=0</mrow>
                    <mrow>3\alpha_1-\alpha_2+\alpha_3&amp;=0</mrow>
                    <mrow>\alpha_1+2\alpha_2-\alpha_3&amp;=0</mrow>
                </md>Aah, a homogeneous system of equations.  And it has a unique solution, the trivial solution.  So, <m>\alpha_1=\alpha_2=\alpha_3=0</m>, as desired.  It is an inescapable conclusion from our assumption of a relation of linear dependence above.  Done.</p>
                <p>Compare this solution very carefully with <xref ref="solution-LI-M20" acro="LI.M20"/>, noting especially how this problem required (and used) the hypothesis that the original set be linearly independent, and how this solution feels more like a proof, while the previous problem could be solved with a fairly simple demonstration of any nontrivial relation of linear dependence.</p>
            </solution>
        </exercise>
        <exercise number="M50" xml:id="exercise-LI-M50">
            <statement>
                <p>Consider the set of vectors from <m>\complex{3}</m>, <m>W</m>, given below.  Find a set <m>T</m> that contains three vectors from <m>W</m> and such that <m>W=\spn{T}</m>.<me>W=\spn{\set{\vect{v}_1,\,\vect{v}_2,\,\vect{v}_3,\,\vect{v}_4,\,\vect{v}_5}}=\spn{\set{\colvector{2\\1\\1},\,\colvector{-1\\-1\\1},\,\colvector{1\\2\\3},\,\colvector{3\\1\\3},\,\colvector{0\\1\\-3}}}</me></p>
            </statement>
            <solution xml:id="solution-LI-M50">
                <p>We want to first find some relations of linear dependence on <m>\set{\vect{v}_1,\,\vect{v}_2,\,\vect{v}_3,\,\vect{v}_4,\,\vect{v}_5}</m> that will allow us to <q>kick out</q> some vectors, in the spirit of <xref ref="example-SCAD" acro="SCAD"/>.  To find relations of linear dependence, we formulate a matrix <m>A</m> whose columns are <m>\vect{v}_1,\,\vect{v}_2,\,\vect{v}_3,\,\vect{v}_4,\,\vect{v}_5</m>.  Then we consider the homogeneous system of equations <m>\homosystem{A}</m> by row-reducing its coefficient matrix (remember that if we formulated the augmented matrix we would just add a column of zeros).  After row-reducing, we obtain<me>\begin{bmatrix}
                \leading{1} &amp; 0 &amp; 0 &amp; 2 &amp; -1\\
                0 &amp; \leading{1} &amp; 0 &amp; 1 &amp; -2\\
                0 &amp; 0 &amp; \leading{1} &amp; 0 &amp; 0
                \end{bmatrix}</me>.  From this we find that solutions can be obtained employing the free variables <m>x_4</m> and <m>x_5</m>.  With appropriate choices we will be able to conclude that vectors <m>\vect{v}_4</m> and <m>\vect{v}_5</m> are unnecessary for creating <m>W</m> via a span.  By <xref ref="theorem-SLSLC" acro="SLSLC"/> the choice of free variables below lead to solutions and linear combinations, which are then rearranged.<md>
                    <mrow>x_4=1, x_5=0&amp;&amp;\Rightarrow&amp;&amp;(-2)\vect{v}_1+(-1)\vect{v}_2+(0)\vect{v}_3+(1)\vect{v}_4+(0)\vect{v}_5=\zerovector&amp;&amp;\Rightarrow&amp;&amp;\vect{v}_4=2\vect{v}_1+\vect{v}_2</mrow>
                    <mrow>x_4=0, x_5=1&amp;&amp;\Rightarrow&amp;&amp;(1)\vect{v}_1+(2)\vect{v}_2+(0)\vect{v}_3+(0)\vect{v}_4+(1)\vect{v}_5=\zerovector&amp;&amp;\Rightarrow&amp;&amp;\vect{v}_5=-\vect{v}_1-2\vect{v}_2</mrow>
                </md>  Since <m>\vect{v}_4</m> and <m>\vect{v}_5</m> can be expressed as linear combinations of <m>\vect{v}_1</m> and <m>\vect{v}_2</m>  we can say that <m>\vect{v}_4</m> and <m>\vect{v}_5</m> are not needed for the linear combinations used to build <m>W</m> (a claim that we could establish carefully with a pair of set equality arguments).  Thus<me>W=\spn{\set{\vect{v}_1,\,\vect{v}_2,\,\vect{v}_3}}=\spn{\set{\colvector{2\\1\\1},\,\colvector{-1\\-1\\1},\,\colvector{1\\2\\3}}}</me>.  That the <m>\set{\vect{v}_1,\,\vect{v}_2,\,\vect{v}_3}</m> is linearly independent set can be established quickly with <xref ref="theorem-LIVRN" acro="LIVRN"/>.</p>
                <p>There are other answers to this question, but notice that any nontrivial linear combination of <m>\vect{v}_1,\,\vect{v}_2,\,\vect{v}_3,\,\vect{v}_4,\,\vect{v}_5</m> will have a zero coefficient on <m>\vect{v}_3</m>, so this vector can never be eliminated from the set used to build the span.</p>
            </solution>
        </exercise>
        <exercise number="M51" xml:id="exercise-LI-M51">
            <statement contributor="manleyperkel">
                <p>Consider the subspace <m>W=\spn{\set{\vect{v}_1,\,\vect{v}_2,\,\vect{v}_3,\,\vect{v}_4}}</m>.  Find a set <m>S</m> so that (1)  <m>S</m> is a subset of <m>W</m>, (2) <m>S</m> is linearly independent, and (3) <m>W=\spn{S}</m>.  Write each vector not included in <m>S</m> as a linear combination of the vectors that are in <m>S</m>.<md>
                    <mrow>\vect{v}_1&amp;=\colvector{1\\-1\\2}
                    &amp;
                    \vect{v}_2&amp;=\colvector{4\\-4\\8}
                    &amp;
                    \vect{v}_3&amp;=\colvector{-3\\2\\-7}
                    &amp;
                    \vect{v}_4&amp;=\colvector{2\\1\\7}</mrow></md></p>
            </statement>
            <solution xml:id="solution-LI-M51">
                <p>This problem can be solved using the approach in <xref ref="solution-LI-M50" acro="LI.M50"/>.  We will provide a solution here that is more ad-hoc, but note that we will have a more straight-forward procedure given by the upcoming <xref ref="theorem-BS" acro="BS"/>.</p>
                <p><m>\vect{v}_1</m> is a nonzero vector, so in a set all by itself we have a linearly independent set.  As <m>\vect{v}_2</m> is a scalar multiple of <m>\vect{v}_1</m>, the equation <m>-4\vect{v}_1+\vect{v}_2=\zerovector</m> is a relation of linear dependence on <m>\set{\vect{v}_1,\,\vect{v}_2}</m>, so we will pass on <m>\vect{v}_2</m>.  No such relation of linear dependence exists on <m>\set{\vect{v}_1,\,\vect{v}_3}</m>, though on <m>\set{\vect{v}_1,\,\vect{v}_3,\,\vect{v}_4}</m> we have the relation of linear dependence <m>7\vect{v}_1+3\vect{v}_3+\vect{v}_4=\zerovector</m>. So take <m>S=\set{\vect{v}_1,\,\vect{v}_3}</m>, which is linearly independent.</p>
                <p>Then<md>
                    <mrow>\vect{v}_2&amp;=4\vect{v_1}+0\vect{v_3}
                    &amp;
                    \vect{v}_4&amp;=-7\vect{v_1}-3\vect{v_3}</mrow>
                </md>The two equations above are enough to justify the set equality<md>
                    <mrow>W
                    &amp;=\spn{\set{\vect{v}_1,\,\vect{v}_2,\,\vect{v}_3,\,\vect{v}_4}}
                    =\spn{\set{\vect{v}_1,\,\vect{v}_3}}
                    =\spn{S}</mrow>
                </md>There are other solutions (for example, swap the roles of <m>\vect{v}_1</m> and <m>\vect{v}_2</m>, but by upcoming theorems we can confidently claim that any solution will be a set <m>S</m> with exactly two vectors.</p>
            </solution>
        </exercise>
        <exercise number="T10" xml:id="exercise-LI-T10">
            <statement contributor="martinjackson">
                <p>Prove that if a set of vectors contains the zero vector, then the set is linearly dependent.  (Ed. <q>The zero vector is death to linearly independent sets.</q>)</p>
            </statement>
        </exercise>
        <exercise number="T12" xml:id="exercise-LI-T12">
            <statement>
                <p>Suppose that <m>S</m> is a linearly independent set of vectors, and <m>T</m> is a subset of <m>S</m>, <m>T\subseteq S</m> (<xref ref="definition-SSET" acro="SSET"/>).  Prove that <m>T</m> is linearly independent.</p>
            </statement>
        </exercise>
        <exercise number="T13" xml:id="exercise-LI-T13">
            <statement>
                <p>Suppose that <m>T</m> is a linearly dependent set of vectors, and <m>T</m> is a subset of <m>S</m>, <m>T\subseteq S</m> (<xref ref="definition-SSET" acro="SSET"/>).  Prove that <m>S</m> is linearly dependent.</p>
            </statement>
        </exercise>
        <exercise number="T15" xml:id="exercise-LI-T15">
            <statement>
                <p>Suppose that <m>\set{\vectorlist{v}{n}}</m> is a set of vectors.   Prove that<md>
                    <mrow>\set{\vect{v}_1-\vect{v}_2,\,\vect{v}_2-\vect{v}_3,\,\vect{v}_3-\vect{v}_4,\,\dots,\,\vect{v}_n-\vect{v}_1}</mrow>
                </md>is a linearly dependent set.</p>
            </statement>
            <solution xml:id="solution-LI-T15">
                <p>Consider the following linear combination<md>
                    <mrow>1\left(\vect{v}_1-\vect{v}_2\right)+&amp;
                    1\left(\vect{v}_2-\vect{v}_3\right)+
                    1\left(\vect{v}_3-\vect{v}_4\right)+\cdots+
                    1\left(\vect{v}_n-\vect{v}_1\right)</mrow>
                    <mrow>&amp;=
                    \vect{v}_1-\vect{v}_2+
                    \vect{v}_2-\vect{v}_3+
                    \vect{v}_3-\vect{v}_4+\dots+
                    \vect{v}_n-\vect{v}_1</mrow>
                    <mrow>&amp;=\vect{v}_1+\zerovector+\zerovector+\cdots+\zerovector-\vect{v}_1</mrow>
                    <mrow>&amp;=\zerovector</mrow></md>.  This is a nontrivial relation of linear dependence (<xref ref="definition-RLDCV" acro="RLDCV"/>), so by <xref ref="definition-LICV" acro="LICV"/> the set is linearly dependent.</p>
            </solution>
        </exercise>
        <exercise number="T20" xml:id="exercise-LI-T20">
            <statement>
                <p>Suppose that <m>\set{\vect{v}_1,\,\vect{v}_2,\,\vect{v}_3,\,\vect{v}_4}</m> is a linearly independent set in <m>\complex{35}</m>.  Prove that<me>\set{
                \vect{v}_1,\,
                \vect{v}_1+\vect{v}_2,\,
                \vect{v}_1+\vect{v}_2+\vect{v}_3,\,
                \vect{v}_1+\vect{v}_2+\vect{v}_3+\vect{v}_4
                }</me>is a linearly independent set.</p>
            </statement>
            <solution xml:id="solution-LI-T20">
                <p>Our hypothesis and our conclusion use the term linear independence, so it will get a workout.  To establish linear independence, we begin with the definition (<xref ref="definition-LICV" acro="LICV"/>) and write a relation of linear dependence (<xref ref="definition-RLDCV" acro="RLDCV"/>)<me>\alpha_1\left(\vect{v}_1\right)+
                \alpha_2\left(\vect{v}_1+\vect{v}_2\right)+
                \alpha_3\left(\vect{v}_1+\vect{v}_2+\vect{v}_3\right)+
                \alpha_4\left(\vect{v}_1+\vect{v}_2+\vect{v}_3+\vect{v}_4\right)
                =\zerovector</me>.  Using the distributive and commutative properties of vector addition and scalar multiplication (<xref ref="theorem-VSPCV" acro="VSPCV"/>) this equation can be rearranged as<me>\left(\alpha_1+\alpha_2+\alpha_3+\alpha_4\right)\vect{v}_1+
            \left(\alpha_2+\alpha_3+\alpha_4\right)\vect{v}_2+
            \left(\alpha_3+\alpha_4\right)\vect{v}_3+
            \left(\alpha_4\right)\vect{v}_4
            =
            \zerovector</me>.  However, this is a relation of linear dependence (<xref ref="definition-RLDCV" acro="RLDCV"/>) on a linearly independent set, <m>\set{\vect{v}_1,\,\vect{v}_2,\,\vect{v}_3,\,\vect{v}_4}</m> (this was our lone hypothesis).  By the definition of linear independence (<xref ref="definition-LICV" acro="LICV"/>) the scalars must all be zero.  This is the homogeneous system of equations<md>
                <mrow>\alpha_1+\alpha_2+\alpha_3+\alpha_4&amp;=0</mrow>
                <mrow>\alpha_2+\alpha_3+\alpha_4&amp;=0</mrow>
                <mrow>\alpha_3+\alpha_4&amp;=0</mrow>
                <mrow>\alpha_4&amp;=0</mrow>
            </md>.  Row-reducing the coefficient matrix of this system (or backsolving) gives the conclusion<md>
                <mrow>\alpha_1=0&amp;&amp;\alpha_2=0&amp;&amp;\alpha_3=0&amp;&amp;\alpha_4=0</mrow>
            </md>.  This means, by <xref ref="definition-LICV" acro="LICV"/>, that the original set<me>\set{
            \vect{v}_1,\,
            \vect{v}_1+\vect{v}_2,\,
            \vect{v}_1+\vect{v}_2+\vect{v}_3,\,
            \vect{v}_1+\vect{v}_2+\vect{v}_3+\vect{v}_4
            }</me>is linearly independent.</p>
            </solution>
        </exercise>
        <exercise number="T50" xml:id="exercise-LI-T50">
            <statement>
                <p>Suppose that <m>A</m> is an <m>m\times n</m> matrix with linearly independent columns and the linear system <m>\linearsystem{A}{\vect{b}}</m> is consistent.  Show that this system has a unique solution.  (Notice that we are not requiring <m>A</m> to be square.)</p>
            </statement>
            <solution xml:id="solution-LI-T50">
                <p>Let <m>A=\matrixcolumns{A}{n}</m>.  <m>\linearsystem{A}{\vect{b}}</m> is consistent, so we know the system has at least one solution (<xref ref="definition-CS" acro="CS"/>).  We would like to show that there are no more than one solution to the system.  Employing Proof Technique<nbsp/><xref ref="technique-U" acro="U" text="global"/>, suppose that <m>\vect{x}</m> and <m>\vect{y}</m> are two solution vectors for <m>\linearsystem{A}{\vect{b}}</m>.  By <xref ref="theorem-SLSLC" acro="SLSLC"/> we know we can write<md>
                    <mrow>\vect{b}&amp;=
                    \vectorentry{\vect{x}}{1}A_1+
                    \vectorentry{\vect{x}}{2}A_2+
                    \vectorentry{\vect{x}}{3}A_3+
                    \cdots+
                    \vectorentry{\vect{x}}{n}A_n</mrow><mrow>\vect{b}&amp;=
                    \vectorentry{\vect{y}}{1}A_1+
                    \vectorentry{\vect{y}}{2}A_2+
                    \vectorentry{\vect{y}}{3}A_3+
                    \cdots+
                    \vectorentry{\vect{y}}{n}A_n</mrow>
                </md>Then<md>
                    <mrow>\zerovector
                    &amp;=\vect{b}-\vect{b}</mrow>
                    <mrow>&amp;=
                    \left(
                    \vectorentry{\vect{x}}{1}A_1+
                    \vectorentry{\vect{x}}{2}A_2+
                    \cdots+
                    \vectorentry{\vect{x}}{n}A_n
                    \right)
                    -
                    \left(
                    \vectorentry{\vect{y}}{1}A_1+
                    \vectorentry{\vect{y}}{2}A_2+
                    \cdots+
                    \vectorentry{\vect{y}}{n}A_n
                    \right)</mrow>
                    <mrow>&amp;=
                    \left(\vectorentry{\vect{x}}{1}-\vectorentry{\vect{y}}{1}\right)A_1+
                    \left(\vectorentry{\vect{x}}{2}-\vectorentry{\vect{y}}{2}\right)A_2+
                    \cdots+
                    \left(\vectorentry{\vect{x}}{n}-\vectorentry{\vect{y}}{n}\right)A_n</mrow>
                </md>.  This is a relation of linear dependence (<xref ref="definition-RLDCV" acro="RLDCV"/>) on a linearly independent set (the columns of <m>A</m>).  So the scalars <em>must</em> all be zero<md>
                    <mrow>\vectorentry{\vect{x}}{1}-\vectorentry{\vect{y}}{1}&amp;=0
                    &amp;
                    \vectorentry{\vect{x}}{2}-\vectorentry{\vect{y}}{2}&amp;=0
                    &amp;
                    \dots&amp;
                    &amp;
                    \vectorentry{\vect{x}}{n}-\vectorentry{\vect{y}}{n}&amp;=0</mrow></md>.  Rearranging these equations yields the statement that <m>\vectorentry{\vect{x}}{i}=\vectorentry{\vect{y}}{i}</m>, for <m>1\leq i\leq n</m>.  However, this is exactly how we define vector equality (<xref ref="definition-CVE" acro="CVE"/>), so <m>\vect{x}=\vect{y}</m> and the system has only one solution.</p>
            </solution>
        </exercise>
    </exercises>
</section>
