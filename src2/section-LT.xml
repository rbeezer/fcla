<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the book                -->
<!--                                              -->
<!--      A First Course in Linear Algebra        -->
<!--                                              -->
<!-- Copyright (C) 2004-2017  Robert A. Beezer    -->
<!-- See the file COPYING for copying conditions. -->

<section xml:id="section-LT" acro="LT">
    <title>Linear Transformations</title>
    <introduction>
        <p>Early in <xref ref="chapter-VS" acro="VS"/> we prefaced the definition of a vector space with the comment that it was <q>one of the two most important definitions in the entire course.</q>  Here comes the other.  Any capsule summary of linear algebra would have to describe the subject as the interplay of linear transformations and vector spaces.  Here we go.</p>
    </introduction>
    <subsection xml:id="subsection-LT-LT" acro="LT">
        <title>Linear Transformations</title>
        <definition xml:id="definition-LT" acro="LT">
            <title>Linear Transformation</title>
            <idx>linear transformation</idx>
            <statement>
                <p>A <term>linear transformation</term>, <m>\ltdefn{T}{U}{V}</m>, is a function that carries elements of the vector space <m>U</m> (called the <term>domain</term>) to the vector space <m>V</m> (called the <term>codomain</term>), and which has two additional properties<ol>
                    <li><m>\lteval{T}{\vect{u}_1+\vect{u}_2}=\lteval{T}{\vect{u}_1}+\lteval{T}{\vect{u}_2}</m> for all <m>\vect{u}_1,\,\vect{u}_2\in U</m>.</li>
                    <li><m>\lteval{T}{\alpha\vect{u}}=\alpha\lteval{T}{\vect{u}}</m> for all <m>\vect{u}\in U</m> and all <m>\alpha\in\complexes</m>.</li>
                </ol></p>
            </statement>
            <notation xml:id="notation-LT" acro="LT">
                <idx>
                    <h>linear transformation</h>
                    <h>notation</h>
                </idx>
                <description>Linear Transformation</description>
                <usage>\ltdefn{T}{U}{V}</usage>
            </notation>
        </definition>
        <p>The two defining conditions in the definition of a linear transformation should <q>feel linear,</q> whatever that means.  Conversely, these two conditions could be taken as <em>exactly</em> what it means <em>to be</em> linear.  As every vector space property derives from vector addition and scalar multiplication, so too, every property of a linear transformation derives from these two defining properties.  While these conditions may be reminiscent of how we test subspaces, they really are quite different, so do not confuse the two.</p>
        <p>Here are two diagrams that convey the essence of the two defining properties of a linear transformation.  In each case, begin in the upper left-hand corner, and follow the arrows around the rectangle to the lower-right hand corner, taking two different routes and doing the indicated operations labeled on the arrows.  There are two results there.  For a linear transformation these two expressions are always equal.</p>
        <figure xml:id="figure-DLTA" acro="DLTA">
            <caption>Definition of Linear Transformation, Additive</caption>
            <image xml:id="image-DLTA">
                <latex-image>
                \begin{tikzpicture}
                \matrix (m) [matrix of math nodes, row sep=5em, column sep=10em, text height=1.5ex, text depth=0.25ex]
                { \vect{u}_1,\,\vect{u}_2 \amp \lteval{T}{\vect{u}_1},\,\lteval{T}{\vect{u}_2} \\
                \vect{u}_1+\vect{u}_2 \amp \lteval{T}{\vect{u}_1+\vect{u}_2}=\lteval{T}{\vect{u}_1}+\lteval{T}{\vect{u}_2}\\};
                \path[->]
                (m-1-1) edge[thick] node[auto] {\(T\)} (m-1-2)
                (m-1-2) edge[thick] node[auto] {\(+\)} (m-2-2)
                (m-1-1) edge[thick] node[auto] {\(+\)} (m-2-1)
                (m-2-1) edge[thick] node[auto] {\(T\)} (m-2-2);
                \end{tikzpicture}
                </latex-image>
            </image>
        </figure>
        <figure xml:id="figure-DLTM" acro="DLTM">
            <caption>Definition of Linear Transformation, Multiplicative</caption>
            <image xml:id="image-DLTM">
                <latex-image>
                \begin{tikzpicture}
                \matrix (m) [matrix of math nodes, row sep=5em, column sep=10em, text height=1.5ex, text depth=0.25ex]
                { \vect{u} \amp \lteval{T}{\vect{u}} \\
                \alpha\vect{u} \amp \lteval{T}{\alpha\vect{u}}=\alpha\lteval{T}{\vect{u}}\\};
                \path[->]
                (m-1-1) edge[thick] node[auto] {\(T\)}      (m-1-2)
                (m-1-2) edge[thick] node[auto] {\(\alpha\)} (m-2-2)
                (m-1-1) edge[thick] node[auto] {\(\alpha\)} (m-2-1)
                (m-2-1) edge[thick] node[auto] {\(T\)}      (m-2-2);
                \end{tikzpicture}
                </latex-image>
            </image>
        </figure>
        <p>A couple of words about notation.  <m>T</m> is the <em>name</em> of the linear transformation, and should be used when we want to discuss the function as a whole.  <m>\lteval{T}{\vect{u}}</m> is how we talk about the output of the function, it is a vector in the vector space <m>V</m>.  When we write <m>\lteval{T}{\vect{x}+\vect{y}}=\lteval{T}{\vect{x}}+\lteval{T}{\vect{y}}</m>, the plus sign on the left is the operation of vector addition in the vector space <m>U</m>, since <m>\vect{x}</m> and <m>\vect{y}</m> are elements of <m>U</m>.  The plus sign on the right is the operation of vector addition in the vector space <m>V</m>, since <m>\lteval{T}{\vect{x}}</m> and <m>\lteval{T}{\vect{y}}</m> are elements of the vector space <m>V</m>.  These two instances of vector addition might be wildly different.</p>
        <p>Let us examine several examples and begin to form a catalog of known linear transformations to work with.</p>
        <example xml:id="example-ALT" acro="ALT">
            <title>A linear transformation</title>
            <idx>
                <h>linear transformation</h>
                <h>checking</h>
            </idx>
            <p>Define <m>\ltdefn{T}{\complex{3}}{\complex{2}}</m> by describing the output of the function for a generic input with the formula<me>\lteval{T}{\colvector{x_1\\x_2\\x_3}}=\colvector{2x_1+x_3\\-4x_2}</me>and check the two defining properties.  We have<md>
                <mrow>\lteval{T}{\vect{x}+\vect{y}}
                &amp;=\lteval{T}{\colvector{x_1\\x_2\\x_3}+\colvector{y_1\\y_2\\y_3}}
                     =\lteval{T}{\colvector{x_1+y_1\\x_2+y_2\\x_3+y_3}}</mrow>
                <mrow>&amp;=\colvector{2(x_1+y_1)+(x_3+y_3)\\-4(x_2+y_2)}
                     =\colvector{(2x_1+x_3)+(2y_1+y_3)\\-4x_2+(-4)y_2}
                     =\colvector{2x_1+x_3\\-4x_2}+\colvector{2y_1+y_3\\-4y_2}</mrow>
                <mrow>&amp;=\lteval{T}{\colvector{x_1\\x_2\\x_3}}+\lteval{T}{\colvector{y_1\\y_2\\y_3}}
                     =\lteval{T}{\vect{x}}+\lteval{T}{\vect{y}}</mrow>
                <intertext>and</intertext>
                <mrow>\lteval{T}{\alpha\vect{x}}
                &amp;=\lteval{T}{\alpha\colvector{x_1\\x_2\\x_3}}
                     =\lteval{T}{\colvector{\alpha x_1\\\alpha x_2\\\alpha x_3}}</mrow>
                <mrow>&amp;=\colvector{2(\alpha x_1)+(\alpha x_3)\\-4(\alpha x_2)}
                     =\colvector{\alpha(2x_1+x_3)\\\alpha(-4x_2)}
                     =\alpha\colvector{2x_1+x_3\\-4x_2}</mrow>
                <mrow>&amp;=\alpha\lteval{T}{\colvector{x_1\\x_2\\x_3}}
                     =\alpha\lteval{T}{\vect{x}}</mrow>
            </md>.</p>
            <p>So by <xref ref="definition-LT" acro="LT"/>, <m>T</m> is a linear transformation.</p>
        </example>
        <p>It can be just as instructive to look at functions that are <em>not</em> linear transformations.  Since the defining conditions must be true for <em>all</em> vectors and scalars, it is enough to find just one situation where the properties fail.</p>
        <example xml:id="example-NLT" acro="NLT">
            <title>Not a linear transformation</title>
            <idx>
                <h>linear transformation</h>
                <h>not</h>
            </idx>
            <p>Define <m>\ltdefn{S}{\complex{3}}{\complex{3}}</m> by<me>\lteval{S}{\colvector{x_1\\x_2\\x_3}}=\colvector{4x_1+2x_2\\0\\x_1+3x_3-2}</me>.</p>
            <p>This function <q>looks</q> linear, but consider<md>
                <mrow>3\,\lteval{S}{\colvector{1\\2\\3}}&amp;=3\,\colvector{8\\0\\8}=\colvector{24\\0\\24}</mrow>
                <intertext>while</intertext>
                <mrow>\lteval{S}{3\,\colvector{1\\2\\3}}&amp;=\lteval{S}{\colvector{3\\6\\9}}=\colvector{24\\0\\28}</mrow>
            </md>.</p>
            <p>So the second required property fails for the choice of <m>\alpha=3</m> and <m>\vect{x}=\colvector{1\\2\\3}</m> and by <xref ref="definition-LT" acro="LT"/>, <m>S</m> is not a linear transformation.  It is just about as easy to find an example where the first defining property fails (try it!).  Notice that it is the <q>-2</q> in the third component of the definition of <m>S</m> that prevents the function from being a linear transformation.</p>
        </example>
        <example xml:id="example-LTPM" acro="LTPM">
            <title>Linear transformation, polynomials to matrices</title>
            <idx>
                <h>linear transformation</h>
                <h>polynomials to matrices</h>
            </idx>
            <p>Define a linear transformation <m>\ltdefn{T}{P_3}{M_{22}}</m> by<me>\lteval{T}{a+bx+cx^2+dx^3}=\begin{bmatrix}a+b&amp;a-2c\\d&amp;b-d\end{bmatrix}</me>.</p>
            <p>We verify the two defining conditions of a linear transformation.  We have<md>
                <mrow>\lteval{T}{\vect{x}+\vect{y}}&amp;=
                \lteval{T}{(a_1+b_1x+c_1x^2+d_1x^3)+(a_2+b_2x+c_2x^2+d_2x^3)}\\
                &amp;=\lteval{T}{(a_1+a_2)+(b_1+b_2)x+(c_1+c_2)x^2+(d_1+d_2)x^3}\\
                &amp;=\begin{bmatrix}
                (a_1+a_2)+(b_1+b_2)&amp;(a_1+a_2)-2(c_1+c_2)\\
                d_1+d_2&amp;(b_1+b_2)-(d_1+d_2)
                \end{bmatrix}</mrow>
                <mrow>&amp;=\begin{bmatrix}
                (a_1+b_1)+(a_2+b_2)&amp;(a_1-2c_1)+(a_2-2c_2)\\
                d_1+d_2&amp;(b_1-d_1)+(b_2-d_2)
                \end{bmatrix}</mrow>
                <mrow>&amp;=\begin{bmatrix}a_1+b_1&amp;a_1-2c_1\\d_1&amp;b_1-d_1\end{bmatrix}+
                     \begin{bmatrix}a_2+b_2&amp;a_2-2c_2\\d_2&amp;b_2-d_2\end{bmatrix}</mrow>
                <mrow>&amp;=\lteval{T}{a_1+b_1x+c_1x^2+d_1x^3}+\lteval{T}{a_2+b_2x+c_2x^2+d_2x^3}\\
                &amp;=\lteval{T}{\vect{x}}+\lteval{T}{\vect{y}}</mrow>
                <intertext>and</intertext>
                <mrow>\lteval{T}{\alpha\vect{x}}&amp;=\lteval{T}{\alpha(a+bx+cx^2+dx^3)}\\
                &amp;=\lteval{T}{(\alpha a)+(\alpha b)x+(\alpha c)x^2+(\alpha d)x^3}\\
                &amp;=\begin{bmatrix}
                (\alpha a)+(\alpha b)&amp;(\alpha a)-2(\alpha c)\\
                \alpha d&amp;(\alpha b)-(\alpha d)
                \end{bmatrix}</mrow>
                <mrow>&amp;=\begin{bmatrix}
                \alpha(a+b)&amp;\alpha(a-2c)\\
                \alpha d&amp;\alpha(b-d)
                \end{bmatrix}</mrow>
                <mrow>&amp;=\alpha\begin{bmatrix}a+b&amp;a-2c\\d&amp;b-d\end{bmatrix}</mrow>
                <mrow>&amp;=\alpha\lteval{T}{a+bx+cx^2+dx^3}\\
                &amp;=\alpha\lteval{T}{\vect{x}}</mrow>
            </md>.</p>
            <p>So by <xref ref="definition-LT" acro="LT"/>, <m>T</m> is a linear transformation.</p>
        </example>
        <example xml:id="example-LTPP" acro="LTPP">
            <title>Linear transformation, polynomials to polynomials</title>
            <idx>
                <h>linear transformation</h>
                <h> polynomials to polynomials</h>
            </idx>
            <p>Define a function <m>\ltdefn{S}{P_4}{P_5}</m> by<me>S(p(x))=(x-2)p(x)</me>.</p>
            <p>Then<md>
                <mrow>\lteval{S}{p(x)+q(x)}&amp;=(x-2)(p(x)+q(x))</mrow>
                <mrow>&amp;=(x-2)p(x)+(x-2)q(x)=\lteval{S}{p(x)}+\lteval{S}{q(x)}</mrow>
                <mrow>\lteval{S}{\alpha p(x)}&amp;=(x-2)(\alpha p(x))=(x-2)\alpha p(x)=\alpha(x-2)p(x)=\alpha\lteval{S}{p(x)}</mrow>
            </md>.</p>
            <p>So by <xref ref="definition-LT" acro="LT"/>, <m>S</m> is a linear transformation.</p>
        </example>
        <p>Linear transformations have many amazing properties, which we will investigate through the next few sections.  However, as a taste of things to come, here is a theorem we can prove now and put to use immediately.</p>
        <theorem xml:id="theorem-LTTZZ" acro="LTTZZ">
            <title>Linear Transformations Take Zero to Zero</title>
            <idx>
                <h>linear transformation</h>
                <h>zero vector</h>
            </idx>
            <statement>
                <p>Suppose <m>\ltdefn{T}{U}{V}</m> is a linear transformation.  Then <m>\lteval{T}{\zerovector}=\zerovector</m>.</p>
            </statement>
            <proof>
                <p>The two zero vectors in the conclusion of the theorem are different.  The first is from <m>U</m> while the second is from <m>V</m>.  We will subscript the zero vectors in this proof to highlight the distinction.  Think about your objects.  (This proof is contributed by <xref ref="markshoemaker"/>).  We have<md>
                    <mrow>\lteval{T}{\zerovector_U}
                    &amp;=\lteval{T}{0\zerovector_U}&amp;&amp;
                        <xref ref="theorem-ZSSM" acro="ZSSM"/>\text{ in }U</mrow>
                    <mrow>&amp;=0\lteval{T}{\zerovector_U}&amp;&amp;
                        <xref ref="definition-LT" acro="LT"/></mrow>
                    <mrow>&amp;=\zerovector_V&amp;&amp;
                        <xref ref="theorem-ZSSM" acro="ZSSM"/>\text{ in }V</mrow>
                </md>.</p>
            </proof>
        </theorem>
        <p>Return to <xref ref="example-NLT" acro="NLT"/> and compute <m>\lteval{S}{\colvector{0\\0\\0}}=\colvector{0\\0\\-2}</m> to quickly see again that <m>S</m> is not a linear transformation, while in <xref ref="example-LTPM" acro="LTPM"/>  compute<md>
            <mrow>\lteval{S}{0+0x+0x^2+0x^3}&amp;=\begin{bmatrix}0&amp;0\\0&amp;0\end{bmatrix}</mrow>
        </md>as an example of <xref ref="theorem-LTTZZ" acro="LTTZZ"/> at work.</p>
        <computation xml:id="sage-LTS" acro="LTS">
            <title>Linear Transformations, Symbolic</title>
            <idx>
                <h>linear transformation</h>
                <h>symbolic</h>
            </idx>
            <p>There are several ways to create a linear transformation in Sage, and many natural operations on these objects are possible.  As previously, rather than use the complex numbers as our number system, we will instead use the rational numbers, since Sage can model them exactly.  We will also use the following transformation repeatedly for examples, when no special properties are required:<md>
                <mrow>&amp;\ltdefn{T}{\complex{3}}{\complex{4}}</mrow>
                <mrow>&amp;\lteval{T}{\colvector{x_1\\x_2\\x_3}}=\colvector{-x_1 + 2x_3\\x_1 + 3x_2 + 7x_3\\x_1 + x_2 + x_3\\2x_1 + 3x_2 + 5x_3}</mrow>
            </md>.To create this linear transformation in Sage, we first create a <q>symbolic</q> function, which requires that we also first define some symbolic variables which are <c>x1</c>, <c>x2</c> and <c>x3</c> in this case.  (You can bypass the intermediate variable <c>outputs</c> in your own work.  We will use it consistently to allow us to spread the definition across several lines without the Sage preparser getting in the way.  In other words, it is safe to combine the the two lines below and not use <c>outputs</c>.)</p>
            <sage xml:id="sagecell-LTS-1">
                <input>
                x1, x2, x3 = var('x1, x2, x3')
                outputs = [ -x1        + 2*x3,
                             x1 + 3*x2 + 7*x3,
                             x1 +   x2 +   x3,
                           2*x1 + 3*x2 + 5*x3]
                T_symbolic(x1, x2, x3) = outputs
                </input>
            </sage>
            <p>You can experiment with <c>T_symbolic</c>, evaluating it at triples of rational numbers, and perhaps doing things like calculating its partial derivatives.  We will use it as input to the <c>linear_transformation()</c> constructor.  We just need to specify carefully the domain and codomain, now as vector spaces over the rationals rather than the complex numbers.</p>
            <sage xml:id="sagecell-LTS-2">
                <input>
                T = linear_transformation(QQ^3, QQ^4, T_symbolic)
                </input>
            </sage>
            <p>You can now, of course, experiment with <c>T</c> via tab-completion, but we will explain the various properties of Sage linear transformations as we work through this chapter.  Even some seemingly simple operations, such as printing <c>T</c> will require some explanation.  But for starters, we can evaluate <c>T</c>.</p>
            <sage xml:id="sagecell-LTS-3">
                <input>
                u = vector(QQ, [3, -1, 2])
                w = T(u)
                w
                </input>
                <output>
                (1, 14, 4, 13)
                </output>
            </sage>
            <sage xml:id="sagecell-LTS-4">
                <input>
                w.parent()
                </input>
                <output>
                Vector space of dimension 4 over Rational Field
                </output>
            </sage>
            <p>Here is a small verification of <xref ref="theorem-LTTZZ" acro="LTTZZ"/>.</p>
            <sage xml:id="sagecell-LTS-5">
                <input>
                zero_in = zero_vector(QQ, 3)
                zero_out = zero_vector(QQ, 4)
                T(zero_in) == zero_out
                </input>
                <output>
                True
                </output>
            </sage>
            <p>Note that Sage will recognize some symbolic functions as not being linear transformations (in other words, inconsistent with <xref ref="definition-LT" acro="LT"/>), but this detection is fairly easy to fool.  We will see some safer ways to create a linear transformation shortly.</p>
        </computation>
    </subsection>
    <subsection xml:id="subsection-LT-LTC" acro="LTC">
        <title>Linear Transformation Cartoons</title>
        <p>Throughout this chapter, and <xref ref="chapter-R" acro="R"/>, we will include drawings of linear transformations.  We will call them <q>cartoons,</q> not because they are humorous, but because they will only expose a portion of the truth.  A Bugs Bunny cartoon might give us some insights on human nature, but the rules of physics and biology are routinely (and grossly) violated.  So it will be with our <term>linear transformation cartoons</term>.  Here is our first, followed by a guide to help you understand how these are meant to describe fundamental truths about linear transformations, while simultaneously violating other truths.</p>
        <figure xml:id="figure-GLT" acro="GLT">
            <caption>General Linear Transformation</caption>
            <image xml:id="image-GLT">
                <latex-image>
                \begin{tikzpicture}
                \tikzset{ltvect/.style={shape=circle, minimum size=0.30em, inner sep=0pt, draw, fill=black}}
                \tikzset{ltedge/.style={->, bend left=20, thick, shorten \lt=0.1em, shorten >=0.1em}}
                % base generic picture, equal ovals
                % vertical axes at x = 5, x = 20  space is [x=10 to x=15]
                \draw ( 5em, 8em) circle [x radius=5em, y radius=8em, thick];
                \draw (20em, 8em) circle [x radius=5em, y radius=8em, thick];
                \node (U) at ( 5em, -1em) {\(U\)};
                \node (V) at (20em, -1em) {\(V\)};
                \draw[->, thick, draw] (U) to node[auto] {\(T\)} (V);
                % inputs
                \node (w)     [ltvect, label=left:\(\vect{w}\)]      at (5em, 13em) {};
                \node (u)     [ltvect, label=left:\(\vect{u}\)]      at (5em, 11em) {};
                \node (zeroU) [ltvect, label=left:\(\zerovector_U\)] at (5em,  8em) {};
                \node (x)     [ltvect, label=left:\(\vect{x}\)]      at (5em,  5em) {};
                % outputs
                \node (v)     [ltvect, label=right:\(\vect{v}\)]      at (20em, 12em) {};
                \node (zeroV) [ltvect, label=right:\(\zerovector_V\)] at (20em,  8em) {};
                \node (y)     [ltvect, label=right:\(\vect{y}\)]      at (20em,  5em) {};
                \node (t)     [ltvect, label=right:\(\vect{t}\)]      at (20em,  3em) {};
                % associations
                \draw[ltedge] (u)     to (v);
                \draw[ltedge] (w)     to (v);
                \draw[ltedge] (zeroU) to (zeroV);
                \draw[ltedge] (x)     to (y);
                \end{tikzpicture}
                </latex-image>
            </image>
        </figure>
        <p>Here we picture a linear transformation <m>\ltdefn{T}{U}{V}</m>, where this information will be consistently displayed along the bottom edge.  The ovals are meant to represent the vector spaces, in this case <m>U</m>, the domain, on the left and <m>V</m>, the codomain, on the right.  Of course, vector spaces are typically infinite sets, so you will have to imagine that characteristic of these sets.  A small dot inside of an oval will represent a vector within that vector space, sometimes with a name, sometimes not (in this case every vector has a name).  The sizes of the ovals are meant to be proportional to the dimensions of the vector spaces.  However, when we make no assumptions about the dimensions, we will draw the ovals as the same size, as we have done here (which is not meant to suggest that the dimensions have to be equal).</p>
        <p>To convey that the linear transformation associates a certain input with a certain output, we will draw an arrow from the input to the output.  So, for example, in this cartoon we suggest that <m>\lteval{T}{\vect{x}}=\vect{y}</m>.  Nothing in the definition of a linear transformation prevents two different inputs being sent to the same output and we see this in <m>\lteval{T}{\vect{u}}=\vect{v}=\lteval{T}{\vect{w}}</m>.  Similarly, an output may not have any input being sent its way, as illustrated by no arrow pointing at <m>\vect{t}</m>.  In this cartoon, we have captured the essence of our one general theorem about linear transformations, <xref ref="theorem-LTTZZ" acro="LTTZZ"/>, <m>\lteval{T}{\zerovector_U}=\zerovector_V</m>.  On occasion we might include this basic fact when it is relevant, at other times maybe not.  Note that the definition of a linear transformation requires that it be a function, so every element of the domain should be associated with some element of the codomain.  This will be reflected by never having an element of the domain without an arrow originating there.</p>
        <p>These cartoons are of course no substitute for careful definitions and proofs, but they can be a handy way to think about the various properties we will be studying.</p>
    </subsection>
    <subsection xml:id="subsection-LT-MLT" acro="MLT">
        <title>Matrices and Linear Transformations</title>
        <p>If you give me a matrix, then I can quickly build you a linear transformation.  Always.  First a motivating example and then the theorem.</p>
        <example xml:id="example-LTM" acro="LTM">
            <title>Linear transformation from a matrix</title>
            <idx>
                <h>linear transformation</h>
                <h>defined by a matrix</h>
            </idx>
            <p>Let<me>A=
            \begin{bmatrix}
            3&amp;-1&amp;8&amp;1\\
            2&amp;0&amp;5&amp;-2\\
            1&amp;1&amp;3&amp;-7
            \end{bmatrix}</me>and define a function <m>\ltdefn{P}{\complex{4}}{\complex{3}}</m> by<me>\lteval{P}{\vect{x}}=A\vect{x}</me>.</p>
            <p>So we are using an old friend, the matrix-vector product (<xref ref="definition-MVP" acro="MVP"/>) as a way to convert a vector with 4 components into a vector with 3 components.  Applying <xref ref="definition-MVP" acro="MVP"/> allows us to write the defining formula for <m>P</m> in a slightly different form,<me>\lteval{P}{\vect{x}}=A\vect{x}=
            \begin{bmatrix}
            3&amp;-1&amp;8&amp;1\\
            2&amp;0&amp;5&amp;-2\\
            1&amp;1&amp;3&amp;-7
            \end{bmatrix}
            \colvector{x_1\\x_2\\x_3\\x_4}
            =
            x_1\colvector{3\\2\\1}+
            x_2\colvector{-1\\0\\1}+
            x_3\colvector{8\\5\\3}+
            x_4\colvector{1\\-2\\-7}</me>.</p>
            <p>So we recognize the action of the function <m>P</m> as using the components of the vector (<m>x_1,\,x_2,\,x_3,\,x_4</m>) as scalars to form the output of <m>P</m> as a linear combination of the four columns of the matrix <m>A</m>, which are all members of <m>\complex{3}</m>, so the result is a vector in <m>\complex{3}</m>.  We can rearrange this expression further, using our definitions of operations in <m>\complex{3}</m> (<xref ref="section-VO" acro="VO"/>).  We have<md>
                <mrow>\lteval{P}{\vect{x}}
                &amp;=A\vect{x}&amp;&amp;
                    \text{Definition of }P</mrow>
                <mrow>&amp;=
                x_1\colvector{3\\2\\1}+
                x_2\colvector{-1\\0\\1}+
                x_3\colvector{8\\5\\3}+
                x_4\colvector{1\\-2\\-7}&amp;&amp;
                    <xref ref="definition-MVP" acro="MVP"/></mrow>
                <mrow>&amp;=
                \colvector{3x_1\\2x_1\\x_1}+
                \colvector{-x_2\\0\\x_2}+
                \colvector{8x_3\\5x_3\\3x_3}+
                \colvector{x_4\\-2x_4\\-7x_4}&amp;&amp;
                    <xref ref="definition-CVSM" acro="CVSM"/></mrow>
                <mrow>&amp;=\colvector{3x_1-x_2+8x_3+x_4\\2x_1+5x_3-2x_4\\x_1+x_2+3x_3-7x_4}&amp;&amp;
                    <xref ref="definition-CVA" acro="CVA"/></mrow>
            </md>.</p>
            <p>You might recognize this final expression as being similar in style to some previous examples (<xref ref="example-ALT" acro="ALT"/>) and some linear transformations defined in the archetypes (Archetype<nbsp/><xref ref="archetype-M" acro="M" text="global"/> through Archetype<nbsp/><xref ref="archetype-R" acro="R" text="global"/>).  But the expression that says the output of this linear transformation is a linear combination of the columns of <m>A</m> is probably the most powerful way of thinking about examples of this type.</p>
            <p>Almost forgot <mdash/> we should verify that <m>P</m> is indeed a linear transformation.  This is easy with two matrix properties from <xref ref="section-MM" acro="MM"/>.  We have<md>
                <mrow>\lteval{P}{\vect{x}+\vect{y}}
                &amp;=A\left(\vect{x}+\vect{y}\right)&amp;&amp;
                    \text{Definition of }P</mrow>
                <mrow>&amp;=A\vect{x}+A\vect{y}&amp;&amp;
                    <xref ref="theorem-MMDAA" acro="MMDAA"/></mrow>
                <mrow>&amp;=\lteval{P}{\vect{x}}+\lteval{P}{\vect{y}}&amp;&amp;
                    \text{Definition of }P</mrow>
                <intertext>and</intertext>
                <mrow>\lteval{P}{\alpha\vect{x}}
                &amp;=A\left(\alpha\vect{x}\right)&amp;&amp;
                    \text{Definition of }P</mrow>
                <mrow>&amp;=\alpha\left(A\vect{x}\right)&amp;&amp;
                    <xref ref="theorem-MMSMM" acro="MMSMM"/></mrow>
                <mrow>&amp;=\alpha\lteval{P}{\vect{x}}&amp;&amp;
                    \text{Definition of }P</mrow>
            </md>.</p>
            <p>So by <xref ref="definition-LT" acro="LT"/>, <m>P</m> is a linear transformation.</p>
        </example>
        <p>So the multiplication of a vector by a matrix <q>transforms</q> the input vector into an output vector, possibly of a different size, by performing a linear combination.  And this transformation happens in a <q>linear</q> fashion.  This <q>functional</q> view of the matrix-vector product is the most important shift you can make right now in how you think about linear algebra.  Here is the theorem, whose proof is very nearly an exact copy of the verification in the last example.</p>
        <theorem xml:id="theorem-MBLT" acro="MBLT">
            <title>Matrices Build Linear Transformations</title>
            <idx>
                <h>linear transformations</h>
                <h>from matrices</h>
            </idx>
            <statement>
                <p>Suppose that <m>A</m> is an <m>m\times n</m> matrix.  Define a function <m>\ltdefn{T}{\complex{n}}{\complex{m}}</m> by <m>\lteval{T}{\vect{x}}=A\vect{x}</m>.  Then <m>T</m> is a linear transformation.</p>
            </statement>
            <proof>
                <p>We have<md>
                    <mrow>\lteval{T}{\vect{x}+\vect{y}}
                    &amp;=A\left(\vect{x}+\vect{y}\right)&amp;&amp;
                        \text{Definition of }T</mrow>
                    <mrow>&amp;=A\vect{x}+A\vect{y}&amp;&amp;
                        <xref ref="theorem-MMDAA" acro="MMDAA"/></mrow>
                    <mrow>&amp;=\lteval{T}{\vect{x}}+\lteval{T}{\vect{y}}&amp;&amp;
                        \text{Definition of }T</mrow>
                    <intertext>and</intertext>
                    <mrow>\lteval{T}{\alpha\vect{x}}
                    &amp;=A\left(\alpha\vect{x}\right)&amp;&amp;
                        \text{Definition of }T</mrow>
                    <mrow>&amp;=\alpha\left(A\vect{x}\right)&amp;&amp;
                        <xref ref="theorem-MMSMM" acro="MMSMM"/></mrow>
                    <mrow>&amp;=\alpha\lteval{T}{\vect{x}}&amp;&amp;
                        \text{Definition of }T</mrow>
                </md>.</p>
                <p>So by <xref ref="definition-LT" acro="LT"/>, <m>T</m> is a linear transformation.</p>
            </proof>
        </theorem>
        <p>So <xref ref="theorem-MBLT" acro="MBLT"/> gives us a rapid way to construct linear transformations.  Grab an <m>m\times n</m> matrix <m>A</m>, define <m>\lteval{T}{\vect{x}}=A\vect{x}</m> and <xref ref="theorem-MBLT" acro="MBLT"/> tells us that <m>T</m> is a linear transformation from <m>\complex{n}</m> to <m>\complex{m}</m>, without any further checking.</p>
        <p>We can turn <xref ref="theorem-MBLT" acro="MBLT"/> around.  You give me a linear transformation and I will give you a matrix.</p>
        <example xml:id="example-MFLT" acro="MFLT">
            <title>Matrix from a linear transformation</title>
            <idx>
                <h>linear transformation</h>
                <h>matrix of</h>
            </idx>
            <p>Define the function <m>\ltdefn{R}{\complex{3}}{\complex{4}}</m> by<me>\lteval{R}{\colvector{x_1\\x_2\\x_3}}=\colvector{2x_1-3x_2+4x_3\\x_1+x_2+x_3\\-x_1+5x_2-3x_3\\x_2-4x_3}</me>.</p>
            <p>You could verify that <m>R</m> is a linear transformation by applying the definition, but we will instead massage the expression defining a typical output until we recognize the form of a known class of linear transformations.  We have<md>
                <mrow>\lteval{R}{\colvector{x_1\\x_2\\x_3}}&amp;=
                \colvector{2x_1-3x_2+4x_3\\x_1+x_2+x_3\\-x_1+5x_2-3x_3\\x_2-4x_3}</mrow>
                <mrow>&amp;=
                \colvector{2x_1\\x_1\\-x_1\\0}+
                \colvector{-3x_2\\x_2\\5x_2\\x_2}+
                \colvector{4x_3\\x_3\\-3x_3\\-4x_3}&amp;&amp;
                    <xref ref="definition-CVA" acro="CVA"/></mrow>
                <mrow>&amp;=
                x_1\colvector{2\\1\\-1\\0}+
                x_2\colvector{-3\\1\\5\\1}+\
                x_3\colvector{4\\1\\-3\\-4}&amp;&amp;
                    <xref ref="definition-CVSM" acro="CVSM"/></mrow>
                <mrow>&amp;=
                \begin{bmatrix}
                2&amp;-3&amp;4\\
                1&amp;1&amp;1\\
                -1&amp;5&amp;-3\\
                0&amp;1&amp;-4
                \end{bmatrix}
                \colvector{x_1\\x_2\\x_3}&amp;&amp;
                    <xref ref="definition-MVP" acro="MVP"/></mrow>
            </md>.</p>
            <p>So if we define the matrix<me>B=
            \begin{bmatrix}
            2&amp;-3&amp;4\\
            1&amp;1&amp;1\\
            -1&amp;5&amp;-3\\
            0&amp;1&amp;-4
            \end{bmatrix}</me>then <m>\lteval{R}{\vect{x}}=B\vect{x}</m>.  By <xref ref="theorem-MBLT" acro="MBLT"/>, we can easily recognize <m>R</m> as a linear transformation since it has the form described in the hypothesis of the theorem.</p>
        </example>
        <p><xref ref="example-MFLT" acro="MFLT"/> was not an accident.  Consider any one of the archetypes where both the domain and codomain are sets of column vectors (Archetype<nbsp/><xref ref="archetype-M" acro="M" text="global"/> through Archetype<nbsp/><xref ref="archetype-R" acro="R" text="global"/>) and you should be able to mimic the previous example.  Here is the theorem, which is notable since it is our first occasion to use the full power of the defining properties of a linear transformation when our hypothesis includes a linear transformation.</p>
        <theorem xml:id="theorem-MLTCV" acro="MLTCV">
            <title>Matrix of a Linear Transformation, Column Vectors</title>
            <idx>
                <h>matrix</h>
                <h>of a linear transformation</h>
            </idx>
            <statement>
                <idx>
                    <h>linear transformation</h>
                    <h>matrix of</h>
                </idx>
                <p>Suppose that <m>\ltdefn{T}{\complex{n}}{\complex{m}}</m> is a linear transformation.  Then there is an <m>m\times n</m> matrix <m>A</m> such that <m>\lteval{T}{\vect{x}}=A\vect{x}</m>.</p>
            </statement>
            <proof>
                <p>The conclusion says a certain matrix exists.  What better way to prove something exists than to actually build it?  So our proof will be constructive (Proof Technique<nbsp/><xref ref="technique-C" acro="C" text="global"/>), and the procedure that we will use abstractly in the proof can be used concretely in specific examples.</p>
                <p>Let <m>\vectorlist{e}{n}</m> be the columns of the identity matrix of size <m>n</m>, <m>I_n</m> (<xref ref="definition-SUV" acro="SUV"/>).  Evaluate the linear transformation <m>T</m> with each of these standard unit vectors as an input, and record the result.  In other words, define <m>n</m> vectors in <m>\complex{m}</m>, <m>\vect{A}_i</m>, <m>1\leq i\leq n</m> by<me>\vect{A}_i=\lteval{T}{\vect{e}_i}</me>.</p>
                <p>Then package up these vectors as the columns of a matrix<me>A=\matrixcolumns{A}{n}</me>.</p>
                <p>Does <m>A</m> have the desired properties?  First, <m>A</m> is clearly an <m>m\times n</m> matrix.  Then<md>
                <mrow>\lteval{T}{\vect{x}}
                &amp;=\lteval{T}{I_n\vect{x}}&amp;&amp;
                    <xref ref="theorem-MMIM" acro="MMIM"/></mrow>
                <mrow>&amp;=\lteval{T}{\matrixcolumns{e}{n}\vect{x}}&amp;&amp;
                    <xref ref="definition-SUV" acro="SUV"/></mrow>
                <mrow>&amp;=\lteval{T}{
                \vectorentry{\vect{x}}{1}\vect{e}_1+
                \vectorentry{\vect{x}}{2}\vect{e}_2+
                \vectorentry{\vect{x}}{3}\vect{e}_3+
                \cdots+
                \vectorentry{\vect{x}}{n}\vect{e}_n
                }&amp;&amp;
                    <xref ref="definition-MVP" acro="MVP"/></mrow>
                <mrow>&amp;=
                \lteval{T}{\vectorentry{\vect{x}}{1}\vect{e}_1}+
                \lteval{T}{\vectorentry{\vect{x}}{2}\vect{e}_2}+
                \lteval{T}{\vectorentry{\vect{x}}{3}\vect{e}_3}+
                \cdots+
                \lteval{T}{\vectorentry{\vect{x}}{n}\vect{e}_n}&amp;&amp;
                    <xref ref="definition-LT" acro="LT"/></mrow>
                <mrow>&amp;=
                \vectorentry{\vect{x}}{1}\lteval{T}{\vect{e}_1}+
                \vectorentry{\vect{x}}{2}\lteval{T}{\vect{e}_2}+
                \vectorentry{\vect{x}}{3}\lteval{T}{\vect{e}_3}+
                \cdots+
                \vectorentry{\vect{x}}{n}\lteval{T}{\vect{e}_n}&amp;&amp;
                    <xref ref="definition-LT" acro="LT"/></mrow>
                <mrow>&amp;=
                \vectorentry{\vect{x}}{1}{\vect{A}_1}+
                \vectorentry{\vect{x}}{2}{\vect{A}_2}+
                \vectorentry{\vect{x}}{3}{\vect{A}_3}+
                \cdots+
                \vectorentry{\vect{x}}{n}{\vect{A}_n}&amp;&amp;
                    \text{Definition of }\vect{A}_i</mrow>
                <mrow>&amp;=A\vect{x}&amp;&amp;
                    <xref ref="definition-MVP" acro="MVP"/></mrow>
            </md>as desired.</p>
            </proof>
        </theorem>
        <p>So if we were to restrict our study of linear transformations to those where the domain and codomain are both vector spaces of column vectors (<xref ref="definition-VSCV" acro="VSCV"/>), every matrix leads to a linear transformation of this type (<xref ref="theorem-MBLT" acro="MBLT"/>), while every such linear transformation leads to a matrix (<xref ref="theorem-MLTCV" acro="MLTCV"/>).  So matrices and linear transformations are fundamentally the same.  We call the matrix <m>A</m> of <xref ref="theorem-MLTCV" acro="MLTCV"/> the <term>matrix representation</term> of <m>T</m>.</p>
        <p>We have defined linear transformations for more general vector spaces than just <m>\complex{m}</m>. Can we extend this correspondence between linear transformations and matrices to more general linear transformations (more general domains and codomains)?  Yes, and this is the main theme of <xref ref="chapter-R" acro="R"/>.  Stay tuned.  For now, let us illustrate <xref ref="theorem-MLTCV" acro="MLTCV"/> with an example.</p>
        <example xml:id="example-MOLT" acro="MOLT">
            <title>Matrix of a linear transformation</title>
            <idx>
                <h>linear transformation</h>
                <h>matrix of</h>
            </idx>
            <p>Suppose <m>\ltdefn{S}{\complex{3}}{\complex{4}}</m> is defined by<me>\lteval{S}{\colvector{x_1\\x_2\\x_3}}=\colvector{3x_1-2x_2+5x_3\\x_1+x_2+x_3\\9x_1-2x_2+5x_3\\4x_2}</me>.</p>
            <p>Then<md>
                <mrow>\vect{C}_1&amp;=\lteval{S}{\vect{e}_1}=\lteval{S}{\colvector{1\\0\\0}}=\colvector{3\\1\\9\\0}</mrow>
                <mrow>\vect{C}_2&amp;=\lteval{S}{\vect{e}_2}=\lteval{S}{\colvector{0\\1\\0}}=\colvector{-2\\1\\-2\\4}</mrow>
                <mrow>\vect{C}_3&amp;=\lteval{S}{\vect{e}_3}=\lteval{S}{\colvector{0\\0\\1}}=\colvector{5\\1\\5\\0}</mrow>
            </md>so define<me>C=\left[\vect{C}_1|\vect{C}_2|\vect{C}_3\right]=
            \begin{bmatrix}
            3&amp;-2&amp;5\\
            1&amp;1&amp;1\\
            9&amp;-2&amp;5\\
            0&amp;4&amp;0
            \end{bmatrix}</me>and <xref ref="theorem-MLTCV" acro="MLTCV"/> guarantees that <m>\lteval{S}{\vect{x}}=C\vect{x}</m>.</p>
            <p>As an illuminating exercise, repeat the computation of <m>\lteval{S}{\vect{z}}</m> two different ways.  First, return to the definition of <m>S</m> and evaluate <m>\lteval{S}{\vect{z}}</m> directly.  Then do the matrix-vector product <m>C\vect{z}</m>.  In both cases you should obtain the same result, <m>\lteval{S}{\vect{z}}</m>.<md>
                <mrow>\vect{z}&amp;=\colvector{2\\-3\\3}
                    &amp;
                \lteval{S}{\vect{z}}&amp;=\colvector{27\\2\\39\\-12}</mrow>
            </md></p>
        </example>
        <computation xml:id="sage-LTM" acro="LTM">
            <title>Linear Transformations, Matrices</title>
            <idx>
                <h>linear transformation</h>
                <h>matrices</h>
            </idx>
            <p>A second way to build a linear transformation is to use a matrix, as motivated by <xref ref="theorem-MBLT" acro="MBLT"/>.  But there is one caveat.  We have seen that Sage has a preference for rows, so when defining a linear transformation with a product of a matrix and a vector, <em>Sage forms a linear combination of the rows of the matrix with the scalars of the vector</em>.  This is expressed by writing the vector on the left of the matrix, where if we were to interpret the vector as a 1-row matrix, then the definition of matrix multiplication would do the right thing.</p>
            <p>Remember throughout, a linear transformation has very little to do with the mechanism by which we define it.  Whether or not we use matrix multiplication with vectors on the left (Sage internally) or matrix multiplication with vectors on the right (your text), what matters is the <em>function</em> that results.  One concession to the <q>vector on the right</q> approach is that we can tell Sage that we mean for the matrix to define the linear transformation by multiplication with the vector on the right.  Here is our running example again <mdash/> with some explanation following.</p>
            <sage xml:id="sagecell-LTM-1">
                <input>
                A = matrix(QQ, [[-1, 0, 2],
                                [ 1, 3, 7],
                                [ 1, 1, 1],
                                [ 2, 3, 5]])
                T = linear_transformation(QQ^3, QQ^4, A, side='right')
                T
                </input>
                <output>
                Vector space morphism represented by the matrix:
                [-1  1  1  2]
                [ 0  3  1  3]
                [ 2  7  1  5]
                Domain: Vector space of dimension 3 over Rational Field
                Codomain: Vector space of dimension 4 over Rational Field
                </output>
            </sage>
            <p>The way <c>T</c> prints reflects the way Sage carries <c>T</c> internally.  But notice that we defined <c>T</c> in a way that is consistent with the text, via the use of the optional <c>side='right'</c> keyword.  If you rework examples from the text, or use Sage to assist with exercises, be sure to remember this option.  In particular, when the matrix is square it might be easy to miss that you have forgotten this option.  Note too that Sage uses a more general term for a linear transformation, <q>vector space morphism.</q>  Just mentally translate from Sage-speak to the terms we use here in the text.</p>
            <p>If the standard way that Sage prints a linear transformation is too confusing, you can get all the relevant information with a handful of commands.</p>
            <sage xml:id="sagecell-LTM-2">
                <input>
                T.matrix(side='right')
                </input>
                                <output>
                [-1  0  2]
                [ 1  3  7]
                [ 1  1  1]
                [ 2  3  5]
                </output>
            </sage>
            <sage xml:id="sagecell-LTM-3">
                <input>
                T.domain()
                </input>
                <output>
                Vector space of dimension 3 over Rational Field
                </output>
            </sage>
            <sage xml:id="sagecell-LTM-4">
                <input>
                T.codomain()
                </input>
                <output>
                Vector space of dimension 4 over Rational Field
                </output>
            </sage>
            <p>So we can build a linear transformation in Sage from a matrix, as promised by <xref ref="theorem-MBLT" acro="MBLT"/>.  Furthermore, <xref ref="theorem-MLTCV" acro="MLTCV"/> says there is a matrix associated with every linear transformation.  This matrix is provided in Sage by the <c>.matrix()</c> method, where we use the option <c>side='right'</c> to be consistent with the text.  Here is <xref ref="example-MOLT" acro="MOLT"/> reprised, where we define the linear transformation via a Sage symbolic function and then recover the matrix of the linear transformation.</p>
            <sage xml:id="sagecell-LTM-5">
                <input>
                x1, x2, x3 = var('x1, x2, x3')
                outputs = [3*x1 - 2*x2 + 5*x3,
                             x1 +   x2 +   x3,
                           9*x1 - 2*x2 + 5*x3,
                                  4*x2       ]
                S_symbolic(x1, x2, x3) = outputs
                S = linear_transformation(QQ^3, QQ^4, S_symbolic)
                C = S.matrix(side='right'); C
                </input>
                <output>
                [ 3 -2  5]
                [ 1  1  1]
                [ 9 -2  5]
                [ 0  4  0]
                </output>
            </sage>
            <sage xml:id="sagecell-LTM-6">
                <input>
                x = vector(QQ, [2, -3, 3])
                S(x) == C*x
                </input>
                <output>
                True
                </output>
            </sage>
        </computation>
    </subsection>
    <subsection xml:id="subsection-LT-LTLC" acro="LTLC">
        <title>Linear Transformations and Linear Combinations</title>
        <p>It is the interaction between linear transformations and linear combinations that lies at the heart of many of the important theorems of linear algebra.  The next theorem distills the essence of this.  The proof is not deep, the result is hardly startling, but it will be referenced frequently.  We have already passed by one occasion to employ it, in the proof of <xref ref="theorem-MLTCV" acro="MLTCV"/>.  Paraphrasing, this theorem says that we can <q>push</q> linear transformations <q>down into</q> linear combinations, or <q>pull</q> linear transformations <q>up out</q> of linear combinations.  We will have opportunities to both push and pull.</p>
        <theorem xml:id="theorem-LTLC" acro="LTLC">
            <title>Linear Transformations and Linear Combinations</title>
            <idx>
                <h>linear transformation</h>
                <h>linear combination</h>
            </idx>
            <statement>
                <idx>
                    <h>linear combination</h>
                    <h>linear transformation</h>
                </idx>
                <p>Suppose that <m>\ltdefn{T}{U}{V}</m> is a linear transformation, <m>\vectorlist{u}{t}</m> are vectors from <m>U</m> and <m>\scalarlist{a}{t}</m> are scalars from <m>\complexes</m>.  Then<me>\lteval{T}{\lincombo{a}{u}{t}}
                =
                a_1\lteval{T}{\vect{u}_1}+
                a_2\lteval{T}{\vect{u}_2}+
                a_3\lteval{T}{\vect{u}_3}+\cdots+
                a_t\lteval{T}{\vect{u}_t}</me>.</p>
            </statement>
            <proof>
                <p>We have<md>
                    <mrow>&amp;\lteval{T}{\lincombo{a}{u}{t}}</mrow>
                    <mrow>&amp;\quad\quad=
                    \lteval{T}{a_1\vect{u}_1}+
                    \lteval{T}{a_2\vect{u}_2}+
                    \lteval{T}{a_3\vect{u}_3}+\cdots+
                    \lteval{T}{a_t\vect{u}_t}&amp;&amp;
                        <xref ref="definition-LT" acro="LT"/></mrow>
                    <mrow>&amp;\quad\quad=
                    a_1\lteval{T}{\vect{u}_1}+
                    a_2\lteval{T}{\vect{u}_2}+
                    a_3\lteval{T}{\vect{u}_3}+\cdots+
                    a_t\lteval{T}{\vect{u}_t}&amp;&amp;
                        <xref ref="definition-LT" acro="LT"/></mrow>
                </md>.</p>
            </proof>
        </theorem>
        <p>Some authors, especially in more advanced texts, take the conclusion of <xref ref="theorem-LTLC" acro="LTLC"/> as the defining condition of a linear transformation.  This has the appeal of being a single condition, rather than the two-part condition of <xref ref="definition-LT" acro="LT"/>.  (See <xref ref="exercise-LT-T20" acro="LT.T20"/>).</p>
        <p>Our next theorem says, informally, that it is enough to know how a linear transformation behaves for inputs from any basis of the domain, and <em>all</em> the other outputs are described by a linear combination of these few values.  Again, the statement of the theorem, and its proof, are not remarkable, but the insight that goes along with it is very fundamental.</p>
        <theorem xml:id="theorem-LTDB" acro="LTDB">
            <title>Linear Transformation Defined on a Basis</title>
            <idx>
                <h>linear transformation</h>
                <h>defined on a basis</h>
            </idx>
            <statement>
                <p>Suppose <m>U</m> is a vector space with basis <m>B=\set{\vectorlist{u}{n}}</m> and the vector space <m>V</m> contains the vectors <m>\vectorlist{v}{n}</m> (which may not be distinct).   Then there is a unique linear transformation, <m>\ltdefn{T}{U}{V}</m>, such that <m>\lteval{T}{\vect{u}_i}=\vect{v}_i</m>, <m>1\leq i\leq n</m>.</p>
            </statement>
            <proof>
                <p>To prove the existence of <m>T</m>, we construct a function and show that it is a linear transformation (Proof Technique<nbsp/><xref ref="technique-C" acro="C" text="global"/>).  Suppose <m>\vect{w}\in U</m> is an arbitrary element of the domain.  Then by <xref ref="theorem-VRRB" acro="VRRB"/> there are unique scalars <m>\scalarlist{a}{n}</m> such that<me>\vect{w}=\lincombo{a}{u}{n}</me>.</p>
                <p>Then <em>define</em> the function <m>T</m> by<me>\lteval{T}{\vect{w}}=\lincombo{a}{v}{n}</me>.</p>
                <p>It should be clear that <m>T</m> behaves as required for <m>n</m> inputs from <m>B</m>.  Since the scalars provided by <xref ref="theorem-VRRB" acro="VRRB"/> are unique, there is no ambiguity in this definition, and <m>T</m> qualifies as a function with domain <m>U</m> and codomain <m>V</m> (<ie/> <m>T</m> is well-defined).  But is <m>T</m> a linear transformation as well?</p>
                <p>Let <m>\vect{x}\in U</m> be a second element of the domain, and suppose the scalars provided by <xref ref="theorem-VRRB" acro="VRRB"/> (relative to <m>B</m>) are <m>\scalarlist{b}{n}</m>.  Then<md>
                    <mrow>\lteval{T}{\vect{w}+\vect{x}}&amp;=
                    \lteval{T}{
                    a_1\vect{u}_1+
                    \cdots+
                    a_n\vect{u}_n+
                    b_1\vect{u}_1+
                    \cdots+
                    b_n\vect{u}_n
                    }</mrow>
                    <mrow>&amp;=
                    \lteval{T}{
                    \left(a_1+b_1\right)\vect{u}_1+
                    \cdots+
                    \left(a_n+b_n\right)\vect{u}_n
                    }&amp;&amp;
                        <xref ref="definition-VS" acro="VS"/></mrow>
                    <mrow>&amp;=
                    \left(a_1+b_1\right)\vect{v}_1+
                    \cdots+
                    \left(a_n+b_n\right)\vect{v}_n&amp;&amp;
                        \text{Definition of }T</mrow>
                    <mrow>&amp;=
                    a_1\vect{v}_1+
                    \cdots+
                    a_n\vect{v}_n+
                    b_1\vect{v}_1+
                    \cdots+
                    b_n\vect{v}_n&amp;&amp;
                        <xref ref="definition-VS" acro="VS"/></mrow>
                    <mrow>&amp;=\lteval{T}{\vect{w}}+\lteval{T}{\vect{x}}</mrow>
                </md>.</p>
                <p>Let <m>\alpha\in\complexes</m> be any scalar.  Then<md>
                    <mrow>\lteval{T}{\alpha\vect{w}}&amp;=
                    \lteval{T}{\alpha\left(\lincombo{a}{u}{n}\right)}</mrow>
                    <mrow>&amp;=
                    \lteval{T}{\lincombo{\alpha a}{u}{n}}&amp;&amp;
                        <xref ref="definition-VS" acro="VS"/></mrow>
                    <mrow>&amp;=\lincombo{\alpha a}{v}{n}&amp;&amp;
                        \text{Definition of }T</mrow>
                    <mrow>&amp;=\alpha\left(\lincombo{a}{v}{n}\right)&amp;&amp;
                        <xref ref="definition-VS" acro="VS"/></mrow>
                    <mrow>&amp;=\alpha\lteval{T}{\vect{w}}</mrow>
                </md>.</p>
                <p>So by <xref ref="definition-LT" acro="LT"/>, <m>T</m> is a linear transformation.</p>
                <p>Is <m>T</m> unique (among all linear transformations that take the <m>\vect{u}_i</m> to the <m>\vect{v}_i</m>)?  Applying Proof Technique<nbsp/><xref ref="technique-U" acro="U" text="global"/>, we posit the existence of a second linear transformation, <m>\ltdefn{S}{U}{V}</m> such that <m>\lteval{S}{\vect{u}_i}=\vect{v}_i</m>, <m>1\leq i\leq n</m>.  Again, let <m>\vect{w}\in U</m> represent an arbitrary element of <m>U</m> and let <m>\scalarlist{a}{n}</m> be the scalars provided by <xref ref="theorem-VRRB" acro="VRRB"/> (relative to <m>B</m>).  We have,<md>
                    <mrow>\lteval{T}{\vect{w}}&amp;=
                    \lteval{T}{\lincombo{a}{u}{n}}&amp;&amp;
                        <xref ref="theorem-VRRB" acro="VRRB"/></mrow>
                    <mrow>&amp;=
                    a_1\lteval{T}{\vect{u}_1}+
                    a_2\lteval{T}{\vect{u}_2}+
                    a_3\lteval{T}{\vect{u}_3}+
                    \cdots+
                    a_n\lteval{T}{\vect{u}_n}&amp;&amp;
                        <xref ref="theorem-LTLC" acro="LTLC"/></mrow>
                    <mrow>&amp;=
                    a_1\vect{v}_1+
                    a_2\vect{v}_2+
                    a_3\vect{v}_3+
                    \cdots+
                    a_n\vect{v}_n&amp;&amp;
                        \text{Definition of }T</mrow>
                    <mrow>&amp;=
                    a_1\lteval{S}{\vect{u}_1}+
                    a_2\lteval{S}{\vect{u}_2}+
                    a_3\lteval{S}{\vect{u}_3}+
                    \cdots+
                    a_n\lteval{S}{\vect{u}_n}&amp;&amp;
                        \text{Definition of }S</mrow>
                    <mrow>&amp;=
                    \lteval{S}{\lincombo{a}{u}{n}}&amp;&amp;
                        <xref ref="theorem-LTLC" acro="LTLC"/></mrow>
                    <mrow>&amp;=
                    \lteval{S}{\vect{w}}&amp;&amp;
                        <xref ref="theorem-VRRB" acro="VRRB"/></mrow>
                </md>.</p>
                <p>So the output of <m>T</m> and <m>S</m> agree on every input, which means they are equal as functions, <m>T=S</m>.  So <m>T</m> is unique.</p>
            </proof>
        </theorem>
        <p>You might recall facts from analytic geometry, such as <q>any two points determine a line</q> and <q>any three non-collinear points determine a parabola.</q>  <xref ref="theorem-LTDB" acro="LTDB"/> has much of the same feel.  By specifying the <m>n</m> outputs for inputs from a basis, an entire linear transformation is determined.  The analogy is not perfect, but the style of these facts are not very dissimilar from <xref ref="theorem-LTDB" acro="LTDB"/>.</p>
        <p>Notice that the statement of <xref ref="theorem-LTDB" acro="LTDB"/> asserts the <em>existence</em> of a linear transformation with certain properties, while the proof shows us exactly how to define the desired linear transformation. The next two examples show how to compute values of linear transformations that we create this way.</p>
        <example xml:id="example-LTDB1" acro="LTDB1">
            <title>Linear transformation defined on a basis</title>
            <idx>
                <h>linear transformation</h>
                <h>defined on a basis</h>
            </idx>
            <p>Consider the linear transformation <m>\ltdefn{T}{\complex{3}}{\complex{2}}</m> that is required to have the following three values,<md>
                <mrow>\lteval{T}{\colvector{1\\0\\0}}=\colvector{2\\1}&amp;&amp;
                \lteval{T}{\colvector{0\\1\\0}}=\colvector{-1\\4}&amp;&amp;
                \lteval{T}{\colvector{0\\0\\1}}=\colvector{6\\0}</mrow>
            </md>.</p>
            <p>Because<me>B=\set{
            \colvector{1\\0\\0},\,
            \colvector{0\\1\\0},\,
            \colvector{0\\0\\1}
            }</me>is a basis for <m>\complex{3}</m> (<xref ref="theorem-SUVB" acro="SUVB"/>), <xref ref="theorem-LTDB" acro="LTDB"/> says there is a unique linear transformation <m>T</m> that behaves this way.</p>
            <p>How do we compute other values of <m>T</m>?  Consider the input<me>\vect{w}=\colvector{2\\-3\\1}=(2)\colvector{1\\0\\0}+(-3)\colvector{0\\1\\0}+(1)\colvector{0\\0\\1}</me>.</p>
            <p>Then<me>\lteval{T}{\vect{w}}=(2)\colvector{2\\1}+ (-3)\colvector{-1\\4}+ (1)\colvector{6\\0}=\colvector{13\\-10}</me>.</p>
            <p>Doing it again,<me>\vect{x}=\colvector{5\\2\\-3}=(5)\colvector{1\\0\\0}+(2)\colvector{0\\1\\0}+(-3)\colvector{0\\0\\1}</me>so<me>\lteval{T}{\vect{x}}=(5)\colvector{2\\1}+ (2)\colvector{-1\\4}+ (-3)\colvector{6\\0}=\colvector{-10\\13}</me>.</p>
            <p>Any other value of <m>T</m> could be computed in a similar manner.  So rather than being given a <em>formula</em> for the outputs of <m>T</m>, the <em>requirement</em> that <m>T</m> behave in a certain way for the inputs chosen from a basis of the domain, is as sufficient as a formula for computing any value of the function.  You might notice some parallels between this example and <xref ref="example-MOLT" acro="MOLT"/> or <xref ref="theorem-MLTCV" acro="MLTCV"/>.</p>
        </example>
        <example xml:id="example-LTDB2" acro="LTDB2">
            <title>Linear transformation defined on a basis</title>
            <idx>
                <h>linear transformation</h>
                <h>defined on a basis</h>
            </idx>
            <p>Consider the linear transformation <m>\ltdefn{R}{\complex{3}}{\complex{2}}</m> with the three values,<md>
                <mrow>\lteval{R}{\colvector{1\\2\\1}}=\colvector{5\\-1}&amp;&amp;
                \lteval{R}{\colvector{-1\\5\\1}}=\colvector{0\\4}&amp;&amp;
                \lteval{R}{\colvector{3\\1\\4}}=\colvector{2\\3}</mrow>
            </md>.</p>
            <p>You can check that<me>D=\set{
            \colvector{1\\2\\1},\,
            \colvector{-1\\5\\1},\,
            \colvector{3\\1\\4}
            }</me>is a basis for <m>\complex{3}</m> (make the vectors the columns of a square matrix and check that the matrix is nonsingular,  <xref ref="theorem-CNMB" acro="CNMB"/>).  By <xref ref="theorem-LTDB" acro="LTDB"/> we know there is a unique linear transformation <m>R</m> with the three specified outputs.  However, we have to work just a bit harder to take an input vector and express it as a linear combination of the vectors in <m>D</m>.</p>
            <p>For example, consider,<me>\vect{y}=\colvector{8\\-3\\5}</me>.</p>
            <p>Then we must first write <m>\vect{y}</m> as a linear combination of the vectors in <m>D</m> and solve for the unknown scalars, to arrive at<me>\vect{y}=\colvector{8\\-3\\5}= (3)\colvector{1\\2\\1}+ (-2)\colvector{-1\\5\\1}+ (1)\colvector{3\\1\\4}</me>.</p>
            <p>Then the proof of <xref ref="theorem-LTDB" acro="LTDB"/> gives us<me>\lteval{R}{\vect{y}}=(3)\colvector{5\\-1}+ (-2)\colvector{0\\4}+ (1)\colvector{2\\3}= \colvector{17\\-8}</me>.</p>
            <p>Any other value of <m>R</m> could be computed in a similar manner.</p>
        </example>
        <p>Here is a third example of a linear transformation defined by its action on a basis, only with more abstract vector spaces involved.</p>
        <example xml:id="example-LTDB3" acro="LTDB3">
            <title>Linear transformation defined on a basis</title>
            <idx>
                <h>linear transformation</h>
                <h>defined on a basis</h>
            </idx>
            <p>The set <me>W=\set{p(x)\in P_3\mid p(1)=0, p(3)=0}\subseteq P_3</me> is a subspace of the vector space of polynomials <m>P_3</m>.  This subspace has <me>C=\set{3-4x+x^2,\,12-13x+x^3}</me> as a basis (check this!).  Suppose we consider the linear transformation <m>\ltdefn{S}{P_3}{M_{22}}</m> with values<md>
                <mrow>\lteval{S}{3-4x+x^2}=\begin{bmatrix}1&amp;-3\\2&amp;0\end{bmatrix}&amp;&amp;
                \lteval{S}{12-13x+x^3}=\begin{bmatrix}0&amp;1\\1&amp;0\end{bmatrix}</mrow>
            </md>.</p>
            <p>By <xref ref="theorem-LTDB" acro="LTDB"/> we know there is a unique linear transformation with these two values.  To illustrate a sample computation of <m>S</m>, consider <m>q(x)=9-6x-5x^2+2x^3</m>.  Verify that <m>q(x)</m> is an element of <m>W</m> (does it have roots at <m>x=1</m> and <m>x=3</m>?), then find the scalars needed to write it as a linear combination of the basis vectors in <m>C</m>.  Because<me>q(x)=9-6x-5x^2+2x^3=(-5)(3-4x+x^2)+(2)(12-13x+x^3)</me>.</p>
            <p>The proof of <xref ref="theorem-LTDB" acro="LTDB"/> gives us<me>\lteval{S}{q}=(-5)\begin{bmatrix}1&amp;-3\\2&amp;0\end{bmatrix}
            +
            (2)\begin{bmatrix}0&amp;1\\1&amp;0\end{bmatrix}
            =
            \begin{bmatrix}-5&amp;17\\-8&amp;0\end{bmatrix}</me>.</p>
            <p>And all the other outputs of <m>S</m> could be computed in the same manner.  Every output of <m>S</m> will have a zero in the second row, second column.  Can you see why this is so?</p>
        </example>
        <p>Informally, we can describe <xref ref="theorem-LTDB" acro="LTDB"/> by saying <q>it is enough to know what a linear transformation does to a basis (of the domain).</q></p>
        <computation xml:id="sage-LTB" acro="LTB">
            <title>Linear Transformations, Bases</title>
            <idx><h>linear transformation</h><h>bases</h></idx>
            <p>A third way to create a linear transformation in Sage is to provide a list of images for a basis, as motivated by <xref ref="theorem-LTDB" acro="LTDB"/>.  The default is to use the standard basis as the inputs (<xref ref="definition-SUV" acro="SUV"/>).  We will, once again, create our running example.</p>
            <sage xml:id="sagecell-LTB-1">
                <input>
                U = QQ^3
                V = QQ^4
                v1 = vector(QQ, [-1, 1, 1, 2])
                v2 = vector(QQ, [ 0, 3, 1, 3])
                v3 = vector(QQ, [ 2, 7, 1, 5])
                T = linear_transformation(U, V, [v1, v2, v3])
                T
                </input>
                <output>
                Vector space morphism represented by the matrix:
                [-1  1  1  2]
                [ 0  3  1  3]
                [ 2  7  1  5]
                Domain: Vector space of dimension 3 over Rational Field
                Codomain: Vector space of dimension 4 over Rational Field
                </output>
            </sage>
            <p>Notice that there is no requirement that the list of images (in Sage or in <xref ref="theorem-LTDB" acro="LTDB"/>) is a basis.  They do not even have to be different.  They could all be the zero vector (try it).</p><p>If we want to use an alternate basis for the domain, it is possible, but there are two caveats.  The first caveat is that we must be sure to provide a basis for the domain, Sage will give an error if the proposed basis is not linearly independent and we are responsible for providing the right number of vectors (which should be easy).</p>
            <p>We have seen that vector spaces can have alternate bases, which print as a <q>user basis.</q>  Here will provide the domain with an alternate basis.  The relevant command will create a subspace, but for now, we need to provide a big enough set to create the entire domain.  It is possible to use fewer linearly independent vectors, and create a proper subspace, but then we will not be able to use this proper subspace to build the linear transformation we want.</p>
            <sage xml:id="sagecell-LTB-2">
                <input>
                u1 = vector(QQ, [ 1,  3, -4])
                u2 = vector(QQ, [-1, -2,  3])
                u3 = vector(QQ, [ 1,  1, -3])
                U = (QQ^3).subspace_with_basis([u1, u2, u3])
                U == QQ^3
                </input>
                <output>
                True
                </output>
            </sage>
            <sage xml:id="sagecell-LTB-3">
                <input>
                U.basis_matrix()
                </input>
                <output>
                [ 1  3 -4]
                [-1 -2  3]
                [ 1  1 -3]
                </output>
            </sage>
            <sage xml:id="sagecell-LTB-4">
                <input>
                U.echelonized_basis_matrix()
                </input>
                <output>
                [1 0 0]
                [0 1 0]
                [0 0 1]
                </output>
            </sage>
            <p>We can use this alternate version of <c>U</c> to create a linear transformation from specified images.  Superficially there is nothing real special about our choices for <c>v1, v2, v3</c>.</p>
            <sage xml:id="sagecell-LTB-5">
                <input>
                V = QQ^4
                v1 = vector(QQ, [-9, -18,  0, -9])
                v2 = vector(QQ, [ 7,  14,  0,  7])
                v3 = vector(QQ, [-7, -17, -1, -10])
                </input>
            </sage>
            <p>Now we create the linear transformation.  Here is the second caveat:  the matrix of the linear transformation is no longer that provided by <xref ref="theorem-MLTCV" acro="MLTCV"/>.  It may be obvious where the matrix comes from, but a full understanding of its interpretation will have to wait until <xref ref="section-MR" acro="MR"/>.</p>
            <sage xml:id="sagecell-LTB-6">
                <input>
                S = linear_transformation(U, V, [v1, v2, v3])
                S.matrix(side='right')
                </input>
                <output>
                [ -9   7  -7]
                [-18  14 -17]
                [  0   0  -1]
                [ -9   7 -10]
                </output>
            </sage>
            <p>We suggested our choices for <c>v1, v2, v3</c> were <q>random.</q>  Not so <mdash/> the linear transformation <c>S</c> just created is equal to the linear transformation <c>T</c> above.  If you have run all the input in this subsection, in order, then you should be able to compare the <em>functions</em> <c>S</c> and <c>T</c>.  The next command should <em>always</em> produce <c>True</c>.</p>
            <sage xml:id="sagecell-LTB-7">
                <input>
                u = random_vector(QQ, 3)
                T(u) == S(u)
                </input>
                <output>
                True
                </output>
            </sage>
            <p>Notice that <c>T == S</c> may not do what you expect here.  Instead, the linear transformation method <c>.is_equal_function()</c> will perform a conclusive check of equality of two linear transformations as functions.</p>
            <sage xml:id="sagecell-LTB-8">
                <input>
                T.is_equal_function(S)
                </input>
                <output>
                True
                </output>
            </sage>
            <p>Can you reproduce this example?  In other words, define some linear transformation, any way you like.  Then give the domain an alternate basis and concoct the correct images to create a second linear transformation (by the method of this subsection) which is equal to the first.</p>
        </computation>
    </subsection>
    <subsection xml:id="subsection-LT-PI" acro="PI">
        <title>Pre-Images</title>
        <p>The definition of a function requires that for each input in the domain there is <em>exactly</em> one output in the codomain.  However, the correspondence does not have to behave the other way around.  An output from the codomain could have many different inputs from the domain which the transformation sends to that output, or there could be no inputs at all which the transformation sends to that output.  To formalize our discussion of this aspect of linear transformations, we define the pre-image.</p>
        <definition xml:id="definition-PI" acro="PI">
            <title>Pre-Image</title>
            <idx>pre-image</idx>
            <statement>
                <p>Suppose that <m>\ltdefn{T}{U}{V}</m> is a linear transformation.  For each <m>\vect{v}</m>, define the <term>pre-image</term> of <m>\vect{v}</m> to be the subset of <m>U</m> given by<me>\preimage{T}{\vect{v}}=\setparts{\vect{u}\in U}{\lteval{T}{\vect{u}}=\vect{v}}</me>.</p>
            </statement>
        </definition>
        <p>In other words, <m>\preimage{T}{\vect{v}}</m> is the set of all those vectors in the domain <m>U</m> that get <q>sent</q> to the vector <m>\vect{v}</m>.</p>
        <!--  TODO:  All preimages form a partition of <m>U</m>, an equivalence relation is about.  Maybe to exercises. -->
        <example xml:id="example-SPIAS" acro="SPIAS">
            <title>Sample pre-images, Archetype S</title>
            <idx>pre-images</idx>
            <!-- Archetype S, Part ltdefn -->
            <p>Archetype<nbsp/><xref ref="archetype-S" acro="S" text="global"/> is the linear transformation defined by<me>\ltdefn{T}{\complex{3}}{M_{22}},\quad
            \lteval{T}{\colvector{a\\b\\c}}=
            \begin{bmatrix}
            a-b&amp;2a+2b+c\\
            3a+b+c&amp;-2a-6b-2c
            \end{bmatrix}</me>.</p>
            <p>We could compute a pre-image for every element of the codomain <m>M_{22}</m>.  However, even in a free textbook, we do not have the room to do that, so we will compute just two.</p>
            <p>Choose<me>\vect{v}=
            \begin{bmatrix}
            2&amp;1\\3&amp;2
            \end{bmatrix}
            \in M_{22}</me>for no particular reason.  What is <m>\preimage{T}{\vect{v}}</m>?  Suppose <m>\vect{u}=\colvector{u_1\\u_2\\u_3}\in\preimage{T}{\vect{v}}</m>.  The condition that <m>\lteval{T}{\vect{u}}=\vect{v}</m> becomes<me>\begin{bmatrix}
            2&amp;1\\3&amp;2
            \end{bmatrix}
            =\vect{v}
            =\lteval{T}{\vect{u}}
            =\lteval{T}{\colvector{u_1\\u_2\\u_3}}\\
            =\begin{bmatrix}
            u_1-u_2&amp;2u_1+2u_2+u_3\\
            3u_1+u_2+u_3&amp;-2u_1-6u_2-2u_3
            \end{bmatrix}</me>.</p>
            <p>Using matrix equality (<xref ref="definition-ME" acro="ME"/>), we arrive at a system of four equations in the three unknowns <m>u_1,\,u_2,\,u_3</m> with an augmented matrix that we can row-reduce in the hunt for solutions,<me>\begin{bmatrix}
            1 &amp; -1 &amp; 0 &amp; 2\\
            2 &amp; 2 &amp; 1 &amp; 1\\
            3 &amp; 1 &amp; 1 &amp; 3\\
            -2 &amp; -6 &amp; -2 &amp; 2
            \end{bmatrix}
            \rref
            \begin{bmatrix}
            \leading{1} &amp; 0 &amp; \frac{1}{4} &amp;  \frac{5}{4}\\
            0 &amp; \leading{1} &amp; \frac{1}{4} &amp;  -\frac{3}{4}\\
            0 &amp; 0 &amp; 0 &amp;  0\\
            0 &amp; 0 &amp; 0 &amp;  0
            \end{bmatrix}</me>.</p>
            <p>We recognize this system as having infinitely many solutions described by the single free variable <m>u_3</m>.  Eventually obtaining the vector form of the solutions (<xref ref="theorem-VFSLS" acro="VFSLS"/>), we can describe the preimage precisely as,<md>
                <mrow>\preimage{T}{\vect{v}}&amp;=\setparts{\vect{u}\in\complex{3}}{\lteval{T}{\vect{u}}=\vect{v}}</mrow>
                <mrow>&amp;=\setparts{\colvector{u_1\\u_2\\u_3}}{u_1=\frac{5}{4}-\frac{1}{4}u_3,\,u_2=-\frac{3}{4}-\frac{1}{4}u_3}</mrow>
                <mrow>&amp;=\setparts{\colvector{\frac{5}{4}-\frac{1}{4}u_3\\-\frac{3}{4}-\frac{1}{4}u_3\\u_3}}{u_3\in\complexes}</mrow>
                <mrow>&amp;=\setparts{\colvector{\frac{5}{4}\\-\frac{3}{4}\\0}+u_3\colvector{-\frac{1}{4}\\-\frac{1}{4}\\1}}{u_3\in\complexes}</mrow>
                <mrow>&amp;=\colvector{\frac{5}{4}\\-\frac{3}{4}\\0}+\spn{\set{\colvector{-\frac{1}{4}\\-\frac{1}{4}\\1}}}</mrow>
            </md>.</p>
            <p>This last line is merely a suggestive way of describing the set on the previous line.  You might create three or four vectors in the preimage, and evaluate <m>T</m> with each.  Was the result what you expected?  For a hint of things to come, you might try evaluating <m>T</m> with just the lone vector in the spanning set above.  What was the result?  Now take a look back at <xref ref="theorem-PSPHS" acro="PSPHS"/>.  Hmmmm.</p>
            <p>OK, let us compute another preimage, but with a different outcome this time.Choose<me>\vect{v}=
            \begin{bmatrix}
            1&amp;1\\2&amp;4
            \end{bmatrix}
            \in M_{22}</me>.</p>
            <p>What is <m>\preimage{T}{\vect{v}}</m>?  Suppose <m>\vect{u}=\colvector{u_1\\u_2\\u_3}\in\preimage{T}{\vect{v}}</m>.  That <m>\lteval{T}{\vect{u}}=\vect{v}</m> becomes<me>\begin{bmatrix}
            1&amp;1\\2&amp;4
            \end{bmatrix}
            =\vect{v}
            =\lteval{T}{\vect{u}}
            =\lteval{T}{\colvector{u_1\\u_2\\u_3}}\\
            =\begin{bmatrix}
            u_1-u_2&amp;2u_1+2u_2+u_3\\
            3u_1+u_2+u_3&amp;-2u_1-6u_2-2u_3
            \end{bmatrix}</me>.</p>
            <p>Using matrix equality (<xref ref="definition-ME" acro="ME"/>), we arrive at a system of four equations in the three unknowns <m>u_1,\,u_2,\,u_3</m> with an augmented matrix that we can row-reduce in the hunt for solutions,<me>\begin{bmatrix}
            1 &amp; -1 &amp; 0 &amp; 1\\
            2 &amp; 2 &amp; 1 &amp; 1\\
            3 &amp; 1 &amp; 1 &amp; 2\\
            -2 &amp; -6 &amp; -2 &amp; 4
            \end{bmatrix}
            \rref
            \begin{bmatrix}
            \leading{1} &amp; 0 &amp; \frac{1}{4} &amp;  0\\
            0 &amp; \leading{1} &amp; \frac{1}{4} &amp;  0\\
            0 &amp; 0 &amp; 0 &amp;  \leading{1}\\
            0 &amp; 0 &amp; 0 &amp;  0
            \end{bmatrix}</me>.</p>
            <p>By <xref ref="theorem-RCLS" acro="RCLS"/> we recognize this system as inconsistent.  So no vector <m>\vect{u}</m> is a member of <m>\preimage{T}{\vect{v}}</m> and so<me>\preimage{T}{\vect{v}}=\emptyset</me>.</p>
        </example>
        <p>The preimage is just a set, it is almost never a subspace of <m>U</m> (you might think about just when <m>\preimage{T}{\vect{v}}</m> is a subspace, see <xref ref="exercise-ILT-T10" acro="ILT.T10"/>).  We will describe its properties going forward, and it will be central to the main ideas of this chapter.</p>
        <computation xml:id="sage-PI" acro="PI">
            <title>Pre-Images</title>
            <idx>pre-images</idx>
            <p>Sage handles pre-images just a bit differently than our approach in the text.  For the moment, we can obtain a single vector in the set that is the pre-image via the <c>.preimage_representative()</c> method.  Understand that this method will return <em>just one</em> element of the pre-image set, and we have no real control over which one.  Also, it is certainly possible that a pre-image is the empty set <mdash/> in this case, the method will raise a <c>ValueError</c>.  We will use our running example to illustrate.</p>
            <sage xml:id="sagecell-PI-1">
                <input>
                A = matrix(QQ, [[-1, 0, 2],
                                [ 1, 3, 7],
                                [ 1, 1, 1],
                                [ 2, 3, 5]])
                T = linear_transformation(QQ^3, QQ^4, A, side='right')
                v = vector(QQ, [1, 2, 0, 1])
                u = T.preimage_representative(v)
                u
                </input>
                <output>
                (-1, 1, 0)
                </output>
            </sage>
            <sage xml:id="sagecell-PI-2">
                <input>
                T(u) == v
                </input>
                <output>
                True
                </output>
            </sage>
            <sage xml:id="sagecell-PI-3">
                <input>
                T.preimage_representative(vector(QQ, [1, 2, 1, 1]))
                </input>
                <output>
                Traceback (most recent call last):
                ...
                ValueError: element is not in the image
                </output>
            </sage>
            <p>Remember, we have defined the pre-image as a set, and Sage just gives us a single element of the set.  We will see in Sage<nbsp/><xref ref="sage-ILT" acro="ILT" text="global"/> that the upcoming <xref ref="theorem-KPI" acro="KPI"/> explains why this is no great shortcoming in Sage.</p>
        </computation>
    </subsection>
    <subsection xml:id="subsection-LT-NLTFO" acro="NLTFO">
        <title>New Linear Transformations From Old</title>
        <p>We can combine linear transformations in natural ways to create new linear transformations.  So we will define these combinations and then prove that the results really are still linear transformations.  First the sum of two linear transformations.</p>
        <definition xml:id="definition-LTA" acro="LTA">
            <title>Linear Transformation Addition</title>
            <idx>
                <h>linear transformation</h>
                <h>addition</h>
            </idx>
            <statement>
                <p>Suppose that <m>\ltdefn{T}{U}{V}</m> and <m>\ltdefn{S}{U}{V}</m> are two linear transformations with the same domain and codomain.  Then their <term>sum</term> is the function <m>\ltdefn{T+S}{U}{V}</m> whose outputs are defined by<me>\lteval{(T+S)}{\vect{u}}=\lteval{T}{\vect{u}}+\lteval{S}{\vect{u}}</me>.</p>
            </statement>
        </definition>
        <p>Notice that the first plus sign in the definition is the operation being defined, while the second one is the vector addition in <m>V</m>.  (Vector addition in <m>U</m> will appear just now in the proof that <m>T+S</m> is a linear transformation.)  <xref ref="definition-LTA" acro="LTA"/> only provides a function.  It would be nice to know that when the constituents (<m>T</m>, <m>S</m>) are linear transformations, then so too is <m>T+S</m>.</p>
        <theorem xml:id="theorem-SLTLT" acro="SLTLT">
            <title>Sum of Linear Transformations is a Linear Transformation</title>
            <idx>
                <h>linear transformation</h>
                <h>addition</h>
            </idx>
            <statement>
                <p>Suppose that <m>\ltdefn{T}{U}{V}</m> and <m>\ltdefn{S}{U}{V}</m> are two linear transformations with the same domain and codomain.  Then <m>\ltdefn{T+S}{U}{V}</m> is a linear transformation.</p>
            </statement>
            <proof>
                <p>We simply check the defining properties of a linear transformation (<xref ref="definition-LT" acro="LT"/>).  This is a good place to consistently ask yourself which objects are being combined with which operations.  We have<md>
                    <mrow>\lteval{(T+S)}{\vect{x}+\vect{y}}&amp;=
                    \lteval{T}{\vect{x}+\vect{y}}+\lteval{S}{\vect{x}+\vect{y}}&amp;&amp;
                        <xref ref="definition-LTA" acro="LTA"/></mrow>
                    <mrow>&amp;=\lteval{T}{\vect{x}}+\lteval{T}{\vect{y}}+\lteval{S}{\vect{x}}+\lteval{S}{\vect{y}}&amp;&amp;
                        <xref ref="definition-LT" acro="LT"/></mrow>
                    <mrow>&amp;=\lteval{T}{\vect{x}}+\lteval{S}{\vect{x}}+\lteval{T}{\vect{y}}+\lteval{S}{\vect{y}}&amp;&amp;
                        <xref ref="property-C" acro="C"/>\text{ in }V</mrow>
                    <mrow>&amp;=\lteval{(T+S)}{\vect{x}}+\lteval{(T+S)}{\vect{y}}&amp;&amp;
                        <xref ref="definition-LTA" acro="LTA"/></mrow>
                    <intertext>and</intertext>
                    <mrow>\lteval{(T+S)}{\alpha\vect{x}}&amp;=
                    \lteval{T}{\alpha\vect{x}}+\lteval{S}{\alpha\vect{x}}&amp;&amp;
                        <xref ref="definition-LTA" acro="LTA"/></mrow>
                    <mrow>&amp;=\alpha\lteval{T}{\vect{x}}+\alpha\lteval{S}{\vect{x}}&amp;&amp;
                        <xref ref="definition-LT" acro="LT"/></mrow>
                    <mrow>&amp;=\alpha\left(\lteval{T}{\vect{x}}+\lteval{S}{\vect{x}}\right)&amp;&amp;
                        <xref ref="property-DVA" acro="DVA"/>\text{ in }V</mrow>
                    <mrow>&amp;=\alpha\lteval{(T+S)}{\vect{x}}&amp;&amp;
                        <xref ref="definition-LTA" acro="LTA"/></mrow>
                </md>.</p>
            </proof>
        </theorem>
        <example xml:id="example-STLT" acro="STLT">
            <title>Sum of two linear transformations</title>
            <idx>
                <h>linear transformation</h>
                <h>sum</h>
            </idx>
            <p>Suppose that <m>\ltdefn{T}{\complex{2}}{\complex{3}}</m> and <m>\ltdefn{S}{\complex{2}}{\complex{3}}</m> are defined by<md>
                <mrow>\lteval{T}{\colvector{x_1\\x_2}}=\colvector{x_1+2x_2\\3x_1-4x_2\\5x_1+2x_2}&amp;&amp;
                \lteval{S}{\colvector{x_1\\x_2}}=\colvector{4x_1-x_2\\x_1+3x_2\\-7x_1+5x_2}</mrow>
            </md>.</p>
            <p>Then by <xref ref="definition-LTA" acro="LTA"/>, we have<md>
                <mrow>\lteval{(T+S)}{\colvector{x_1\\x_2}}&amp;=\lteval{T}{\colvector{x_1\\x_2}}+\lteval{S}{\colvector{x_1\\x_2}}</mrow>
                <mrow>&amp;=\colvector{x_1+2x_2\\3x_1-4x_2\\5x_1+2x_2}+\colvector{4x_1-x_2\\x_1+3x_2\\-7x_1+5x_2}
                =\colvector{5x_1+x_2\\4x_1-x_2\\-2x_1+7x_2}</mrow>
            </md>and by <xref ref="theorem-SLTLT" acro="SLTLT"/> we know <m>T+S</m> is also a linear transformation from <m>\complex{2}</m> to <m>\complex{3}</m>.</p>
        </example>
        <definition xml:id="definition-LTSM" acro="LTSM">
            <title>Linear Transformation Scalar Multiplication</title>
            <idx>
                <h>linear transformation</h>
                <h>scalar multiplication</h>
            </idx>
            <statement>
                <p>Suppose that <m>\ltdefn{T}{U}{V}</m> is a linear transformation and <m>\alpha\in\complexes</m>.  Then the <term>scalar multiple</term> is the function <m>\ltdefn{\alpha T}{U}{V}</m> whose outputs are defined by<me>\lteval{(\alpha T)}{\vect{u}}=\alpha\lteval{T}{\vect{u}}</me>.</p>
            </statement>
        </definition>
        <p>Given that <m>T</m> is a linear transformation, it would be nice to know that <m>\alpha T</m> is also a linear transformation.</p>
        <theorem xml:id="theorem-MLTLT" acro="MLTLT">
            <title>Multiple of a Linear Transformation is a Linear Transformation</title>
            <idx>
                <h>linear transformation</h>
                <h>addition</h>
            </idx>
            <statement>
                <p>Suppose that <m>\ltdefn{T}{U}{V}</m> is a linear transformation and <m>\alpha\in\complexes</m>.  Then <m>\ltdefn{(\alpha T)}{U}{V}</m> is a linear transformation.</p>
            </statement>
            <proof>
                <p>We simply check the defining properties of a linear transformation (<xref ref="definition-LT" acro="LT"/>).  This is another good place to consistently ask yourself which objects are being combined with which operations.  We have<md>
                    <mrow>\lteval{(\alpha T)}{\vect{x}+\vect{y}}&amp;=
                    \alpha\left(\lteval{T}{\vect{x}+\vect{y}}\right)&amp;&amp;
                        <xref ref="definition-LTSM" acro="LTSM"/></mrow>
                    <mrow>&amp;=\alpha\left(\lteval{T}{\vect{x}}+\lteval{T}{\vect{y}}\right)&amp;&amp;
                        <xref ref="definition-LT" acro="LT"/></mrow>
                    <mrow>&amp;=\alpha\lteval{T}{\vect{x}}+\alpha\lteval{T}{\vect{y}}&amp;&amp;
                        <xref ref="property-DVA" acro="DVA"/>\text{ in }V</mrow>
                    <mrow>&amp;=\lteval{(\alpha T)}{\vect{x}}+\lteval{(\alpha T)}{\vect{y}}&amp;&amp;
                        <xref ref="definition-LTSM" acro="LTSM"/></mrow>
                    <intertext>and</intertext>
                    <mrow>\lteval{(\alpha T)}{\beta\vect{x}}&amp;=
                    \alpha\lteval{T}{\beta\vect{x}}&amp;&amp;
                        <xref ref="definition-LTSM" acro="LTSM"/></mrow>
                    <mrow>&amp;=\alpha\left(\beta\lteval{T}{\vect{x}}\right)&amp;&amp;
                        <xref ref="definition-LT" acro="LT"/></mrow>
                    <mrow>&amp;=\left(\alpha\beta\right)\lteval{T}{\vect{x}}&amp;&amp;
                        <xref ref="property-SMA" acro="SMA"/>\text{ in }V</mrow>
                    <mrow>&amp;=\left(\beta\alpha\right)\lteval{T}{\vect{x}}&amp;&amp;
                            \text{Commutativity}</mrow>
                    <mrow>&amp;=\beta\left(\alpha\lteval{T}{\vect{x}}\right)&amp;&amp;
                        <xref ref="property-SMA" acro="SMA"/>\text{ in }V</mrow>
                    <mrow>&amp;=\beta\left(\lteval{(\alpha T)}{\vect{x}}\right)&amp;&amp;
                        <xref ref="definition-LTSM" acro="LTSM"/></mrow>
                </md>.</p>
            </proof>
        </theorem>
        <example xml:id="example-SMLT" acro="SMLT">
            <title>Scalar multiple of a linear transformation</title>
            <idx>
                <h>linear transformation</h>
                <h>scalar multiple</h>
            </idx>
            <p>Suppose that <m>\ltdefn{T}{\complex{4}}{\complex{3}}</m> is defined by<me>\lteval{T}{\colvector{x_1\\x_2\\x_3\\x_4}}=\colvector{x_1+2x_2-x_3+2x_4\\x_1+5x_2-3x_3+x_4\\-2x_1+3x_2-4x_3+2x_4}</me>.</p>
            <p>For the sake of an example, choose <m>\alpha=2</m>, so by <xref ref="definition-LTSM" acro="LTSM"/>, we have<md>
                <mrow>\lteval{\alpha T}{\colvector{x_1\\x_2\\x_3\\x_4}}
                &amp;=2\lteval{T}{\colvector{x_1\\x_2\\x_3\\x_4}}
                 =2\colvector{x_1+2x_2-x_3+2x_4\\x_1+5x_2-3x_3+x_4\\-2x_1+3x_2-4x_3+2x_4}</mrow>
                <mrow>&amp;=\colvector{2x_1+4x_2-2x_3+4x_4\\2x_1+10x_2-6x_3+2x_4\\-4x_1+6x_2-8x_3+4x_4}</mrow>
            </md>and by <xref ref="theorem-MLTLT" acro="MLTLT"/> we know <m>2T</m> is also a linear transformation from <m>\complex{4}</m> to <m>\complex{3}</m>.</p>
        </example>
        <p>Now, let us imagine we have two vector spaces, <m>U</m> and <m>V</m>, and we collect every possible linear transformation from <m>U</m> to <m>V</m> into one big set, and call it <m>\vslt{U}{V}</m>.  <xref ref="definition-LTA" acro="LTA"/> and <xref ref="definition-LTSM" acro="LTSM"/> tell us how we can <q>add</q> and <q>scalar multiply</q> two elements of <m>\vslt{U}{V}</m>.  <xref ref="theorem-SLTLT" acro="SLTLT"/> and <xref ref="theorem-MLTLT" acro="MLTLT"/> tell us that if we do these operations, then the resulting functions are linear transformations that are also in <m>\vslt{U}{V}</m>.   Hmmmm, sounds like a vector space to me!  A set of objects, an addition and a scalar multiplication.  Why not?</p>
        <theorem xml:id="theorem-VSLT" acro="VSLT">
            <title>Vector Space of Linear Transformations</title>
            <idx>
                <h>vector space</h>
                <h>linear transformations</h>
            </idx>
            <statement>
                <idx>
                    <h>linear transformation</h>
                    <h>vector space of</h>
                </idx>
                <p>Suppose that <m>U</m> and <m>V</m> are vector spaces.  Then the set of all linear transformations from <m>U</m> to <m>V</m>, <m>\vslt{U}{V}</m>, is a vector space when the operations are those given in <xref ref="definition-LTA" acro="LTA"/> and <xref ref="definition-LTSM" acro="LTSM"/>.</p>
            </statement>
            <proof>
                <p><xref ref="theorem-SLTLT" acro="SLTLT"/> and <xref ref="theorem-MLTLT" acro="MLTLT"/> provide two of the ten properties in <xref ref="definition-VS" acro="VS"/>.  However, we still need to verify the remaining eight properties.  By and large, the proofs are straightforward and rely on concocting the obvious object, or by reducing the question to the same vector space property in the vector space <m>V</m>.</p>
                <p>The zero vector is of some interest, though. What linear transformation would we add to any other linear transformation, so as to keep the second one unchanged?  The answer is <m>\ltdefn{Z}{U}{V}</m> defined by <m>\lteval{Z}{\vect{u}}=\zerovector_V</m> for every <m>\vect{u}\in U</m>.  Notice how we do not need to know any of the specifics about <m>U</m> and <m>V</m> to make this definition of <m>Z</m>.</p>
            </proof>
        </theorem>
        <definition xml:id="definition-LTC" acro="LTC">
            <title>Linear Transformation Composition</title>
            <idx>
                <h>linear transformation</h>
                <h>composition</h>
            </idx>
            <statement>
                <p>Suppose that <m>\ltdefn{T}{U}{V}</m> and <m>\ltdefn{S}{V}{W}</m> are linear transformations.  Then the <term>composition</term> of <m>S</m> and <m>T</m> is the function <m>\ltdefn{(\compose{S}{T})}{U}{W}</m> whose outputs are defined by<me>\lteval{(\compose{S}{T})}{\vect{u}}=\lteval{S}{\lteval{T}{\vect{u}}}</me>.</p>
            </statement>
        </definition>
        <p>Given that <m>T</m> and <m>S</m> are linear transformations, it would be nice to know that <m>\compose{S}{T}</m> is also a linear transformation.</p>
        <theorem xml:id="theorem-CLTLT" acro="CLTLT">
            <title>Composition of Linear Transformations is a Linear Transformation</title>
            <idx>
                <h>linear transformation</h>
                <h>composition</h>
            </idx>
            <statement>
                <p>Suppose that <m>\ltdefn{T}{U}{V}</m> and <m>\ltdefn{S}{V}{W}</m> are linear transformations.  Then <m>\ltdefn{(\compose{S}{T})}{U}{W}</m> is a linear transformation.</p>
            </statement>
            <proof>
                <p>We simply check the defining properties of a linear transformation (<xref ref="definition-LT" acro="LT"/>).  We have<md>
                    <mrow>\lteval{(\compose{S}{T})}{\vect{x}+\vect{y}}
                    &amp;=\lteval{S}{\lteval{T}{\vect{x}+\vect{y}}}&amp;&amp;
                        <xref ref="definition-LTC" acro="LTC"/></mrow>
                    <mrow>&amp;=\lteval{S}{\lteval{T}{\vect{x}}+\lteval{T}{\vect{y}}}&amp;&amp;
                        <xref ref="definition-LT" acro="LT"/>\text{ for }T</mrow>
                    <mrow>&amp;=\lteval{S}{\lteval{T}{\vect{x}}}+\lteval{S}{\lteval{T}{\vect{y}}}&amp;&amp;
                        <xref ref="definition-LT" acro="LT"/>\text{ for }S</mrow>
                    <mrow>&amp;=\lteval{(\compose{S}{T})}{\vect{x}}+\lteval{(\compose{S}{T})}{\vect{y}}&amp;&amp;
                        <xref ref="definition-LTC" acro="LTC"/></mrow>
                    <intertext>and</intertext>
                    <mrow>\lteval{(\compose{S}{T})}{\alpha\vect{x}}
                    &amp;=\lteval{S}{\lteval{T}{\alpha\vect{x}}}&amp;&amp;
                        <xref ref="definition-LTC" acro="LTC"/></mrow>
                    <mrow>&amp;=\lteval{S}{\alpha\lteval{T}{\vect{x}}}&amp;&amp;
                        <xref ref="definition-LT" acro="LT"/>\text{ for }T</mrow>
                    <mrow>&amp;=\alpha\lteval{S}{\lteval{T}{\vect{x}}}&amp;&amp;
                        <xref ref="definition-LT" acro="LT"/>\text{ for }S</mrow>
                    <mrow>&amp;=\alpha\lteval{(\compose{S}{T})}{\vect{x}}&amp;&amp;
                        <xref ref="definition-LTC" acro="LTC"/></mrow>
                </md>.</p>
            </proof>
        </theorem>
        <example xml:id="example-CTLT" acro="CTLT">
            <title>Composition of two linear transformations</title>
            <idx>
                <h>linear transformations</h>
                <h>compositions</h>
            </idx>
            <p>Suppose that <m>\ltdefn{T}{\complex{2}}{\complex{4}}</m> and <m>\ltdefn{S}{\complex{4}}{\complex{3}}</m> are defined by<md>
                <mrow>\lteval{T}{\colvector{x_1\\x_2}}=\colvector{x_1+2x_2\\3x_1-4x_2\\5x_1+2x_2\\6x_1-3x_2}&amp;&amp;
                \lteval{S}{\colvector{x_1\\x_2\\x_3\\x_4}}=
                \colvector{2x_1-x_2+x_3-x_4\\5x_1-3x_2+8x_3-2x_4\\-4x_1+3x_2-4x_3+5x_4}</mrow>
            </md>.</p>
            <p>Then by <xref ref="definition-LTC" acro="LTC"/><md>
                <mrow>\lteval{(\compose{S}{T})}{\colvector{x_1\\x_2}}&amp;=
                \lteval{S}{\lteval{T}{\colvector{x_1\\x_2}}}
                =\lteval{S}{\colvector{x_1+2x_2\\3x_1-4x_2\\5x_1+2x_2\\6x_1-3x_2}}</mrow>
                <mrow>&amp;=\colvector{
                2(x_1+2x_2)-(3x_1-4x_2)+(5x_1+2x_2)-(6x_1-3x_2)</mrow>
                <mrow>5(x_1+2x_2)-3(3x_1-4x_2)+8(5x_1+2x_2)-2(6x_1-3x_2)</mrow>
                <mrow>-4(x_1+2x_2)+3(3x_1-4x_2)-4(5x_1+2x_2)+5(6x_1-3x_2)
                }</mrow>
                <mrow>&amp;=\colvector{
                -2x_1+13x_2</mrow>
                <mrow>24x_1+44x_2</mrow>
                <mrow>15x_1-43x_2
                }</mrow>
            </md>and by <xref ref="theorem-CLTLT" acro="CLTLT"/> <m>\compose{S}{T}</m> is a linear transformation from <m>\complex{2}</m> to <m>\complex{3}</m>.</p>
        </example>
        <p>Here is an interesting exercise that will presage an important result later.  In <xref ref="example-STLT" acro="STLT"/> compute (via <xref ref="theorem-MLTCV" acro="MLTCV"/>) the matrix of  <m>T</m>, <m>S</m> and <m>T+S</m>.  Do you see a relationship between these three matrices?</p>
        <p>In <xref ref="example-SMLT" acro="SMLT"/> compute (via <xref ref="theorem-MLTCV" acro="MLTCV"/>) the matrix of  <m>T</m> and  <m>2T</m>.  Do you see a relationship between these two matrices?</p>
        <p>Here is the tough one.  In <xref ref="example-CTLT" acro="CTLT"/> compute (via <xref ref="theorem-MLTCV" acro="MLTCV"/>) the matrix of  <m>T</m>, <m>S</m> and <m>\compose{S}{T}</m>.  Do you see a relationship between these three matrices???</p>
        <computation xml:id="sage-OLT" acro="OLT">
            <title>Operations on Linear Transformations</title>
            <idx>
                <h>linear transformation</h>
                <h>operations on</h>
            </idx>
            <p>It is possible in Sage to add linear transformations (<xref ref="definition-LTA" acro="LTA"/>), multiply them by scalars (<xref ref="definition-LTSM" acro="LTSM"/>) and compose (<xref ref="definition-LTC" acro="LTC"/>) them.  Then <xref ref="theorem-SLTLT" acro="SLTLT"/> <xref ref="theorem-MLTLT" acro="MLTLT"/>, and <xref ref="theorem-CLTLT" acro="CLTLT"/> (respectively) tell us the results are again linear transformations.  Here are some examples:</p>
            <sage xml:id="sagecell-OLT-1">
                <input>
                U = QQ^4
                V = QQ^2
                A = matrix(QQ, 2, 4, [[-1, 3, 4,  5],
                                      [ 2, 0, 3, -1]])
                T = linear_transformation(U, V, A, side='right')
                B = matrix(QQ, 2, 4, [[-7, 4, -2,  0],
                                      [ 1, 1,  8, -3]])
                S = linear_transformation(U, V, B, side='right')
                P = S + T
                P
                </input>
                <output>
                Vector space morphism represented by the matrix:
                [-8  3]
                [ 7  1]
                [ 2 11]
                [ 5 -4]
                Domain: Vector space of dimension 4 over Rational Field
                Codomain: Vector space of dimension 2 over Rational Field
                </output>
            </sage>
            <sage xml:id="sagecell-OLT-2">
                <input>
                Q = S*5
                Q
                </input>
                <output>
                Vector space morphism represented by the matrix:
                [-35   5]
                [ 20   5]
                [-10  40]
                [  0 -15]
                Domain: Vector space of dimension 4 over Rational Field
                Codomain: Vector space of dimension 2 over Rational Field
                </output>
            </sage>
            <p>Perhaps the only surprise in all this is the necessity of writing scalar multiplication on the right of the linear transformation (rather on the left, as we do in the text).  We will recycle the linear transformation <c>T</c> from above and redefine <c>S</c> to form an example of composition.</p>
            <sage xml:id="sagecell-OLT-3">
                <input>
                W = QQ^3
                C = matrix(QQ, [[ 4, -2],
                                [-1,  3],
                                [-3,  2]])
                S = linear_transformation(V, W, C, side='right')
                R = S*T
                R
                </input>
                <output>
                Vector space morphism represented by the matrix:
                [ -8   7   7]
                [ 12  -3  -9]
                [ 10   5  -6]
                [ 22  -8 -17]
                Domain: Vector space of dimension 4 over Rational Field
                Codomain: Vector space of dimension 3 over Rational Field
                </output>
            </sage>
            <p>We use the star symbol (<c>*</c>) to indicate composition of linear transformations.  Notice that the order of the two linear transformations we compose is important, and Sage's order agrees with the text.  The order does not have to agree, and there are good arguments to have it reversed, so be careful if you read about composition elsewhere.</p>
            <p>This is a good place to expand on <xref ref="theorem-VSLT" acro="VSLT"/>, which says that with definitions of addition and scalar multiplication of linear transformations we then arrive at a vector space.  A vector space full of linear transformations.  Objects in Sage have <q>parents</q> <mdash/> vectors have vector spaces for parents, fractions of integers have the rationals as parents.  What is the parent of a linear transformation?  Let us see, by investigating the parent of <c>S</c> just defined above.</p>
            <sage xml:id="sagecell-OLT-4">
                <input>
                P = S.parent()
                P
                </input>
                <output>
                Set of Morphisms (Linear Transformations) from
                Vector space of dimension 2 over Rational Field to
                Vector space of dimension 3 over Rational Field
                </output>
            </sage>
            <p><q>Morphism</q> is a general term for a function that <q>preserves structure</q> or <q>respects operations.</q>  In Sage a collection of morphisms is referenced as a <q>homset</q> or a <q>homspace.</q>  In this example, we have a homset that is the vector space of linear transformations that go from a dimension 2 vector space over the rationals to a dimension 3 vector space over the rationals.  What can we do with it?  A few things, but not everything you might imagine.  It does have a basis, containing a few very simple linear transformations:</p>
            <sage xml:id="sagecell-OLT-5">
                <input>
                P.basis()
                </input>
                <output>
                (Vector space morphism represented by the matrix:
                [1 0 0]
                [0 0 0]
                Domain: Vector space of dimension 2 over Rational Field
                Codomain: Vector space of dimension 3 over Rational Field,
                Vector space morphism represented by the matrix:
                [0 1 0]
                [0 0 0]
                Domain: Vector space of dimension 2 over Rational Field
                Codomain: Vector space of dimension 3 over Rational Field,
                Vector space morphism represented by the matrix:
                [0 0 1]
                [0 0 0]
                Domain: Vector space of dimension 2 over Rational Field
                Codomain: Vector space of dimension 3 over Rational Field,
                Vector space morphism represented by the matrix:
                [0 0 0]
                [1 0 0]
                Domain: Vector space of dimension 2 over Rational Field
                Codomain: Vector space of dimension 3 over Rational Field,
                Vector space morphism represented by the matrix:
                [0 0 0]
                [0 1 0]
                Domain: Vector space of dimension 2 over Rational Field
                Codomain: Vector space of dimension 3 over Rational Field,
                Vector space morphism represented by the matrix:
                [0 0 0]
                [0 0 1]
                Domain: Vector space of dimension 2 over Rational Field
                Codomain: Vector space of dimension 3 over Rational Field)
                </output>
            </sage>
            <p>You can create a set of linear transformations with the <c>Hom()</c> function, simply by giving the domain and codomain.</p>
            <sage xml:id="sagecell-OLT-6">
                <input>
                H = Hom(QQ^6, QQ^9)
                H
                </input>
                <output>
                Set of Morphisms (Linear Transformations) from
                Vector space of dimension 6 over Rational Field to
                Vector space of dimension 9 over Rational Field
                </output>
            </sage>
            <p>An understanding of Sage's homsets is not critical to understanding the use of Sage during the remainder of this course.  But such an understanding can be very useful in understanding some of Sage's more advanced and powerful features.</p>
        </computation>
    </subsection>
    <exercises xml:id="readingquestions-LT">
        <title>Reading Questions</title>
        <exercise xml:id="reading-LT-1">
            <statement>
                <p>Is the function below a linear transformation?  Why or why not?<me>\ltdefn{T}{\complex{3}}{\complex{2}},\quad\lteval{T}{\colvector{x_1\\x_2\\x_3}}=\colvector{3x_1-x_2+x_3\\8x_2-6}</me>.</p>
            </statement>
        </exercise>
        <exercise xml:id="reading-LT-2">
            <statement>
                <p>Determine the matrix representation of the linear transformation <m>S</m> below.<me>\ltdefn{S}{\complex{2}}{\complex{3}},\quad\lteval{S}{\colvector{x_1\\x_2}}=\colvector{3x_1+5x_2\\8x_1-3x_2\\-4x_1}</me>.</p>
            </statement>
        </exercise>
        <exercise xml:id="reading-LT-3">
            <statement>
                <p><xref ref="theorem-LTLC" acro="LTLC"/> has a fairly simple proof.  Yet the result itself is very powerful.  Comment on why we might say this.</p>
            </statement>
        </exercise>
    </exercises>
    <exercises xml:id="exercises-LT">
        <title>Exercises</title>
        <exercise number="C15" xml:id="exercise-LT-C15">
            <statement>
                <p>The archetypes below are all linear transformations whose domains and codomains are vector spaces of column vectors (<xref ref="definition-VSCV" acro="VSCV"/>).  For each one, compute the matrix representation described in the proof of <xref ref="theorem-MLTCV" acro="MLTCV"/>.</p>
                <p>Archetype<nbsp/><xref ref="archetype-M" acro="M" text="global"/>,
                Archetype<nbsp/><xref ref="archetype-N" acro="N" text="global"/>,
                Archetype<nbsp/><xref ref="archetype-O" acro="O" text="global"/>,
                Archetype<nbsp/><xref ref="archetype-P" acro="P" text="global"/>,
                Archetype<nbsp/><xref ref="archetype-Q" acro="Q" text="global"/>,
                Archetype<nbsp/><xref ref="archetype-R" acro="R" text="global"/></p>
            </statement>
        </exercise>
        <exercise number="C16" xml:id="exercise-LT-C16">
            <statement contributor="chrisblack">
                <p>Find the matrix representation of <m>\ltdefn{T}{\complex{3}}{\complex{4}}</m>, <m>\lteval{T}{\colvector{x\\y\\z}} = \colvector{3x + 2y + z\\ x + y + z \\ x - 3y \\2x + 3y + z }</m>.</p>
            </statement>
            <solution xml:id="solution-LT-C16" contributor="chrisblack">
                <p><m>A_T = \begin{bmatrix} 3 &amp; 2 &amp; 1\\ 1 &amp; 1 &amp; 1\\ 1 &amp; -3  &amp; 0\\ 2 &amp; 3 &amp; 1\end{bmatrix}</m>.</p>
            </solution>
        </exercise>
        <exercise number="C20" xml:id="exercise-LT-C20">
            <statement>
                <p>Let <m>\vect{w}=\colvector{-3\\1\\4}</m>.  Referring to <xref ref="example-MOLT" acro="MOLT"/>, compute <m>\lteval{S}{\vect{w}}</m> two different ways.  First use the definition of <m>S</m>, then compute the matrix-vector product <m>C\vect{w}</m> (<xref ref="definition-MVP" acro="MVP"/>).</p>
            </statement>
            <solution xml:id="solution-LT-C20">
                <p>In both cases the result will be <m>\lteval{S}{\vect{w}}=\colvector{9\\2\\-9\\4}</m>.</p>
            </solution>
        </exercise>
        <exercise number="C25" xml:id="exercise-LT-C25">
            <statement>
                <p>Define the linear transformation<me>\ltdefn{T}{\complex{3}}{\complex{2}},\quad\lteval{T}{\colvector{x_1\\x_2\\x_3}}=\colvector{2x_1-x_2+5x_3\\-4x_1+2x_2-10x_3}</me>.  Verify that <m>T</m> is a linear transformation.</p>
            </statement>
            <solution xml:id="solution-LT-C25">
                <p>We can rewrite <m>T</m> as follows:<me>\lteval{T}{\colvector{x_1\\x_2\\x_3}}=\colvector{2x_1-x_2+5x_3\\-4x_1+2x_2-10x_3}
                =x_1\colvector{2\\-4}+x_2\colvector{-1\\2}+x_3\colvector{5\\-10}
                =\begin{bmatrix}
                2 &amp; -1 &amp; 5\\
                -4 &amp; 2 &amp; -10
                \end{bmatrix}
                \colvector{x_1\\x_2\\x_3}</me>and <xref ref="theorem-MBLT" acro="MBLT"/> tell us that any function of this form is a linear transformation.</p>
            </solution>
        </exercise>
        <exercise number="C26" xml:id="exercise-LT-C26">
            <statement>
                <p>Verify that the function below is a linear transformation.<me>\ltdefn{T}{P_2}{\complex{2}},\quad \lteval{T}{a+bx+cx^2}=\colvector{2a-b\\b+c}</me>.</p>
            </statement>
            <solution xml:id="solution-LT-C26">
                <p>Check the two conditions of <xref ref="definition-LT" acro="LT"/>.<md>
                <mrow>\lteval{T}{\vect{u}+\vect{v}}
                &amp;=\lteval{T}{\left(a+bx+cx^2\right)+\left(d+ex+fx^2\right)}</mrow>
                <mrow>&amp;=\lteval{T}{\left(a+d\right)+\left(b+e\right)x+\left(c+f\right)x^2}</mrow>
                <mrow>&amp;=\colvector{2(a+d)-(b+e)\\(b+e)+(c+f)}</mrow>
                <mrow>&amp;=\colvector{(2a-b)+(2d-e)\\(b+c)+(e+f)}</mrow>
                <mrow>&amp;=\colvector{2a-b\\b+c}+\colvector{2d-e\\e+f}</mrow>
                <mrow>&amp;=\lteval{T}{\vect{u}}+\lteval{T}{\vect{v}}</mrow>
                <intertext>and</intertext>
                <mrow>\lteval{T}{\alpha\vect{u}}
                &amp;=\lteval{T}{\alpha\left(a+bx+cx^2\right)}</mrow>
                <mrow>&amp;=\lteval{T}{\left(\alpha a\right)+\left(\alpha b\right)x+\left(\alpha c\right)x^2}</mrow>
                <mrow>&amp;=\colvector{2(\alpha a)-(\alpha b)\\(\alpha b)+(\alpha c)}</mrow>
                <mrow>&amp;=\colvector{\alpha(2a-b)\\\alpha(b+c)}</mrow>
                <mrow>&amp;=\alpha\colvector{2a-b\\b+c}</mrow>
                <mrow>&amp;=\alpha\lteval{T}{\vect{u}}</mrow>
            </md>.  So <m>T</m> is indeed a linear transformation.</p>
            </solution>
        </exercise>
        <exercise number="C30" xml:id="exercise-LT-C30">
            <statement>
                <p>Define the linear transformation<me>\ltdefn{T}{\complex{3}}{\complex{2}},\quad\lteval{T}{\colvector{x_1\\x_2\\x_3}}=\colvector{2x_1-x_2+5x_3\\-4x_1+2x_2-10x_3}</me>.  Compute the preimages, <m>\preimage{T}{\colvector{2\\3}}</m> and <m>\preimage{T}{\colvector{4\\-8}}</m>.</p>
            </statement>
            <solution xml:id="solution-LT-C30">
                <p>For the first pre-image, we want <m>\vect{x}\in\complex{3}</m> such that <m>\lteval{T}{\vect{x}}=\colvector{2\\3}</m>.  This becomes,<me>\colvector{2x_1-x_2+5x_3\\-4x_1+2x_2-10x_3}=\colvector{2\\3}</me>.  Vector equality gives a system of two linear equations in three variables, represented by the augmented matrix<me>\begin{bmatrix}
                2 &amp; -1 &amp; 5 &amp; 2\\
                -4 &amp; 2 &amp; -10 &amp; 3
                \end{bmatrix}
                \rref
                \begin{bmatrix}
                \leading{1} &amp; -\frac{1}{2} &amp; \frac{5}{2} &amp; 0\\
                0 &amp; 0 &amp; 0 &amp; \leading{1}
                \end{bmatrix}</me>so the system is inconsistent and the pre-image is the empty set.  For the second pre-image the same procedure leads to an augmented matrix with a different vector of constants<me>\begin{bmatrix}
                2 &amp; -1 &amp; 5 &amp; 4\\
                -4 &amp; 2 &amp; -10 &amp; -8
                \end{bmatrix}
                \rref
                \begin{bmatrix}
                \leading{1} &amp; -\frac{1}{2} &amp; \frac{5}{2} &amp; 2\\
                0 &amp; 0 &amp; 0 &amp; 0
                \end{bmatrix}</me>.  This system is consistent and has infinitely many solutions, as we can see from the presence of the  two free variables (<m>x_2</m> and <m>x_3</m>) both to zero.  We apply <xref ref="theorem-VFSLS" acro="VFSLS"/> to obtain<me>\preimage{T}{\colvector{4\\-8}}=
                \setparts{
                \colvector{2\\0\\0}+
                x_2\colvector{\frac{1}{2}\\1\\0}+
                x_3\colvector{-\frac{5}{2}\\0\\1}
                }{
                x_2,\,x_3\in\complexes
                }</me>.</p>
            </solution>
        </exercise>
        <exercise number="C31" xml:id="exercise-LT-C31">
            <statement>
                <p>For the linear transformation <m>S</m> compute the pre-images.<md>
                    <mrow>\ltdefn{S}{\complex{3}}{\complex{3}},\quad \lteval{S}{\colvector{a\\b\\c}}=
                    \colvector{a-2b-c\\3a-b+2c\\a+b+2c }</mrow>
                </md><md>
                    <mrow>\preimage{S}{\colvector{-2\\5\\3}}&amp;
                    &amp;
                    \preimage{S}{\colvector{-5\\5\\7}}&amp;</mrow>
                </md></p>
            </statement>
            <solution xml:id="solution-LT-C31">
                <p>We work from the definition of the pre-image, <xref ref="definition-PI" acro="PI"/>.   Setting<me>\lteval{S}{\colvector{a\\b\\c}}=\colvector{-2\\5\\3}</me>we arrive at a system of three equations in three variables, with an augmented matrix that we row-reduce in a search for solutions,<me>\begin{bmatrix}
                 1 &amp; -2 &amp; -1 &amp; -2 \\
                 3 &amp; -1 &amp; 2 &amp; 5 \\
                 1 &amp; 1 &amp; 2 &amp; 3
                \end{bmatrix}
                \rref
                \begin{bmatrix}
                 \leading{1} &amp; 0 &amp; 1 &amp; 0 \\
                 0 &amp; \leading{1} &amp; 1 &amp; 0 \\
                 0 &amp; 0 &amp; 0 &amp; \leading{1}
                \end{bmatrix}</me>.  With a leading 1 in the last column, this system is inconsistent (<xref ref="theorem-RCLS" acro="RCLS"/>), and there are no values of <m>a</m>, <m>b</m> and <m>c</m> that will create an element of the pre-image.  So the preimage is the empty set.</p>
                <p>We work from the definition of the pre-image, <xref ref="definition-PI" acro="PI"/>.   Setting<me>\lteval{S}{\colvector{a\\b\\c}}=\colvector{-5\\5\\7}</me>we arrive at a system of three equations in three variables, with an augmented matrix that we row-reduce in a search for solutions,<me>\begin{bmatrix}
                 1 &amp; -2 &amp; -1 &amp; -5 \\
                 3 &amp; -1 &amp; 2 &amp; 5 \\
                 1 &amp; 1 &amp; 2 &amp; 7
                \end{bmatrix}
                \rref
                \begin{bmatrix}
                 \leading{1} &amp; 0 &amp; 1 &amp; 3 \\
                 0 &amp; \leading{1} &amp; 1 &amp; 4 \\
                 0 &amp; 0 &amp; 0 &amp; 0
                \end{bmatrix}</me>.  The solution set to this system, which is also the desired pre-image, can be expressed using the vector form of the solutions (<xref ref="theorem-VFSLS" acro="VFSLS"/>)<me>\preimage{S}{\colvector{-5\\5\\7}}
                =\setparts{\colvector{3\\4\\0}+c\colvector{-1\\-1\\1}}{c\in\complex{}}
                =\colvector{3\\4\\0}+\spn{\set{\colvector{-1\\-1\\1}}}</me>.  Does the final expression for this set remind you of <xref ref="theorem-KPI" acro="KPI"/>?</p>
            </solution>
        </exercise>
        <exercise number="C40" xml:id="exercise-LT-C40">
            <statement contributor="chrisblack">
                <p>If <m>\ltdefn{T}{\complex{2}}{\complex{2}}</m> satisfies <m>\lteval{T}{\colvector{2\\1}} = \colvector{3\\4}</m> and <m>\lteval{T}{\colvector{1\\1}} = \colvector{-1\\2}</m>, find <m>\lteval{T}{\colvector{4\\3}}</m>.</p>
            </statement>
            <solution xml:id="solution-LT-C40" contributor="chrisblack">
                <p>Since <m>\colvector{4\\3}= \colvector{2\\1} + 2\colvector{1\\1}</m>, we have<md>
                    <mrow>T\left(\colvector{4\\3}\right)
                    &amp;= \lteval{T}{ \colvector{2\\1} + 2\colvector{1\\1}}
                    = \lteval{T}{\colvector{2\\1}} + 2\,\lteval{T}{\colvector{1\\1}}</mrow>
                    <mrow>&amp;= \colvector{3\\4} + 2\colvector{-1\\2}
                    &amp;= \colvector{1\\8}</mrow>
                </md>.</p>
            </solution>
        </exercise>
        <exercise number="C41" xml:id="exercise-LT-C41">
            <statement contributor="chrisblack">
                <p>If <m>\ltdefn{T}{\complex{2}}{\complex{3}}</m> satisfies <m>\lteval{T}{\colvector{2\\3}} = \colvector{2\\2\\1}</m> and <m>\lteval{T}{\colvector{3\\4}} = \colvector{-1\\0\\2}</m>, find the matrix representation of <m>T</m>.</p>
            </statement>
            <solution xml:id="solution-LT-C41" contributor="chrisblack">
                <p>First, we need to write the standard basis vectors <m>\vect{e}_1</m> and <m>\vect{e}_2</m> as linear combinations of <m>\colvector{2\\3}</m> and <m>\colvector{3\\4}</m>.  Starting with <m>\vect{e}_1</m>, we see that <m>\vect{e}_1 = -4\colvector{2\\3} + 3\colvector{3\\4}</m>, so we have<md>
                    <mrow>\lteval{T}{\vect{e}_1}
                    &amp;= \lteval{T}{-4\colvector{2\\3} + 3\colvector{3\\4}}
                    = -4\,\lteval{T}{\colvector{2\\3}} + 3 \,\lteval{T}{\colvector{3\\4}}</mrow>
                    <mrow>&amp;= -4\colvector{2\\2\\1} + 3\colvector{-1\\0\\2}
                    = \colvector{-11\\-8\\2}</mrow>
                </md>.  Repeating the process for <m>\vect{e}_2</m>,  we have
                    <m>\vect{e}_2 = 3\colvector{2\\3} - 2\colvector{3\\4}</m>, and we then see that<md>
                    <mrow>\lteval{T}{\vect{e}_2}
                    &amp;= \lteval{T}{3\colvector{2\\3} -2 \colvector{3\\4}}
                    = 3\,\lteval{T}{\colvector{2\\3}} - 2 \,\lteval{T}{\colvector{3\\4}}</mrow>
                    <mrow>&amp;= 3\colvector{2\\2\\1} -2 \colvector{-1\\0\\2}
                    = \colvector{8\\6\\-1}</mrow>
                </md>.  Thus, the matrix representation of <m>T</m> is <m>A_T = \begin{bmatrix} -11 &amp; 8\\ -8 &amp; 6\\2 &amp; -1 \end{bmatrix}</m>.</p>
            </solution>
        </exercise>
        <exercise number="C42" xml:id="exercise-LT-C42">
            <statement contributor="chrisblack">
                <p>Define <m>\ltdefn{T}{M_{22}}{\complex{1}}</m> by <m>\lteval{T}{\begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}} = a + b + c - d</m>.  Find the pre-image <m>\preimage{T}{3}</m>.</p>
            </statement>
            <solution xml:id="solution-LT-C42" contributor="chrisblack">
                <p>The preimage <m>\preimage{T}{3}</m> is the set of all matrices <m>\begin{bmatrix} a &amp; b\\c &amp; d\end{bmatrix}</m> so that <m>\lteval{T}{\begin{bmatrix}a &amp; b \\ c &amp; d \end{bmatrix}} = 3</m>. A matrix <m>\begin{bmatrix} a &amp; b\\ c &amp; d \end{bmatrix}</m> is in the preimage if <m>a + b + c - d = 3</m>, <ie/> <m>d = a + b + c - 3</m>. This is the set. (But the set is <em>not</em> a vector space. Why not?)<md>
                <mrow>\preimage{T}{3} &amp;=
                \setparts{\begin{bmatrix} a &amp; b \\ c &amp; a + b + c - 3\end{bmatrix}}{a,b,c\in\complexes}</mrow>
            </md></p>
            </solution>
        </exercise>
        <exercise number="C43" xml:id="exercise-LT-C43">
            <statement contributor="chrisblack">
                <p>Define <m>\ltdefn{T}{P_3}{P_2}</m> by <m>\lteval{T}{a + bx + cx^2 + dx^3} = b + 2cx + 3dx^2</m>. Find the pre-image of <m>\mathbf{0}</m>. Does this linear transformation seem familiar?</p>
            </statement>
            <solution xml:id="solution-LT-C43" contributor="chrisblack">
                <p>The preimage <m>\preimage{T}{0}</m> is the set of all polynomials <m>a + bx + cx^2 + dx^3</m> so that <m>\lteval{T}{a + bx + cx^2 + dx^3} = 0</m>.  Thus, <m>b + 2cx + 3dx^2 = 0</m>, where the <m>0</m> represents the zero polynomial.  In order to satisfy this equation, we must have <m>b = 0</m>, <m>c = 0</m>, and <m>d = 0</m>.  Thus, <m>\preimage{T}{0}</m> is precisely the set of all constant polynomials <ndash/> polynomials of degree 0. Symbolically, this is <m>\preimage{T}{0} = \setparts{a}{a\in\complexes}</m>.</p> <p>Does this seem familiar?  What other operation sends constant functions to 0?</p>
            </solution>
        </exercise>
        <exercise number="M10" xml:id="exercise-LT-M10">
            <statement>
                <p>Define two linear transformations, <m>\ltdefn{T}{\complex{4}}{\complex{3}}</m> and <m>\ltdefn{S}{\complex{3}}{\complex{2}}</m> by<md>
                    <mrow>\lteval{S}{\colvector{x_1\\x_2\\x_3}}
                    &amp;=
                    \colvector{
                    x_1-2x_2+3x_3</mrow>
                    <mrow>5x_1+4x_2+2x_3
                    }
                    &amp;
                    \lteval{T}{\colvector{x_1\\x_2\\x_3\\x_4}}
                    &amp;=
                    \colvector{
                    -x_1+3x_2+x_3+9x_4</mrow>
                    <mrow>2x_1+x_3+7x_4</mrow>
                    <mrow>4x_1+2x_2+x_3+2x_4
                    }</mrow>
                </md>.  Using the proof of <xref ref="theorem-MLTCV" acro="MLTCV"/> compute the matrix representations of the three linear transformations <m>T</m>, <m>S</m> and <m>\compose{S}{T}</m>.  Discover and comment on the relationship between these three matrices.</p>
            </statement>
            <solution xml:id="solution-LT-M10">
                <p>We have<me>\begin{bmatrix}
                1 &amp; -2 &amp; 3\\
                5 &amp; 4 &amp; 2
                \end{bmatrix}
                \begin{bmatrix}
                -1 &amp; 3 &amp; 1 &amp; 9 \\
                 2 &amp; 0 &amp; 1 &amp; 7 \\
                 4 &amp; 2 &amp; 1 &amp; 2
                \end{bmatrix}
                =
                \begin{bmatrix}
                 7 &amp; 9 &amp; 2 &amp; 1 \\
                 11 &amp; 19 &amp; 11 &amp; 77
                \end{bmatrix}</me>.</p>
            </solution>
        </exercise>
        <exercise number="M60" xml:id="exercise-LT-M60">
            <statement>
                <p>Suppose <m>U</m> and <m>V</m> are vector spaces and define a function <m>\ltdefn{Z}{U}{V}</m> by <m>\lteval{Z}{\vect{u}}=\zerovector_{V}</m> for every <m>\vect{u}\in U</m>.  Prove that <m>Z</m> is a (stupid) linear transformation.  (See <xref ref="exercise-ILT-M60" acro="ILT.M60"/>, <xref ref="exercise-SLT-M60" acro="SLT.M60"/>, <xref ref="exercise-IVLT-M60" acro="IVLT.M60"/>.)</p>
            </statement>
        </exercise>
        <exercise number="T20" xml:id="exercise-LT-T20">
            <statement>
                <p>Use the conclusion of <xref ref="theorem-LTLC" acro="LTLC"/> to motivate a new definition of a linear transformation.  Then prove that your new definition is equivalent to <xref ref="definition-LT" acro="LT"/>.  (Proof Technique<nbsp/><xref ref="technique-D" acro="D" text="global"/> and Proof Technique<nbsp/><xref ref="technique-E" acro="E" text="global"/> might be helpful if you are not sure what you are being asked to prove here.)</p>
            </statement>
        </exercise>
        <exercisegroup>
            <introduction>
                <p><xref ref="theorem-SER" acro="SER"/> established three properties of matrix similarity that are collectively known as the defining properties of an <q>equivalence relation</q>.  Exercises T30 and T31 extend this idea to linear transformations.</p>
            </introduction>
            <exercise number="T30" xml:id="exercise-LT-T30">
                <statement>
                    <p>Suppose that <m>\ltdefn{T}{U}{V}</m> is a linear transformation.  Say that two vectors from <m>U</m>, <m>\vect{x}</m> and <m>\vect{y}</m>, are <term>related</term> exactly when <m>\lteval{T}{\vect{x}}=\lteval{T}{\vect{y}}</m> in <m>V</m>.  Prove the three properties of an equivalence relation on <m>U</m>: (a) for any <m>\vect{x}\in U</m>, <m>\vect{x}</m> is related to <m>\vect{x}</m>, (b) if <m>\vect{x}</m> is related to <m>\vect{y}</m>, then <m>\vect{y}</m> is related to <m>\vect{x}</m>, and (c) if <m>\vect{x}</m> is related to <m>\vect{y}</m> and <m>\vect{y}</m> is related to <m>\vect{z}</m>, then <m>\vect{x}</m> is related to <m>\vect{z}</m>.</p>
                </statement>
            </exercise>
            <exercise number="T31" xml:id="exercise-LT-T31">
                <statement>
                    <p>Equivalence relations always create a partition of the set they are defined on, via a construction called equivalence classes.  For the relation in the previous problem, the equivalence classes are the pre-images.  Prove directly that the collection of pre-images partition <m>U</m> by showing that (a) every <m>\vect{x}\in U</m> is contained in some pre-image, and that (b) any two different pre-images do not have any elements in common.</p>
                </statement>
                <solution xml:id="solution-LT-T31">
                    <p>Choose  <m>\vect{x}\in U</m>, then <m>\lteval{T}{\vect{x}}\in V</m> and we can form <m>\preimage{T}{\lteval{T}{\vect{x}}}</m>.  Almost trivially, <m>\vect{x}\in\preimage{T}{\lteval{T}{\vect{x}}}</m>, so every vector in <m>U</m> is in <em>some</em> preimage.  For (b), suppose that <m>\preimage{T}{\vect{v}_1}</m> and <m>\preimage{T}{\vect{v}_2}</m> are two <em>different</em> preimages, and the vector <m>\vect{u}\in U</m> is an element of both.  Then <m>\lteval{T}{\vect{u}}=\vect{v}_1</m> and <m>\lteval{T}{\vect{u}}=\vect{v}_2</m>.  But because <m>T</m> is a function, we conclude that <m>\vect{v}_1=\vect{v}_2</m>.  It then follows that <m>\preimage{T}{\vect{v}_1}=\preimage{T}{\vect{v}_2}</m>, contrary to our assumption that they were different.  So there cannot be a common element <m>\vect{u}</m>.</p>
                </solution>
            </exercise>
        </exercisegroup>
    </exercises>
</section>
