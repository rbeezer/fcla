<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the book                -->
<!--                                              -->
<!--      A First Course in Linear Algebra        -->
<!--                                              -->
<!-- Copyright (C) 2004-2017  Robert A. Beezer    -->
<!-- See the file COPYING for copying conditions. -->

<section xml:id="section-PD" acro="PD">
    <title>Properties of Dimension</title>
    <introduction>
        <p>Once the dimension of a vector space is known, then the determination of whether or not a set of vectors is linearly independent, or if it spans the vector space, can often be much easier.  In this section we will state a workhorse theorem and then apply it to the column space and row space of a matrix.  It will also help us describe a super-basis for <m>\complex{m}</m>.</p>
    </introduction>
    <subsection xml:id="subsection-PD-GT" acro="GT">
        <title>Goldilocks' Theorem</title>
        <p>We begin with a useful theorem that we will need later, and in the proof of the main theorem in this subsection.  This theorem says that we can extend linearly independent sets, one vector at a time, by adding vectors from outside the span of the linearly independent set, all the while preserving the  linear independence of the set.</p>
        <theorem xml:id="theorem-ELIS" acro="ELIS">
            <idx>
                <h>linearly independent</h>
                <h>extending sets</h>
            </idx>
            <title>Extending Linearly Independent Sets</title>
            <statement>
                <p>Suppose <m>V</m> is a vector space and <m>S</m> is a linearly independent set of vectors from <m>V</m>.  Suppose <m>\vect{w}</m> is a vector such that <m>\vect{w}\not\in\spn{S}</m>.  Then the set <m>S^\prime=S\cup\set{\vect{w}}</m> is linearly independent.</p>
            </statement>
            <proof>
                <p>Suppose <m>S=\set{\vectorlist{v}{m}}</m> and begin with a relation of linear dependence on <m>S^\prime</m>,<me>\lincombo{a}{v}{m}+a_{m+1}\vect{w}=\zerovector</me>.</p>
                <p>There are two cases to consider.  First suppose that <m>a_{m+1}=0</m>.  Then the relation of linear dependence on <m>S^\prime</m> becomes<me>\lincombo{a}{v}{m}=\zerovector</me>and by the linear independence of the set <m>S</m>, we conclude that <m>a_1=a_2=a_3=\cdots=a_m=0</m>.  So all of the scalars in the relation of linear dependence on <m>S^\prime</m> are zero.</p>
                <p>In the second case, suppose that <m>a_{m+1}\neq 0</m>.    Then the relation of linear dependence on <m>S^\prime</m> becomes<md>
                    <mrow>a_{m+1}\vect{w}&amp;=-a_1\vect{v}_1-a_2\vect{v}_2-a_3\vect{v}_3-\cdots-a_m\vect{v}_m</mrow>
                    <mrow>\vect{w}&amp;=-\frac{a_1}{a_{m+1}}\vect{v}_1-\frac{a_2}{a_{m+1}}\vect{v}_2-\frac{a_3}{a_{m+1}}\vect{v}_3-\cdots-\frac{a_m}{a_{m+1}}\vect{v}_m</mrow>
                </md>.</p>
                <p>This equation expresses <m>\vect{w}</m> as a linear combination of the vectors in <m>S</m>, contrary to the assumption that <m>\vect{w}\not\in\spn{S}</m>, so this case leads to a contradiction.</p>
                <p>The first case yielded only a trivial relation of linear dependence on <m>S^\prime</m> and the second case led to a contradiction.  So <m>S^\prime</m> is a linearly independent set since any relation of linear dependence is trivial.</p>
            </proof>
        </theorem>
        <p>In the story <i>Goldilocks and the Three Bears</i>, the young girl Goldilocks visits the empty house of the three bears while out walking in the woods.  One bowl of porridge is too hot, the other too cold, the third is just right.  One chair is too hard, one too soft, the third is just right.  So it is with sets of vectors <mdash/> some are too big (linearly dependent), some are too small (they do not span), and some are just right (bases).  Here is Goldilocks' Theorem.</p>
        <theorem xml:id="theorem-G" acro="G">
            <idx>goldilocks</idx>
            <title>Goldilocks</title>
            <statement>
                <p>Suppose that <m>V</m> is a vector space of dimension <m>t</m>.  Let <m>S=\set{\vectorlist{v}{m}}</m> be a set of vectors from <m>V</m>.  Then<ol>
                    <li>If <m>m\gt t</m>, then <m>S</m> is linearly dependent.</li>
                    <li>If <m>m\lt t</m>, then <m>S</m> does not span <m>V</m>.</li>
                    <li>If <m>m=t</m> and <m>S</m> is linearly independent, then <m>S</m> spans <m>V</m>.</li>
                    <li>If <m>m=t</m> and <m>S</m> spans <m>V</m>, then <m>S</m> is linearly independent.</li>
                </ol></p>
            </statement>
            <proof>
                <p>Let <m>B</m> be a basis of <m>V</m>.  Since <m>\dimension{V}=t</m>, <xref ref="definition-B" acro="B"/> and <xref ref="theorem-BIS" acro="BIS"/> imply that <m>B</m> is a linearly independent set of <m>t</m> vectors that spans <m>V</m>.<ol>
                    <li>Suppose to the contrary that <m>S</m> is linearly independent.  Then <m>B</m> is a smaller set of vectors that spans <m>V</m>.   This contradicts <xref ref="theorem-SSLD" acro="SSLD"/>.</li>
                    <li>Suppose to the contrary that <m>S</m> does span <m>V</m>.  Then <m>B</m> is a larger set of vectors that is linearly independent.   This contradicts <xref ref="theorem-SSLD" acro="SSLD"/>.</li>
                    <li> Suppose to the contrary that <m>S</m> does not span <m>V</m>.  Then we can choose a vector <m>\vect{w}</m> such that <m>\vect{w}\in V</m> and <m>\vect{w}\not\in\spn{S}</m>.  By <xref ref="theorem-ELIS" acro="ELIS"/>, the set <m>S^\prime=S\cup\set{\vect{w}}</m> is again linearly independent.  Then <m>S^\prime</m> is a set of <m>m+1=t+1</m> vectors that are linearly independent, while <m>B</m> is a set of <m>t</m> vectors that span <m>V</m>.    This contradicts <xref ref="theorem-SSLD" acro="SSLD"/>.</li>
                    <li>Suppose to the contrary that <m>S</m> is linearly dependent.  Then by <xref ref="theorem-DLDS" acro="DLDS"/> (which can be upgraded, with no changes in the proof, to the setting of a general vector space), there is a vector in <m>S</m>, say <m>\vect{v}_k</m> that is equal to a linear combination of the other vectors in <m>S</m>.  Let <m>S^\prime=S\setminus\set{\vect{v}_k}</m>, the set of <q>other</q> vectors in <m>S</m>.  Then it is easy to show that <m>V=\spn{S}=\spn{S^\prime}</m>.  So <m>S^\prime</m> is a set of <m>m-1=t-1</m> vectors that spans <m>V</m>, while <m>B</m> is a set of <m>t</m> linearly independent vectors in <m>V</m>.  This contradicts <xref ref="theorem-SSLD" acro="SSLD"/>.</li>
                </ol></p>
            </proof>
        </theorem>
        <p>There is a tension in the construction of a basis.  Make a set too big and you will end up with relations of linear dependence among the vectors.  Make a set too small and you will not have enough raw material to span the entire vector space.  Make a set just the right size (the dimension) and you only need to have linear independence or spanning, and you get the other property for free.  These roughly-stated ideas are made precise by <xref ref="theorem-G" acro="G"/>.</p>
        <p>The structure and proof of this theorem also deserve comment.  The hypotheses seem innocuous.  We presume we know the dimension of the vector space in hand, then we mostly just look at the size of the set <m>S</m>. From this we get big conclusions about spanning and linear independence.  Each of the four proofs relies on ultimately contradicting <xref ref="theorem-SSLD" acro="SSLD"/>, so in a way we could think of this entire theorem as a corollary of <xref ref="theorem-SSLD" acro="SSLD"/>.    (See Proof Technique<nbsp/><xref ref="technique-LC" acro="LC" text="global"/>.) The proofs of the third and fourth parts parallel each other in style: introduce <m>\vect{w}</m> using <xref ref="theorem-ELIS" acro="ELIS"/> or toss <m>\vect{v}_k</m> using <xref ref="theorem-DLDS" acro="DLDS"/>.  Then obtain a contradiction to <xref ref="theorem-SSLD" acro="SSLD"/>.</p>
        <p><xref ref="theorem-G" acro="G"/> is useful in both concrete examples and as a tool in other proofs.  We will use it often to bypass verifying linear independence or spanning.</p>
        <example xml:id="example-BPR" acro="BPR">
            <idx>
                <h>basis</h>
                <h>polynomials</h>
            </idx>
            <title>Bases for <m>P_n</m>, reprised</title>
            <p>In <xref ref="example-BP" acro="BP"/> we claimed that<md>
                <mrow>B&amp;=\set{1,\,x,\,x^2,\,x^3,\,\ldots,\,x^n}</mrow>
                <mrow>C&amp;=\set{1,\,1+x,\,1+x+x^2,\,1+x+x^2+x^3,\,\ldots,\,1+x+x^2+x^3+\cdots+x^n}</mrow>
            </md>were both bases for <m>P_n</m> (<xref ref="example-VSP" acro="VSP"/>).  Suppose we had first verified that <m>B</m> was a basis, so we would then know that <m>\dimension{P_n}=n+1</m>.  The size of <m>C</m> is <m>n+1</m>, the right size to be a basis.  We could then verify that <m>C</m> is linearly independent.  We would not have to make any special efforts to prove that <m>C</m> spans <m>P_n</m>, since <xref ref="theorem-G" acro="G"/> would allow us to conclude this property of <m>C</m> directly.  Then we would be able to say that <m>C</m> is a basis of <m>P_n</m> also.</p>
        </example>
        <example xml:id="example-BDM22" acro="BDM22">
            <idx>
                <h>basis</h>
                <h>subspace of matrices</h>
            </idx>
            <title>Basis by dimension in <m>M_{22}</m></title>
            <p>In <xref ref="example-DSM22" acro="DSM22"/> we showed that<me>B=
            \set{
            \begin{bmatrix}-2&amp;1\\1&amp;0\end{bmatrix},\,
            \begin{bmatrix}-2&amp;0\\0&amp;1\end{bmatrix}
            }</me>is a basis for the subspace <m>Z</m> of <m>M_{22}</m> (<xref ref="example-VSM" acro="VSM"/>) given by<me>Z=\setparts{\begin{bmatrix}a&amp;b\\c&amp;d\end{bmatrix}}{2a+b+3c+4d=0,\,-a+3b-5c-d=0}</me>.</p>
            <p>This tells us that <m>\dimension{Z}=2</m>.  In this example we will find another basis.  We can construct two new matrices in <m>Z</m> by forming linear combinations of the matrices in <m>B</m>.<md>
                <mrow>2\begin{bmatrix}-2&amp;1\\1&amp;0\end{bmatrix}+
                 (-3)\begin{bmatrix}-2&amp;0\\0&amp;1\end{bmatrix}&amp;=
                 \begin{bmatrix}2&amp;2\\2&amp;-3\end{bmatrix}</mrow>
                <mrow>3\begin{bmatrix}-2&amp;1\\1&amp;0\end{bmatrix}+
                 1\begin{bmatrix}-2&amp;0\\0&amp;1\end{bmatrix}&amp;=
                \begin{bmatrix}-8&amp;3\\3&amp;1\end{bmatrix}</mrow>
            </md></p>
            <p>Then the set<me>C=
            \set{
            \begin{bmatrix}2&amp;2\\2&amp;-3\end{bmatrix},\,
            \begin{bmatrix}-8&amp;3\\3&amp;1\end{bmatrix}
            }</me>has the right size to be a basis of <m>Z</m>.  Let us see if it is a linearly independent set.  The relation of linear dependence<md>
                <mrow>a_1\begin{bmatrix}2&amp;2\\2&amp;-3\end{bmatrix}+
                a_2\begin{bmatrix}-8&amp;3\\3&amp;1\end{bmatrix}&amp;=\zeromatrix</mrow>
                <mrow>\begin{bmatrix}2a_1-8a_2&amp;2a_1+3a_2\\2a_1+3a_2&amp;-3a_1+a_2\end{bmatrix}&amp;=
                \begin{bmatrix}0&amp;0\\0&amp;0\end{bmatrix}</mrow>
            </md>leads to the homogeneous system of equations whose coefficient matrix<me>\begin{bmatrix}
            2&amp;-8\\
            2&amp;3\\
            2&amp;3\\
            -3&amp;1
            \end{bmatrix}</me>row-reduces to<me>\begin{bmatrix}
            \leading{1}&amp;0\\
            0&amp;\leading{1}\\
            0&amp;0\\
            0&amp;0
            \end{bmatrix}</me>.</p>
            <p>So with <m>a_1=a_2=0</m> as the only solution, the set is linearly independent.  Now we can apply <xref ref="theorem-G" acro="G"/> to see that <m>C</m> also spans <m>Z</m> and therefore is a second basis for <m>Z</m>.</p>
        </example>
        <example xml:id="example-SVP4" acro="SVP4">
            <idx>
                <h>basis</h>
                <h>polynomials</h>
            </idx>
            <title>Sets of vectors in <m>P_4</m></title>
            <p>In <xref ref="example-BSP4" acro="BSP4"/> we showed that<me>B=\set{x-2,\,x^2-4x+4,\,x^3-6x^2+12x-8,\,x^4-8x^3+24x^2-32x+16}</me>is a basis for <m>W=\setparts{p(x)}{p\in P_4,\ p(2)=0}</m>.  So <m>\dimension{W}=4</m>.</p>
            <p>The set<me>\set{3x^2-5x-2,\,2x^2-7x+6,\,x^3-2x^2+x-2}</me>is a subset of <m>W</m> (check this) and it happens to be linearly independent (check this, too).  However, by <xref ref="theorem-G" acro="G"/> it cannot span <m>W</m>.</p>
            <p>The set<me>\set{3x^2-5x-2,\,2x^2-7x+6,\,x^3-2x^2+x-2,\,-x^4+2x^3+5x^2-10x,\,x^4-16}</me>is another subset of <m>W</m> (check this) and <xref ref="theorem-G" acro="G"/> tells us that it must be linearly dependent.</p>
            <p>The set<me>\set{x-2,\,x^2-2x,\,x^3-2x^2,\,x^4-2x^3}</me>is a third subset of <m>W</m> (check this) and is linearly independent (check this).  Since it has the right size to be a basis, and is linearly independent, <xref ref="theorem-G" acro="G"/> tells us that it also spans <m>W</m>, and therefore is a basis of <m>W</m>.</p>
        </example>
        <p>A simple consequence of <xref ref="theorem-G" acro="G"/> is the observation that a proper subspace has strictly smaller dimension than its parent vector space.  Hopefully this may seem intuitively obvious, but it still requires proof, and we will cite this result later.</p>
        <theorem xml:id="theorem-PSSD" acro="PSSD">
            <idx>
                <h>dimension</h>
                <h>proper subspaces</h>
            </idx>
            <title>Proper Subspaces have Smaller Dimension</title>
            <statement>
                <p>Suppose that <m>U</m> and <m>V</m> are subspaces of the vector space <m>W</m>, such that <m>U\subsetneq V</m>.  Then <m>\dimension{U}\lt\dimension{V}</m>.</p>
            </statement>
            <proof>
                <p>Suppose that <m>\dimension{U}=m</m> and <m>\dimension{V}=t</m>.  Then <m>U</m> has a basis <m>B</m> of size <m>m</m>.  If <m>m\gt t</m>, then by <xref ref="theorem-G" acro="G"/>, <m>B</m> is linearly dependent, which is a contradiction.  If <m>m=t</m>, then by <xref ref="theorem-G" acro="G"/>, <m>B</m> spans <m>V</m>.  Then <m>U=\spn{B}=V</m>, also a contradiction. All that remains is that <m>m\lt t</m>, which is the desired conclusion.</p>
            </proof>
        </theorem>
        <p>The final theorem of this subsection is an extremely powerful tool for establishing the equality of two sets that are subspaces.  Notice that the hypotheses include the equality of two integers (dimensions) while the conclusion is the equality of two sets (subspaces).  It is the extra <q>structure</q> of a vector space and its dimension that makes possible this huge leap from an integer equality to a set equality.</p>
        <theorem xml:id="theorem-EDYES" acro="EDYES">
            <idx>
                <h>subspaces</h>
                <h>equal dimension</h>
            </idx>
            <title>Equal Dimensions Yields Equal Subspaces</title>
            <statement>
                <p>Suppose that <m>U</m> and <m>V</m> are subspaces of the vector space <m>W</m>, such that <m>U\subseteq V</m> and <m>\dimension{U}=\dimension{V}</m>.  Then <m>U=V</m>.</p>
            </statement>
            <proof>
                <p>The hypothesis that <m>U\subseteq V</m> allows for two cases.  Either <m>U\subsetneq V</m> or <m>U=V</m>.</p>
                <p>If <m>U\subsetneq V</m>, then the hypotheses of <xref ref="theorem-PSSD" acro="PSSD"/> are met and we have a contradiction to the hypothesis that <m>\dimension{U}=\dimension{V}</m>.</p>
                <p>Since the first case is impossible, the second case must always occur, which is our desired conclusion.</p>
            </proof>
        </theorem>
    </subsection>
    <subsection xml:id="subsection-PD-RT" acro="RT">
        <title>Ranks and Transposes</title>
        <p>We now prove one of the most surprising theorems about matrices.  Notice the paucity of hypotheses compared to the precision of the conclusion.</p>
        <theorem xml:id="theorem-RMRT" acro="RMRT">
            <idx>
                <h>rank</h>
                <h>transpose</h>
            </idx>
            <title>Rank of a Matrix is the Rank of the Transpose</title>
            <statement>
                <p>Suppose <m>A</m> is an <m>m\times n</m> matrix.  Then <m>\rank{A}=\rank{\transpose{A}}</m>.</p>
            </statement>
            <proof>
                <p>Suppose we row-reduce <m>A</m> to the matrix <m>B</m> in reduced row-echelon form, and <m>B</m> has <m>r</m> nonzero rows.  The quantity <m>r</m> tells us three things about <m>B</m>: the number of leading 1's, the number of nonzero rows and the number of pivot columns.  For this proof we will be interested in the latter two.</p>
                <p><xref ref="theorem-BRS" acro="BRS"/> and <xref ref="theorem-BCS" acro="BCS"/> each has a conclusion that provides a basis, for the row space and the column space, respectively.  In each case, these bases contain <m>r</m> vectors.  This observation makes the following go.</p>
                <p><md>
                    <mrow>\rank{A}&amp;=\dimension{\csp{A}}&amp;&amp;
                        <xref ref="definition-ROM" acro="ROM"/></mrow>
                    <mrow>&amp;=r&amp;&amp;
                        <xref ref="theorem-BCS" acro="BCS"/></mrow>
                    <mrow>&amp;=\dimension{\rsp{A}}&amp;&amp;
                        <xref ref="theorem-BRS" acro="BRS"/></mrow>
                    <mrow>&amp;=\dimension{\csp{\transpose{A}}}&amp;&amp;
                        <xref ref="theorem-CSRST" acro="CSRST"/></mrow>
                    <mrow>&amp;=\rank{\transpose{A}}&amp;&amp;
                        <xref ref="definition-ROM" acro="ROM"/></mrow>
                </md></p>
                <p><xref ref="jacoblinenthal"/> helped with this proof.</p>
            </proof>
        </theorem>
        <p>This says that the row space and the column space of a matrix have the same dimension, which should be very surprising.  It does <em>not</em> say that column space and the row space are identical.  Indeed, if the matrix is not square, then the sizes (number of slots) of the vectors in each space are different, so the sets are not even comparable.</p>
        <p>It is not hard to construct by yourself examples of matrices that illustrate <xref ref="theorem-RMRT" acro="RMRT"/>, since it applies equally well to <em>any</em> matrix.  Grab a matrix, row-reduce it, count the nonzero rows or the number of pivot columns.  That is the rank.  Transpose the matrix, row-reduce that, count the nonzero rows or the pivot columns.  That is the rank of the transpose.  The theorem says the two will be equal.  Every time.  Here is an example anyway.</p>
        <example xml:id="example-RRTI" acro="RRTI">
            <idx>
                <h>rank</h>
                <h>of transpose</h>
            </idx>
            <!-- Archetype I, Part matrixreduced -->
            <title>Rank, rank of transpose, Archetype I</title>
            <p>Archetype<nbsp/><xref ref="archetype-I" acro="I" text="global"/> has a <m>4\times 7</m> coefficient matrix which row-reduces to<me>\begin{bmatrix}
            \leading{1} &amp; 4 &amp; 0 &amp; 0 &amp; 2 &amp; 1 &amp; -3\\
            0 &amp; 0 &amp; \leading{1} &amp;  0 &amp; 1 &amp;  -3 &amp; 5\\
            0 &amp; 0 &amp;  0 &amp; \leading{1} &amp; 2 &amp;  -6 &amp; 6\\
            0 &amp; 0 &amp;  0 &amp; 0 &amp; 0 &amp; 0 &amp; 0
            \end{bmatrix}</me>so the rank is <m>3</m>.  Row-reducing the transpose yields<me>\begin{bmatrix}
            \leading{1} &amp; 0 &amp; 0 &amp; -\frac{31}{7}\\
            0 &amp; \leading{1} &amp; 0 &amp; \frac{12}{7}\\
            0 &amp; 0 &amp; \leading{1} &amp; \frac{13}{7}\\
            0 &amp; 0 &amp; 0 &amp; 0\\
            0 &amp; 0 &amp; 0 &amp; 0\\
            0 &amp; 0 &amp; 0 &amp; 0\\
            0 &amp; 0 &amp; 0 &amp; 0
            \end{bmatrix}</me>demonstrating that the rank of the transpose is also <m>3</m>.</p>
        </example>
    </subsection>
    <subsection xml:id="subsection-PD-DFS" acro="DFS">
        <title>Dimension of Four Subspaces</title>
        <p>That the rank of a matrix equals the rank of its transpose is a fundamental and surprising result.  However, applying <xref ref="theorem-FS" acro="FS"/> we can easily determine the dimension of all four fundamental subspaces associated with a matrix.</p>
        <theorem xml:id="theorem-DFS" acro="DFS">
            <idx>
                <h>four subspaces</h>
                <h>dimension</h>
            </idx>
            <title>Dimensions of Four Subspaces</title>
            <statement>
                <p>Suppose that <m>A</m> is an <m>m\times n</m> matrix, and <m>B</m> is a row-equivalent matrix in reduced row-echelon form with <m>r</m> nonzero rows.  Then<ol>
                    <li><m>\dimension{\nsp{A}}=n-r</m></li>
                    <li><m>\dimension{\csp{A}}=r</m></li>
                    <li><m>\dimension{\rsp{A}}=r</m></li>
                    <li><m>\dimension{\lns{A}}=m-r</m></li>
                </ol>.</p>
            </statement>
            <proof>
                <p>If <m>A</m> row-reduces to a matrix in reduced row-echelon form with <m>r</m> nonzero rows, then the matrix <m>C</m> of extended echelon form (<xref ref="definition-EEF" acro="EEF"/>) will be an <m>r\times n</m> matrix in reduced row-echelon form with no zero rows and <m>r</m> pivot columns (<xref ref="theorem-PEEF" acro="PEEF"/>).  Similarly, the matrix <m>L</m> of extended echelon form (<xref ref="definition-EEF" acro="EEF"/>) will be an <m>m-r\times m</m> matrix in reduced row-echelon form with no zero rows and <m>m-r</m> pivot columns (<xref ref="theorem-PEEF" acro="PEEF"/>).</p>
                <p><md>
                    <mrow>\dimension{\nsp{A}}
                    &amp;=\dimension{\nsp{C}}&amp;&amp;
                        <xref ref="theorem-FS" acro="FS"/></mrow>
                    <mrow>&amp;=n-r&amp;&amp;
                        <xref ref="theorem-BNS" acro="BNS"/></mrow>
                    <mrow>\ </mrow>
                    <mrow>\dimension{\csp{A}}
                    &amp;=\dimension{\nsp{L}}&amp;&amp;
                        <xref ref="theorem-FS" acro="FS"/></mrow>
                    <mrow>&amp;=m-(m-r)&amp;&amp;
                        <xref ref="theorem-BNS" acro="BNS"/></mrow>
                    <mrow>&amp;=r</mrow>
                    <mrow>\ </mrow>
                    <mrow>\dimension{\rsp{A}}
                    &amp;=\dimension{\rsp{C}}&amp;&amp;
                        <xref ref="theorem-FS" acro="FS"/></mrow>
                    <mrow>&amp;=r&amp;&amp;
                        <xref ref="theorem-BRS" acro="BRS"/></mrow>
                    <mrow>\ </mrow>
                    <mrow>\dimension{\lns{A}}
                    &amp;=\dimension{\rsp{L}}&amp;&amp;
                        <xref ref="theorem-FS" acro="FS"/></mrow>
                    <mrow>&amp;=m-r&amp;&amp;
                        <xref ref="theorem-BRS" acro="BRS"/></mrow>
                    </md></p>
            </proof>
        </theorem>
        <p>There are many different ways to state and prove this result, and indeed, the equality of the dimensions of the column space and row space is just a slight expansion of <xref ref="theorem-RMRT" acro="RMRT"/>.  However, we have restricted our techniques to applying <xref ref="theorem-FS" acro="FS"/> and then determining dimensions with bases provided by <xref ref="theorem-BNS" acro="BNS"/> and <xref ref="theorem-BRS" acro="BRS"/>.  This provides an appealing symmetry to the results and the proof.</p>
        <p>In <xref ref="section-S"/> we saw two general constructions (<xref ref="theorem-SIIS"/>, <xref ref="theorem-SSIS"/>) that combined two subspaces to create new subspaces.  The four subspaces involved have dimensions related to each other, a result that will can be very useful.</p>
        <theorem xml:id="theorem-SID" acro="SID">
            <title>Sum and Intersection Dimensions</title>
            <idx>
                <h>subspace</h>
                <h>sum, intersection dimensions</h>
            </idx>
            <statement>
                <p>Suppose that <m>U</m> and <m>V</m> are subspaces of a vector space.  Then
                    <me>\dimension{U+V} = \dimension{U} + \dimension{V} - \dimension{U\cap V}</me>.</p>
            </statement>
            <proof>
                <p>Let <m>A=\set{\vectorlist{x}{k}}</m> be a basis of the subspace <m>U\cap V</m>.  Because <m>U</m> and <m>V</m> are each individually subspaces of <m>U+V</m>, we can extend <m>A</m> via repeated applications of <xref ref="theorem-ELIS"/> to form bases for <m>U</m> and <m>V</m>.  To wit, let <m>\set{\vectorlist{u}{r}}\subseteq U</m> be a set of vectors such that <m>C=\set{\vectorlist{x}{k},\,\vectorlist{u}{r}}</m> is a basis for <m>U</m>.  Similarly, let <m>\set{\vectorlist{v}{s}}\subseteq V</m> be a set of vectors such that <m>D=\set{\vectorlist{x}{k},\,\vectorlist{v}{s}}</m> is a basis for <m>V</m>.  Note that we have implicitly determined (by <xref ref="definition-D"/>) that <m>\dimension{U\cap V}=k</m>, <m>\dimension{U}=k+r</m>, and <m>\dimension{V}=k+s</m>.</p>
                <p>With this setup, we claim that
                    <me>B=\set{\vectorlist{x}{k},\,\vectorlist{u}{r},\,\vectorlist{v}{s}}</me>
                is a basis for the subspace <m>U+V</m>.  We establish spanning first.  Suppose that <m>\vect{x}\in U+V</m>, so <m>\vect{x} = \vect{u}+\vect{v}</m> where <m>\vect{u}\in U</m> and <m>\vect{v}\in V</m>.  Since <m>\vect{u}\in U</m> we can express <m>\vect{u}</m> as a linear combination of the vectors in <m>C</m>, and since <m>\vect{v}\in V</m> we can express <m>\vect{v}</m> as a linear combination of the vectors in <m>D</m>.  So<md>
                    <mrow>\vect{x} &amp;= \vect{u} + \vect{v}</mrow>
                    <mrow>&amp;=\left(\lincombo{a}{x}{k}+\lincombo{c}{u}{r}\right) + </mrow>
                    <mrow>&amp;\quad\quad\left(\lincombo{b}{x}{k}+\lincombo{d}{v}{s}\right)</mrow>
                    <mrow>&amp;=\left(a_1+b_1\right)\vect{x_1} + \left(a_2+b_2\right)\vect{x_2} +\left(a_3+b_3\right)\vect{x_3} + \cdots + \left(a_k+b_k\right)\vect{x}_k +</mrow>
                    <mrow>&amp;\quad\quad\lincombo{c}{u}{r} + \lincombo{d}{v}{s}</mrow>
                </md>and we see that <m>\vect{x}</m> is a linear combination of the vectors of <m>B</m>, so <m>B</m> spans <m>U+V</m> by <xref ref="definition-SSVS"/>.</p>
                <p>To establish linear independence, we begin with a relation of linear independence (<xref ref="definition-RLD"/>) on <m>B</m>,
                    <me>\lincombo{a}{x}{k}+\lincombo{c}{u}{r} + \lincombo{d}{v}{s}=\zerovector</me>.
                We rearrange this equation, and give a name to the common vector that is the expression on either side of the equality.<md>
                    <mrow>\vect{w} &amp;= \lincombo{a}{x}{k}+\lincombo{c}{u}{r}</mrow>
                    <mrow>&amp;= - \lincombo{d}{v}{s}</mrow>
                </md>.  From the first expression we see that <m>\vect{w}</m> is a linear combination of the basis <m>C</m> and so <m>\vect{w}\in U</m>.  The second expression shows that <m>\vect{w}</m> is a linear combination of the basis <m>D</m> and so <m>\vect{w}\in V</m>.  Thus, <m>\vect{w}\in U\cap V</m> and we can express <m>\vect{w}</m> as a linear combination of the vectors of <m>A</m>.  We use this observation, and another expression for <m>\vect{w}</m> from just above, to form an equality that we then rearrange,<md>
                    <mrow>\vect{w} = \lincombo{e}{x}{k} = -\lincombo{d}{v}{s}</mrow>
                    <mrow>\zerovector = \lincombo{e}{x}{k} + \lincombo{d}{v}{s}</mrow>
                </md>Aha!  Finally, we have a relation of linear dependence on a linearly independent set (<m>D</m>) and so by <xref ref="definition-LI"/> we conclude that
                    <me>e_1=e_2=e_3=\cdots=e_k=d_1=d_2=d_3=\cdots=d_s=0</me>
                and in particular, <m>\vect{w}=\zerovector</m>.</p>
                <p>Now, if we return to the equation where we first introduced <m>\vect{w}</m>, it becomes
                    <me>\zerovector = \vect{w} = \lincombo{a}{x}{k}+\lincombo{c}{u}{r}</me>.
                Now, this is a relation of linear dependence on a linearly independent set (<m>C</m>) and so by <xref ref="definition-LI"/> we conclude that
                    <me>a_1=a_2=e_3=\cdots=a_k=c_1=c_2=c_3=\cdots=c_r=0</me>
                and we have determined that all of the scalars in our original relation of linear dependence on <m>B</m> are zero, and so by <xref ref="definition-LI"/>, we see that <m>B</m> is linearly independent.</p>
                <p>Having established that <m>B</m> is a basis, we can say that <m>\dimension{U+V}=k+r+s</m>.  So we have our result,<md>
                    <mrow>\dimension{U+V} &amp;= k + r + s</mrow>
                    <mrow>&amp;= \left(k + r\right) + \left(k + s\right) - k</mrow>
                    <mrow>&amp;= \dimension{U} + \dimension{V} - \dimension{U\cap V}</mrow>
                </md>.</p>
            </proof>
        </theorem>
        <p>An alternate version of the conclusion of <xref ref="theorem-SID"/> is
            <me>\dimension{U+V} + \dimension{U\cap V} = \dimension{U} + \dimension{V}</me>
        which has an appealing symmetry.  <xref ref="exercise-PD-T60"/> is highly related to this theorem.</p>
        <computation xml:id="sage-DMS" acro="DMS">
            <title>Dimensions of Matrix Subspaces</title>
            <idx>
                <h>dimensions</h>
                <h>matrix subspaces</h>
            </idx>
            <p>The theorems in this section about the dimensions of various subspaces associated with matrices can be tested easily in Sage.  For a large arbitrary matrix, we first verify <xref ref="theorem-RMRT" acro="RMRT"/>, followed by the four conclusions of <xref ref="theorem-DFS" acro="DFS"/>.</p>
            <sage xml:id="sagecell-DMS-1">
                <input>
                A = matrix(QQ, [
                    [ 1, -2,  3,  2,  0,  2,  2, -1,  3,  8,  0,  7],
                    [-1,  2, -2, -1,  3, -3,  5, -2, -6, -7,  6, -2],
                    [ 0,  0,  1,  1,  0,  0,  1, -3, -2,  0,  3,  8],
                    [-1, -1,  0, -1, -1,  0, -6, -2, -5, -6,  5,  1],
                    [ 1, -3,  2,  1, -4,  4, -6,  2,  7,  7, -5,  2],
                    [-2,  2, -2, -2,  3, -3,  6, -1, -8, -8,  7, -7],
                    [ 0, -3,  2,  0, -3,  3, -7,  1,  2,  3, -1,  0],
                    [ 0, -1,  2,  1,  2,  0,  4, -3, -3,  2,  6,  6],
                    [-1,  1,  0, -1,  2, -1,  6, -2, -6, -3,  8,  0],
                    [ 0, -4,  4,  0, -2,  4, -4, -2, -2,  4,  8,  6]
                               ])
                m = A.nrows()
                n = A.ncols()
                r = A.rank()
                m, n, r
                </input>
                <output>
                (10, 12, 7)
                </output>
            </sage>
            <sage xml:id="sagecell-DMS-2">
                <input>
                A.transpose().rank() == r
                </input>
                <output>
                True
                </output>
            </sage>
            <sage xml:id="sagecell-DMS-3">
                <input>
                A.right_kernel().dimension() == n - r
                </input>
                <output>
                True
                </output>
            </sage>
            <sage xml:id="sagecell-DMS-4">
                <input>
                A.column_space().dimension() == r
                </input>
                <output>
                True
                </output>
            </sage>
            <sage xml:id="sagecell-DMS-5">
                <input>
                A.row_space().dimension() == r
                </input>
                <output>
                True
                </output>
            </sage>
            <sage xml:id="sagecell-DMS-6">
                <input>
                A.left_kernel().dimension() == m - r
                </input>
                <output>
                True
                </output>
            </sage>
        </computation>
    </subsection>
    <reading-questions xml:id="readingquestions-PD">
        <exercise component="fcla-rq" xml:id="reading-PD-1" label="reading-PD-1">
            <title>Title of Theorem G</title>
            <statement>
                <p>Why does <xref ref="theorem-G" acro="G"/> have the title it does?</p>
            </statement>
            <response/>
        </exercise>
        <exercise component="fcla-rq" xml:id="reading-PD-2" label="reading-PD-2">
            <title>Theorem RMRT surprising</title>
            <statement>
                <p>Why is <xref ref="theorem-RMRT" acro="RMRT"/> so surprising ?</p>
            </statement>
            <response/>
        </exercise>
        <exercise component="fcla-rq" xml:id="reading-PD-3" label="reading-PD-3">
            <title>Calculate dimensions of the 4 subspaces</title>
            <statement>
                <p>Row-reduce the matrix <m>A</m> to reduced row-echelon form.  Without any further computations, compute the dimensions of the four subspaces, (a) <m>\nsp{A}</m>, (b) <m>\csp{A}</m>, (c) <m>\rsp{A}</m>  and  (d) <m>\lns{A}</m>.<me>A=
                \begin{bmatrix}
                 1 &amp; -1 &amp; 2 &amp; 8 &amp; 5 \\
                 1 &amp; 1 &amp; 1 &amp; 4 &amp; -1 \\
                 0 &amp; 2 &amp; -3 &amp; -8 &amp; -6 \\
                 2 &amp; 0 &amp; 1 &amp; 8 &amp; 4
                \end{bmatrix}</me></p>
            </statement>
            <response/>
        </exercise>
    </reading-questions>
    <exercises xml:id="exercises-PD">
        <title>Exercises</title>
        <exercise number="C10" xml:id="exercise-PD-C10">
            <statement>
                <p><xref ref="example-SVP4" acro="SVP4"/> leaves several details for the reader to check.  Verify these five claims.</p>
            </statement>
        </exercise>
        <exercise number="C40" xml:id="exercise-PD-C40">
            <statement>
                <p>Determine if the set <m>T=\set{x^2-x+5,\,4x^3-x^2+5x,\,3x+2}</m> spans the vector space of polynomials with degree 4 or less, <m>P_4</m>.  (Compare the solution to this exercise with <xref ref="solution-LISS-C40" acro="LISS.C40"/>.)</p>
            </statement>
            <solution xml:id="solution-PD-C40">
                <p>The vector space <m>P_4</m> has dimension 5 by <xref ref="theorem-DP" acro="DP"/>.  Since <m>T</m> contains only 3 vectors, and <m>3\lt 5</m>, <xref ref="theorem-G" acro="G"/> tells us that <m>T</m> does not span <m>P_4</m>.</p>
            </solution>
        </exercise>
        <exercise number="T05" xml:id="exercise-PD-T05">
            <statement>
                <p>Trivially, if <m>U</m> and <m>V</m> are two subspaces of <m>W</m> with <m>U = V</m>, then <m>\dimension{U}=\dimension{V}</m>.  Combine this fact, <xref ref="theorem-PSSD" acro="PSSD"/>, and <xref ref="theorem-EDYES" acro="EDYES"/> all into one grand combined theorem.  You might look to <xref ref="theorem-PIP" acro="PIP"/> for stylistic inspiration.  (Notice this problem does not ask you to prove anything.  It just asks you to roll up three theorems into one compact, logically equivalent statement.)</p>
            </statement>
        </exercise>
        <exercise number="T10" xml:id="exercise-PD-T10">
            <statement>
                <p>Prove the following theorem, which could be viewed as a reformulation of parts (3) and (4) of <xref ref="theorem-G" acro="G"/>, or more appropriately as a corollary of <xref ref="theorem-G" acro="G"/> (Proof Technique<nbsp/><xref ref="technique-LC" acro="LC" text="global"/>).</p>
                <p>Suppose <m>V</m> is a vector space and <m>S</m> is a subset of <m>V</m> such that the number of vectors in <m>S</m> equals the dimension of <m>V</m>.  Then <m>S</m> is linearly independent if and only if <m>S</m> spans <m>V</m>.</p>
            </statement>
        </exercise>
        <exercise number="T15" xml:id="exercise-PD-T15">
            <statement>
                <p>Suppose that <m>A</m> is an <m>m\times n</m> matrix and let <m>\text{min}(m,\,n)</m> denote the minimum of <m>m</m> and <m>n</m>.  Prove that <m>\rank{A}\leq \text{min}(m,\,n)</m>.  (If <m>m</m> and <m>n</m> are two numbers, then <m>\text{min}(m,\,n)</m> stands for the number that is the smaller of the two.  For example <m>\text{min}(4,\,6)=4</m>.)</p>
            </statement>
        </exercise>
        <exercise number="T20" xml:id="exercise-PD-T20">
            <statement>
                <p>Suppose that <m>A</m> is an <m>m\times n</m> matrix and <m>\vect{b}\in\complex{m}</m>.  Prove that the linear system <m>\linearsystem{A}{\vect{b}}</m> is consistent if and only if <m>\rank{A}=\rank{\augmented{A}{\vect{b}}}</m>.</p>
            </statement>
            <solution xml:id="solution-PD-T20">
                <proof>
                    <case direction="forward">
                        <p>Suppose first that <m>\linearsystem{A}{\vect{b}}</m> is consistent.  Then by <xref ref="theorem-CSCS" acro="CSCS"/>, <m>\vect{b}\in\csp{A}</m>.  This means that <m>\csp{A}=\csp{\augmented{A}{\vect{b}}}</m> and so it follows that <m>\rank{A}=\rank{\augmented{A}{\vect{b}}}</m>.</p>
                    </case>
                    <case direction="backward">
                        <p>Adding a column to a matrix will only increase the size of its column space, so in all cases, <m>\csp{A}\subseteq\csp{\augmented{A}{\vect{b}}}</m>.  However, if we assume that <m>\rank{A}=\rank{\augmented{A}{\vect{b}}}</m>, then by <xref ref="theorem-EDYES" acro="EDYES"/> we conclude that <m>\csp{A}=\csp{\augmented{A}{\vect{b}}}</m>.  Then <m>\vect{b}\in\csp{\augmented{A}{\vect{b}}}=\csp{A}</m> so by <xref ref="theorem-CSCS" acro="CSCS"/>, <m>\linearsystem{A}{\vect{b}}</m> is consistent.</p>
                    </case>
                </proof>
            </solution>
        </exercise>
        <exercise number="T25" xml:id="exercise-PD-T25">
            <statement>
                <p>Suppose that <m>V</m> is a vector space with finite dimension.  Let <m>W</m> be any subspace of <m>V</m>.  Prove that <m>W</m> has finite dimension.</p>
            </statement>
        </exercise>
        <exercise number="T33" xml:id="exercise-PD-T33">
            <statement>
                <p>Part of <xref ref="exercise-B-T50" acro="B.T50"/> is the half of the proof where we assume the matrix <m>A</m> is nonsingular and prove that a set is a basis.  In <xref ref="solution-B-T50" acro="B.T50"/> we proved directly that the set was both linearly independent and a spanning set.  Shorten this part of the proof by applying <xref ref="theorem-G" acro="G"/>.  Be careful, there is one subtlety.</p>
            </statement>
            <solution xml:id="solution-PD-T33">
                <p>By <xref ref="theorem-DCM" acro="DCM"/> we know that <m>\complex{n}</m> has dimension <m>n</m>.  So by <xref ref="theorem-G" acro="G"/> we need only establish that the set <m>C</m> is linearly independent or a spanning set.  However, the hypotheses also require that <m>C</m> be of size <m>n</m>.  We assumed that <m>B=\set{\vectorlist{x}{n}}</m> had size <m>n</m>, but there is no guarantee that <m>C=\set{A\vect{x}_1,\,A\vect{x}_2,\,A\vect{x}_3,\,\dots,\,A\vect{x}_n}</m> will have size <m>n</m>.  There could be some <q>collapsing</q> or <q>collisions.</q></p>
                <p>Suppose we establish that <m>C</m> is linearly independent.  Then <m>C</m> must have <m>n</m> distinct elements or else we could fashion a nontrivial relation of linear dependence involving duplicate elements.</p>
                <p>If we instead choose to prove that <m>C</m> is a spanning set, then we could establish the uniqueness of the elements of <m>C</m> quite easily.  Suppose that <m>A\vect{x}_i=A\vect{x}_j</m>.  Then<md>
                    <mrow>A(\vect{x}_i-\vect{x}_j)&amp;=A\vect{x}_i - A\vect{x}_j=\zerovector</mrow>
                </md>.  Since <m>A</m> is nonsingular, we conclude that <m>\vect{x}_i-\vect{x}_j=\zerovector</m>, or <m>\vect{x}_i=\vect{x}_j</m>, contrary to our description of <m>B</m>.</p>
            </solution>
        </exercise>
        <exercise number="T60" xml:id="exercise-PD-T60">
            <statement contributor="joeriegsecker">
                <p>Suppose that <m>W</m> is a vector space with dimension 5, and <m>U</m> and <m>V</m> are subspaces of <m>W</m>, each of dimension 3.  Prove that <m>U\cap V</m> contains a nonzero vector.  State a more general result.</p>
            </statement>
            <solution xml:id="solution-PD-T60">
                <p>Let <m>\set{\vect{u}_1,\,\vect{u}_2,\,\vect{u}_3}</m> and <m>\set{\vect{v}_1,\,\vect{v}_2,\,\vect{v}_3}</m> be bases for <m>U</m> and <m>V</m> (respectively).  Then, the set <m>\set{\vect{u}_1,\,\vect{u}_2,\,\vect{u}_3,\,\vect{v}_1,\,\vect{v}_2,\,\vect{v}_3}</m> is linearly dependent, since <xref ref="theorem-G" acro="G"/> says we cannot have 6 linearly independent vectors in a vector space of dimension 5.  So we can assert that there is a nontrivial relation of linear dependence,<me>a_1\vect{u}_1+a_2\vect{u}_2+a_3\vect{u}_3+b_1\vect{v}_1+b_2\vect{v}_2+b_3\vect{v}_3=\zerovector</me>where <m>a_1,\,a_2,\,a_3</m> and <m>b_1,\,b_2,\,b_3</m> are not all zero.</p>
                <p>We can rearrange this equation as<me>a_1\vect{u}_1+a_2\vect{u}_2+a_3\vect{u}_3=-b_1\vect{v}_1-b_2\vect{v}_2-b_3\vect{v}_3</me>This is an equality of two vectors, so we can give this common vector a name, say <m>\vect{w}</m>,<me>\vect{w}=a_1\vect{u}_1+a_2\vect{u}_2+a_3\vect{u}_3=-b_1\vect{v}_1-b_2\vect{v}_2-b_3\vect{v}_3</me>This is the desired nonzero vector, as we will now show.</p>
                <p>First, since <m>\vect{w}=a_1\vect{u}_1+a_2\vect{u}_2+a_3\vect{u}_3</m>, we can see that <m>\vect{w}\in U</m>.  Similarly, <m>\vect{w}=-b_1\vect{v}_1-b_2\vect{v}_2-b_3\vect{v}_3</m>, so <m>\vect{w}\in V</m>.  This establishes that <m>\vect{w}\in U\cap V</m> (<xref ref="definition-SI" acro="SI"/>).</p>
                <p>Is <m>\vect{w}\neq\zerovector</m>?  Suppose not, in other words, suppose <m>\vect{w}=\zerovector</m>.  Then<me>\zerovector=\vect{w}=a_1\vect{u}_1+a_2\vect{u}_2+a_3\vect{u}_3</me>Because <m>\set{\vect{u}_1,\,\vect{u}_2,\,\vect{u}_3}</m> is a basis for <m>U</m>, it is a linearly independent set and the relation of linear dependence above means we must conclude that <m>a_1=a_2=a_3=0</m>.  By a similar process, we would conclude that <m>b_1=b_2=b_3=0</m>.  But this is a contradiction since <m>a_1,\,a_2,\,a_3,\,b_1,\,b_2,\,b_3</m> were chosen so that some were nonzero.  So <m>\vect{w}\neq\zerovector</m>.</p>
                <p>How does this generalize?  All we really needed was the original relation of linear dependence that resulted because we had <q>too many</q> vectors in <m>W</m>.  A more general statement would be: Suppose that <m>W</m> is a vector space with dimension <m>n</m>, <m>U</m> is a subspace of dimension <m>p</m> and <m>V</m> is a subspace of dimension <m>q</m>.  If <m>p+q\gt n</m>, then <m>U\cap V</m> contains a nonzero vector.</p>
            </solution>
            <solution xml:id="solution-PD-T60-alternate">
                <p>The previous solution to this exercise is reminiscent of techniques from the proof of <xref ref="theorem-SID"/>.  If we assume the theorem, then a solution becomes much more direct.  The key observation is that the subspace <m>U+V</m> is a subspace of <m>W</m> and so cannot have dimension greater than <m>5</m>, since a larger basis would violate <xref ref="theorem-G"/>.  So we have<md>
                    <mrow>\dimension{U\cap V} &amp;= \dimension{U} + \dimension{V} - \dimension{U+V}&amp;&amp;<xref ref="theorem-SID"/></mrow>
                    <mrow>&amp;= 3 + 3 - \dimension{U+V}&amp;&amp;\text{Hypothesis}</mrow>
                    <mrow>&amp;\geq 6 - 5 = 1&amp;&amp;<xref ref="theorem-G"/></mrow>
                </md>So <m>U\cap V</m> is a nontrivial subspace and so contains a nonzero vector.</p>
            </solution>
        </exercise>
    </exercises>
</section>
