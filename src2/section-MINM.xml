<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the book                -->
<!--                                              -->
<!--      A First Course in Linear Algebra        -->
<!--                                              -->
<!-- Copyright (C) 2004-2017  Robert A. Beezer    -->
<!-- See the file COPYING for copying conditions. -->

<section xml:id="section-MINM" acro="MINM">
    <title>Matrix Inverses and Nonsingular Matrices</title>
    <introduction>
        <p>We saw in <xref ref="theorem-CINM" acro="CINM"/> that if a square matrix <m>A</m> is nonsingular, then there is a matrix <m>B</m> so that <m>AB=I_n</m>.  In other words, <m>B</m> is halfway to being an inverse of <m>A</m>.  We will see in this section that <m>B</m> automatically fulfills the second condition (<m>BA=I_n</m>).  <xref ref="example-MWIAA" acro="MWIAA"/> showed us that the coefficient matrix from Archetype<nbsp/><xref ref="archetype-A" acro="A" text="global"/> had no inverse.  Not coincidentally, this coefficient matrix is singular.  We will make all these connections precise now.  Not many examples or definitions in this section, just theorems.</p>
    </introduction>
    <subsection xml:id="subsection-MINM-NMI" acro="NMI">
        <title>Nonsingular Matrices are Invertible</title>
        <p><subject key="math.la.d.mat.singular.inv"/></p>
        <p>We need a couple of technical results for starters.  Some books would call these minor, but essential, results <term>lemmas</term>.  We'll just call 'em theorems.  See Proof Technique<nbsp/><xref ref="technique-LC" acro="LC" text="global"/> for more on the distinction.</p>
        <p>The first of these technical results is interesting in that the hypothesis says something about a product of two square matrices and the conclusion then says the same thing about each individual matrix in the product.  This result has an analogy in the algebra of complex numbers: suppose <m>\alpha,\,\beta\in\complexes</m>, then <m>\alpha\beta\neq 0</m> if and only if <m>\alpha\neq 0</m> and <m>\beta\neq 0</m>.  We can view this result as suggesting that the term <q>nonsingular</q> for matrices is like the term <q>nonzero</q> for scalars.  Consider too that we know singular matrices, as coefficient matrices for systems of equations, will sometimes lead to systems with no solutions, or systems with infinitely many solutions (<xref ref="theorem-NMUS" acro="NMUS"/>).  What do linear equations with zero look like?  Consider <m>0x=5</m>, which has no solution, and <m>0x=0</m>, which has infinitely many solutions.  In the algebra of scalars, zero is exceptional (meaning different, not better), and in the algebra of matrices, singular matrices are also the exception.  While there is only one zero scalar, and there are infinitely many singular matrices, we will see that singular matrices are a distinct minority.</p>
        <theorem xml:id="theorem-NPNT" acro="NPNT">
            <title>Nonsingular Product has Nonsingular Terms</title>
            <idx>
                <h>nonsingular matrix</h>
                <h>product of nonsingular matrices</h>
            </idx>
            <statement>
                <p>Suppose that <m>A</m> and <m>B</m> are square matrices of size <m>n</m>.  The product <m>AB</m> is nonsingular if and only if <m>A</m> and <m>B</m> are both nonsingular.</p>
            </statement>
            <proof>
                <case direction="forward">
                    <p>For this portion of the proof we will form the logically-equivalent contrapositive and prove that statement using two cases.  <q><m>AB</m> is nonsingular implies <m>A</m> and <m>B</m> are both nonsingular</q> becomes <q><m>A</m> or <m>B</m> is singular implies <m>AB</m> is singular.</q>  (Be sure to undertstand why the <q>and</q> became an <q>or</q>, see Proof Technique<nbsp/><xref ref="technique-CP" acro="CP" text="global"/>.)</p>
                    <p>Case 1.  Suppose <m>B</m> is singular.  Then there is a nonzero vector <m>\vect{z}</m> that is a solution to <m>\homosystem{B}</m>.  So<md>
                        <mrow>(AB)\vect{z}
                        &amp;=A(B\vect{z})&amp;&amp;
                            <xref ref="theorem-MMA" acro="MMA"/></mrow>
                        <mrow>&amp;=A\zerovector&amp;&amp;
                            <xref ref="theorem-SLEMM" acro="SLEMM"/></mrow>
                        <mrow>&amp;=\zerovector&amp;&amp;
                            <xref ref="theorem-MMZM" acro="MMZM"/></mrow>
                    </md>.</p>
                    <p>With <xref ref="theorem-SLEMM" acro="SLEMM"/> we can translate this vector equality to the statement that <m>\vect{z}</m> is a nonzero solution to <m>\homosystem{AB}</m>.  Thus <m>AB</m> is singular (<xref ref="definition-NM" acro="NM"/>), as desired.</p>
                    <p>Case 2.  Suppose <m>A</m> is singular, and <m>B</m> is not singular.  In other words, with Case 1 complete, we can be more precise about this remaining case and assume that <m>B</m> is nonsingular.  Because <m>A</m> is singular, there is a nonzero vector <m>\vect{y}</m> that is a solution to <m>\homosystem{A}</m>.  Now consider the linear system <m>\linearsystem{B}{\vect{y}}</m>.  Since <m>B</m> is nonsingular, the system has a unique solution (<xref ref="theorem-NMUS" acro="NMUS"/>), which we will denote as <m>\vect{w}</m>.  We first claim <m>\vect{w}</m> is not the zero vector either.  Assuming the opposite, suppose that <m>\vect{w}=\zerovector</m> (Proof Technique<nbsp/><xref ref="technique-CD" acro="CD" text="global"/>).  Then<md>
                        <mrow>\vect{y}
                        &amp;=B\vect{w}&amp;&amp;
                            <xref ref="theorem-SLEMM" acro="SLEMM"/></mrow>
                        <mrow>&amp;=B\zerovector&amp;&amp;\text{Hypothesis}</mrow>
                        <mrow>&amp;=\zerovector&amp;&amp;
                            <xref ref="theorem-MMZM" acro="MMZM"/></mrow><intertext>contrary to <m>\vect{y}</m> being nonzero.  So <m>\vect{w}\neq\zerovector</m>.  The pieces are in place, so here we go,</intertext><mrow>(AB)\vect{w}
                        &amp;=A(B\vect{w})&amp;&amp;
                            <xref ref="theorem-MMA" acro="MMA"/></mrow>
                        <mrow>&amp;=A\vect{y}&amp;&amp;
                            <xref ref="theorem-SLEMM" acro="SLEMM"/></mrow>
                        <mrow>&amp;=\zerovector&amp;&amp;
                            <xref ref="theorem-SLEMM" acro="SLEMM"/></mrow>
                    </md>.</p>
                    <p>With <xref ref="theorem-SLEMM" acro="SLEMM"/> we can translate this vector equality to the statement that <m>\vect{w}</m> is a nonzero solution to <m>\homosystem{AB}</m>.  Thus <m>AB</m> is singular (<xref ref="definition-NM" acro="NM"/>), as desired.  And this conclusion holds for both cases.</p>
                </case>
                <case direction="backward">
                    <p> Now assume that both <m>A</m> and <m>B</m> are nonsingular.  Suppose that <m>\vect{x}\in\complex{n}</m> is a solution to <m>\homosystem{AB}</m>.  Then<md>
                        <mrow>\zerovector
                        &amp;=\left(AB\right)\vect{x}&amp;&amp;
                            <xref ref="theorem-SLEMM" acro="SLEMM"/></mrow>
                        <mrow>&amp;=A\left(B\vect{x}\right)&amp;&amp;
                            <xref ref="theorem-MMA" acro="MMA"/></mrow>
                    </md>.</p>
                    <p>By <xref ref="theorem-SLEMM" acro="SLEMM"/>, <m>B\vect{x}</m> is a solution to <m>\homosystem{A}</m>, and by the definition of a nonsingular matrix (<xref ref="definition-NM" acro="NM"/>), we conclude that <m>B\vect{x}=\zerovector</m>.  Now, by an entirely similar argument, the nonsingularity of <m>B</m> forces us to conclude that <m>\vect{x}=\zerovector</m>.  So the only solution to <m>\homosystem{AB}</m> is the zero vector and we conclude that <m>AB</m> is nonsingular by <xref ref="definition-NM" acro="NM"/>.</p>
                </case>
            </proof>
        </theorem>
        <p>This is a powerful result in the <q>forward</q> direction, because it allows us to begin with a hypothesis that something complicated (the matrix product <m>AB</m>) has the property of being nonsingular, and we can then conclude that the simpler constituents (<m>A</m> and <m>B</m> individually) then also have the property of being nonsingular.  If we had thought that the matrix product was an artificial construction, results like this would make us begin to think twice.</p>
        <p>The contrapositive of this entire result is equally interesting.  It says that <m>A</m> or <m>B</m> (or both) is a singular matrix if and only if the product <m>AB</m> is singular.  (See Proof Technique<nbsp/><xref ref="technique-CP" acro="CP" text="global"/>.)</p>
        <!--   TODO: Insert reference to a technique about negations. -->
        <theorem xml:id="theorem-OSIS" acro="OSIS">
            <title>One-Sided Inverse is Sufficient</title>
            <idx>
                <h>matrix inverse</h>
                <h>one-sided</h>
            </idx>
            <statement>
                <p>Suppose <m>A</m> and <m>B</m> are  square matrices of size <m>n</m> such that <m>AB=I_n</m>.  Then <m>BA=I_n</m>.</p>
            </statement>
            <proof>
                <p>The matrix <m>I_n</m> is nonsingular (since it row-reduces easily to <m>I_n</m>, <xref ref="theorem-NMRRI" acro="NMRRI"/>).  So <m>A</m> and <m>B</m> are nonsingular by <xref ref="theorem-NPNT" acro="NPNT"/>, so in particular <m>B</m> is nonsingular.  We can therefore apply <xref ref="theorem-CINM" acro="CINM"/> to assert the existence of a matrix <m>C</m> so that <m>BC=I_n</m>.  This application of <xref ref="theorem-CINM" acro="CINM"/> could be a bit confusing, mostly because of the names of the matrices involved.  <m>B</m> is nonsingular, so there must be a <q>right-inverse</q> for <m>B</m>, and we are calling it <m>C</m>.</p>
                <p>Now<md>
                    <mrow>BA
                    &amp;=(BA)I_n&amp;&amp;
                        <xref ref="theorem-MMIM" acro="MMIM"/></mrow>
                    <mrow>&amp;=(BA)(BC)&amp;&amp;
                        <xref ref="theorem-CINM" acro="CINM"/></mrow>
                    <mrow>&amp;=B(AB)C&amp;&amp;
                        <xref ref="theorem-MMA" acro="MMA"/></mrow>
                    <mrow>&amp;=BI_nC&amp;&amp;\text{Hypothesis}</mrow>
                    <mrow>&amp;=BC&amp;&amp;
                        <xref ref="theorem-MMIM" acro="MMIM"/></mrow>
                    <mrow>&amp;=I_n&amp;&amp;
                        <xref ref="theorem-CINM" acro="CINM"/></mrow>
                </md>which is the desired conclusion.</p>
            </proof>
        </theorem>
        <p>So <xref ref="theorem-OSIS" acro="OSIS"/> tells us that if <m>A</m> is nonsingular, then the matrix <m>B</m> guaranteed by <xref ref="theorem-CINM" acro="CINM"/> will be both a <q>right-inverse</q> and a <q>left-inverse</q> for <m>A</m>, so <m>A</m> is invertible and <m>\inverse{A}=B</m>.</p>
        <p>So if you have a nonsingular matrix, <m>A</m>, you can use the procedure described in <xref ref="theorem-CINM" acro="CINM"/> to find an inverse for <m>A</m>.  If <m>A</m> is singular, then the procedure in <xref ref="theorem-CINM" acro="CINM"/> will fail as the first <m>n</m> columns of <m>M</m> will not row-reduce to the identity matrix.  However, we can say a bit more.  When <m>A</m> is singular, then <m>A</m> does not have an inverse (which is very different from saying that the procedure in <xref ref="theorem-CINM" acro="CINM"/> fails to find an inverse).  This may feel like we are splitting hairs, but it is important that we do not make unfounded assumptions.  These observations motivate the next theorem.</p>
        <theorem xml:id="theorem-NI" acro="NI">
            <title>Nonsingularity is Invertibility</title>
            <idx>
                <h>matrix inverse</h>
                <h>nonsingular matrix</h>
            </idx>
            <statement>
                <idx>
                    <h>nonsingular matrix</h>
                    <h>matrix inverse</h>
                </idx>
                <p>Suppose that <m>A</m> is a square matrix.  Then <m>A</m> is nonsingular if and only if <m>A</m> is invertible.</p>
            </statement>
            <proof>
                <case direction="backward">
                    <p>Since <m>A</m> is invertible, we can write <m>I_n=A\inverse{A}</m> (<xref ref="definition-MI" acro="MI"/>).  Notice that <m>I_n</m> is nonsingular (<xref ref="theorem-NMRRI" acro="NMRRI"/>) so <xref ref="theorem-NPNT" acro="NPNT"/> implies that <m>A</m> (and <m>\inverse{A}</m>) is nonsingular.</p>
                </case>
                <case direction="forward">
                    <p>Suppose now that <m>A</m> is nonsingular.  By <xref ref="theorem-CINM" acro="CINM"/> we find <m>B</m> so that <m>AB=I_n</m>.  Then <xref ref="theorem-OSIS" acro="OSIS"/> tells us that <m>BA=I_n</m>.  So <m>B</m> is <m>A</m>'s inverse, and by construction, <m>A</m> is invertible.</p>
                </case>
            </proof>
        </theorem>
        <p>So for a square matrix, the properties of having an inverse and of having a trivial null space are one and the same.  Cannot have one without the other.</p>
        <theorem xml:id="theorem-NME3" acro="NME3">
            <title>Nonsingular Matrix Equivalences, Round 3</title>
            <idx>
                <h>nonsingular matrix</h>
                <h>equivalences</h>
            </idx>
            <statement>
                <p>Suppose that <m>A</m> is a square matrix of size <m>n</m>.  The following are equivalent.<ol>
                    <li><m>A</m> is nonsingular.</li>
                    <li><m>A</m> row-reduces to the identity matrix.</li>
                    <li> The null space of <m>A</m> contains only the zero vector, <m>\nsp{A}=\set{\zerovector}</m>.</li>
                    <li> The linear system <m>\linearsystem{A}{\vect{b}}</m> has a unique solution for every possible choice of <m>\vect{b}</m>.</li>
                    <li> The columns of <m>A</m> are a linearly independent set.</li>
                    <li><m>A</m> is invertible.</li>
                </ol></p>
            </statement>
            <proof>
                <p>We can update our list of equivalences for nonsingular matrices (<xref ref="theorem-NME2" acro="NME2"/>) with the equivalent condition from <xref ref="theorem-NI" acro="NI"/>.</p>
            </proof>
        </theorem>
        <p>In the case that <m>A</m> is a nonsingular coefficient matrix of a system of equations, the inverse allows us to very quickly compute the unique solution, for any vector of constants.</p>
        <theorem xml:id="theorem-SNCM" acro="SNCM">
            <title>Solution with Nonsingular Coefficient Matrix</title>
            <idx>
                <h>coefficient matrix</h>
                <h>nonsingular</h>
            </idx>
            <statement>
                <p>Suppose that <m>A</m> is nonsingular.  Then the unique solution to <m>\linearsystem{A}{\vect{b}}</m> is <m>\inverse{A}\vect{b}</m>.</p>
            </statement>
            <proof>
                <p>By <xref ref="theorem-NMUS" acro="NMUS"/> we know already that <m>\linearsystem{A}{\vect{b}}</m> has a unique solution for every choice of <m>\vect{b}</m>.  We need to show that the expression stated is indeed a solution (<em>the</em> solution).  That is easy, just <q>plug it in</q> to the vector equation representation of the system (<xref ref="theorem-SLEMM" acro="SLEMM"/>)<md>
                    <mrow>A\left(\inverse{A}\vect{b}\right)
                    &amp;=\left(A\inverse{A}\right)\vect{b}&amp;&amp;
                        <xref ref="theorem-MMA" acro="MMA"/></mrow>
                    <mrow>&amp;=I_n\vect{b}&amp;&amp;
                        <xref ref="definition-MI" acro="MI"/></mrow>
                    <mrow>&amp;=\vect{b}&amp;&amp;
                        <xref ref="theorem-MMIM" acro="MMIM"/></mrow>
                    </md>.</p>
                <p>Since <m>A\vect{x}=\vect{b}</m> is true when we substitute <m>\inverse{A}\vect{b}</m> for <m>\vect{x}</m>, <m>\inverse{A}\vect{b}</m> is a (the!) solution to <m>\linearsystem{A}{\vect{b}}</m>.</p>
            </proof>
        </theorem>
        <computation xml:id="sage-MI" acro="MI">
            <title>Matrix Inverse</title>
            <idx>matrix inverse</idx>
            <p>Now we know that invertibility is equivalent to nonsingularity, and that the procedure outlined in <xref ref="theorem-CINM" acro="CINM"/> will always yield an inverse for a nonsingular matrix.  But rather than using that procedure, Sage implements a <c>.inverse()</c> method.  In the following, we compute the inverse of a <m>3\times 3</m> matrix, and then purposely convert it to a singular matrix by replacing the last column by a linear combination of the first two.</p>
            <sage xml:id="sagecell-MI-1">
                <input>
                A = matrix(QQ,[[ 3,  7, -6],
                               [ 2,  5, -5],
                               [-3, -8,  8]])
                A.is_singular()
                </input>
                <output>
                False
                </output>
            </sage>
            <sage xml:id="sagecell-MI-2">
                <input>
                Ainv = A.inverse(); Ainv
                </input>
                <output>
                [ 0  8  5]
                [ 1 -6 -3]
                [ 1 -3 -1]
                </output>
            </sage>
            <sage xml:id="sagecell-MI-3">
                <input>
                A*Ainv
                </input>
                <output>
                [1 0 0]
                [0 1 0]
                [0 0 1]
                </output>
            </sage>
            <sage xml:id="sagecell-MI-4">
                <input>
                col_0 = A.column(0)
                col_1 = A.column(1)
                C = column_matrix([col_0, col_1, 2*col_0 - 4*col_1])
                C.is_singular()
                </input>
                <output>
                True
                </output>
            </sage>
            <sage xml:id="sagecell-MI-5">
                <input>
                C.inverse()
                </input>
                <output>
                Traceback (most recent call last):
                ...
                ZeroDivisionError: input matrix must be nonsingular
                </output>
            </sage>
            <p>Notice how the failure to invert <c>C</c> is explained by the matrix being singular.</p>
            <p>Systems with nonsingular coefficient matrices can be solved easily with the matrix inverse.  We will recycle <c>A</c> as a coefficient matrix, so be sure to execute the code above.</p>
            <sage xml:id="sagecell-MI-6">
                <input>
                const = vector(QQ, [2, -1, 4])
                A.solve_right(const)
                </input>
                <output>
                (12, -4, 1)
                </output>
            </sage>
            <sage xml:id="sagecell-MI-7">
                <input>
                A.inverse()*const
                </input>
                <output>
                (12, -4, 1)
                </output>
            </sage>
            <sage xml:id="sagecell-MI-8">
                <input>
                A.solve_right(const) == A.inverse()*const
                </input>
                <output>
                True
                </output>
            </sage>
            <p>If you find it more convenient, you can use the same notation as the text for a matrix inverse.</p>
            <sage xml:id="sagecell-MI-9">
                <input>
                A^-1
                </input>
                <output>
                [ 0  8  5]
                [ 1 -6 -3]
                [ 1 -3 -1]
                </output>
            </sage>
        </computation>
        <computation xml:id="sage-NME3" acro="NME3">
            <title>Nonsingular Matrix Equivalences, Round 3</title>
            <idx>nonsingular matrix equivalences, round 3</idx>
            <p>For square matrices, Sage has the methods <c>.is_singular()</c> and <c>.is_invertible()</c>.  By <xref ref="theorem-NI" acro="NI"/> we know these two functions to be logical opposites.  One way to express this is that these two methods will always return different values.  Here we demonstrate with a nonsingular matrix and a singular matrix.  The comparison <c>!=</c> is <q>not equal.</q></p>
            <sage xml:id="sagecell-NME3-1">
                <input>
                nonsing = matrix(QQ, [[ 0, -1,  1, -3],
                                      [ 1,  1,  0, -3],
                                      [ 0,  4, -3,  8],
                                      [-2, -4,  1,  5]])
                nonsing.is_singular() != nonsing.is_invertible()
                </input>
                <output>
                True
                </output>
            </sage>
            <sage xml:id="sagecell-NME3-2">
                <input>
                sing = matrix(QQ, [[ 1, -2, -6,  8],
                                   [-1,  3,  7, -8],
                                   [ 0, -4, -3, -2],
                                   [ 0,  3,  1,  4]])
                sing.is_singular() != sing.is_invertible()
                </input>
                <output>
                True
                </output>
            </sage>
            <p>We could test other properties of the matrix inverse, such as <xref ref="theorem-SS" acro="SS"/>.</p>
            <sage xml:id="sagecell-NME3-3">
                <input>
                A = matrix(QQ, [[ 3, -5, -2,  8],
                                [-1,  2,  0, -1],
                                [-2,  4,  1, -4],
                                [ 4, -5,  0,  8]])
                B = matrix(QQ, [[ 1,  2,  4, -1],
                                [-2, -3, -8, -2],
                                [-2, -4, -7,  5],
                                [ 2,  5,  7, -8]])
                A.is_invertible() and B.is_invertible()
                </input>
                <output>
                True
                </output>
            </sage>
            <sage xml:id="sagecell-NME3-4">
                <input>
                (A*B).inverse() == B.inverse()*A.inverse()
                </input>
                <output>
                True
                </output>
            </sage>
        </computation>
    </subsection>
    <subsection xml:id="subsection-MINM-UM" acro="UM">
        <title>Unitary Matrices</title>
        <p>Recall that the adjoint of a matrix is <m>\adjoint{A}=\transpose{\left(\conjugate{A}\right)}</m> (<xref ref="definition-A" acro="A"/>).</p>
        <definition xml:id="definition-UM" acro="UM">
            <title>Unitary Matrices</title>
            <idx>
                <h>matrix</h>
                <h>unitary</h>
            </idx>
            <statement>
                <p>Suppose that <m>U</m> is a square matrix of size <m>n</m> such that <m>\adjoint{U}U=I_n</m>.  Then we say <m>U</m> is <term>unitary</term>.</p>
            </statement>
        </definition>
        <p>This condition may seem rather far-fetched at first glance.  Would there be <em>any</em> matrix that behaved this way?  Well, yes, here is one.</p>
        <example xml:id="example-UM3" acro="UM3">
            <title>Unitary matrix of size 3</title>
            <idx>
                <h>unitary</h>
                <h>size 3</h>
            </idx>
            <p>Let <me>U=
            \begin{bmatrix}
            \frac{1 + i }{{\sqrt{5}}} &amp;
               \frac{3 + 2\,i }{{\sqrt{55}}} &amp;
               \frac{2+2i}{\sqrt{22}} \\
            \frac{1 - i }{{\sqrt{5}}} &amp;
               \frac{2 + 2\,i }{{\sqrt{55}}} &amp;
               \frac{-3 + i }{{\sqrt{22}}} \\
            \frac{i }{{\sqrt{5}}} &amp;
               \frac{3 - 5\,i }{{\sqrt{55}}} &amp;
               -\frac{2}{\sqrt{22}}
            \end{bmatrix}</me>.  The computations get a bit tiresome, but if you work your way through the computation of <m>\adjoint{U}U</m>, you <em>will</em> arrive at the <m>3\times 3</m> identity matrix <m>I_3</m>.</p>
        </example>
        <!--   Above example from Mathematica input:                            -->
        <!--   GramSchmidt[{{1 + I, 1 - I, I}, {I, 1 + I, -I}, {I, -1 + I, 1}}, -->
        <!--               InnerProduct -> (Conjugate[#1].#2 &)] // Simplify    -->
        <p>Unitary matrices do not have to look quite so gruesome.  Here is a larger one that is a bit more pleasing.</p>
        <example xml:id="example-UPM" acro="UPM">
            <title>Unitary permutation matrix</title>
            <idx>
                <h>unitary</h>
                <h>permutation matrix</h>
            </idx>
            <p>The matrix<me>P=
            \begin{bmatrix}
            0&amp;1&amp;0&amp;0&amp;0\\
            0&amp;0&amp;0&amp;1&amp;0\\
            1&amp;0&amp;0&amp;0&amp;0\\
            0&amp;0&amp;0&amp;0&amp;1\\
            0&amp;0&amp;1&amp;0&amp;0
            \end{bmatrix}</me>is unitary as can be easily checked.  Notice that it is just a rearrangement of the columns of the <m>5\times 5</m> identity matrix, <m>I_5</m> (<xref ref="definition-IM" acro="IM"/>).</p>
            <p>An interesting exercise is to build another <m>5\times 5</m> unitary matrix, <m>R</m>, using a different rearrangement of the columns of <m>I_5</m>.  Then form the product <m>PR</m>.  This will be another unitary matrix (<xref ref="exercise-MINM-T10" acro="MINM.T10"/>).  If you were to build all <m>5!=5\times 4\times 3\times 2\times 1=120</m> matrices of this type you would have a set that remains closed under matrix multiplication.  It is an example of another algebraic structure known as a <term>group</term> since together the set and the one operation (matrix multiplication here) is closed, associative, has an identity (<m>I_5</m>), and inverses (<xref ref="theorem-UMI" acro="UMI"/>).  Notice though that the operation in this group is not commutative!</p>
        </example>
        <p>If a matrix <m>A</m> has only real number entries (we say it is a <term>real matrix</term>) then the defining property of being unitary simplifies to <m>\transpose{A}A=I_n</m>.  In this case we, and everybody else, call the matrix <term>orthogonal</term>, so you may often encounter this term in your other reading when the complex numbers are not under consideration.</p>
        <p>Unitary matrices have easily computed inverses.  They also have columns that form orthonormal sets.  Here are the theorems that show us that unitary matrices are not as strange as they might initially appear.</p>
        <theorem xml:id="theorem-UMI" acro="UMI">
            <title>Unitary Matrices are Invertible</title>
            <idx>
                <h>matrix</h>
                <h>unitary is invertible</h>
            </idx>
            <statement>
                <p>Suppose that <m>U</m> is a unitary matrix of size <m>n</m>.  Then <m>U</m> is nonsingular, and <m>\inverse{U}=\adjoint{U}</m>.</p>
            </statement>
            <proof>
                <p>By <xref ref="definition-UM" acro="UM"/>, we know that <m>\adjoint{U}U=I_n</m>.  The matrix <m>I_n</m> is nonsingular (since it row-reduces easily to <m>I_n</m>, <xref ref="theorem-NMRRI" acro="NMRRI"/>).  So by <xref ref="theorem-NPNT" acro="NPNT"/>, <m>U</m> and <m>\adjoint{U}</m> are both nonsingular matrices.</p>
                <p>The equation <m>\adjoint{U}U=I_n</m> gets us halfway to an inverse of <m>U</m>, and <xref ref="theorem-OSIS" acro="OSIS"/> tells us that then <m>U\adjoint{U}=I_n</m> also.  So <m>U</m> and <m>\adjoint{U}</m> are inverses of each other (<xref ref="definition-MI" acro="MI"/>).</p>
            </proof>
        </theorem>
        <theorem xml:id="theorem-CUMOS" acro="CUMOS">
            <title>Columns of Unitary Matrices are Orthonormal Sets</title>
            <idx>
                <h>unitary matrices</h>
                <h>columns</h>
            </idx>
            <statement>
                <p>Suppose that <m>S=\set{\vectorlist{A}{n}}</m> is the set of columns of a square matrix <m>A</m> of size <m>n</m>.  Then <m>A</m> is a unitary matrix if and only if <m>S</m> is an orthonormal set.</p>
            </statement>
            <proof>
                <p>The proof revolves around recognizing that a typical entry of the product <m>\adjoint{A}A</m> is an inner product of columns of <m>A</m>.  Here are the details to support this claim.<md>
                <mrow>\matrixentry{\adjoint{A}A}{ij}
                &amp;=\sum_{k=1}^{n}\matrixentry{\adjoint{A}}{ik}\matrixentry{A}{kj}&amp;&amp;
                    <xref ref="theorem-EMP" acro="EMP"/></mrow>
                <mrow>&amp;=\sum_{k=1}^{n}\matrixentry{\transpose{\conjugate{A}}}{ik}\matrixentry{A}{kj}&amp;&amp;
                    <xref ref="theorem-EMP" acro="EMP"/></mrow>
                <mrow>&amp;=\sum_{k=1}^{n}\matrixentry{\,\conjugate{A}\,}{ki}\matrixentry{A}{kj}&amp;&amp;
                    <xref ref="definition-TM" acro="TM"/></mrow>
                <mrow>&amp;=\sum_{k=1}^{n}\conjugate{\matrixentry{A}{ki}}\matrixentry{A}{kj}&amp;&amp;
                    <xref ref="definition-CCM" acro="CCM"/></mrow>
                <mrow>&amp;=\sum_{k=1}^{n}\conjugate{\vectorentry{\vect{A}_i}{k}}\vectorentry{\vect{A}_j}{k}</mrow>
                <mrow>&amp;=\innerproduct{\vect{A}_i}{\vect{A}_j}&amp;&amp;
                    <xref ref="definition-IP" acro="IP"/></mrow>
                </md></p>
            <p>We now employ this equality in a chain of equivalences,<md>
                <mrow>&amp;S=\set{\vectorlist{A}{n}}\text{ is an orthonormal set}</mrow>
                <mrow>&amp;\iff \innerproduct{\vect{A}_i}{\vect{A}_j}=
                \begin{cases}
                0 &amp;\text{if }i\neq j\\
                1 &amp;\text{if }i=j
                \end{cases}&amp;&amp;
                    <xref ref="definition-ONS" acro="ONS"/></mrow>
                <mrow>&amp;\iff \matrixentry{\adjoint{A}A}{ij}=
                \begin{cases}
                0 &amp;\text{if }i\neq j\\
                1 &amp;\text{if }i=j
                \end{cases}</mrow>
                <mrow>&amp;\iff \matrixentry{\adjoint{A}A}{ij}=\matrixentry{I_n}{ij},\ 1\leq i\leq n,\ 1\leq j\leq n&amp;&amp;
                    <xref ref="definition-IM" acro="IM"/></mrow>
                <mrow>&amp;\iff \adjoint{A}A=I_n&amp;&amp;
                    <xref ref="definition-ME" acro="ME"/></mrow>
                <mrow>&amp;\iff A\text{ is a unitary matrix}&amp;&amp;
                    <xref ref="definition-UM" acro="UM"/></mrow>
            </md></p>
            </proof>
        </theorem>
        <example xml:id="example-OSMC" acro="OSMC">
            <title>Orthonormal set from matrix columns</title>
            <idx>
                <h>orthonormal</h>
                <h>matrix columns</h>
            </idx>
            <p>The matrix<me>U=
            \begin{bmatrix}
            \frac{1 + i }{{\sqrt{5}}} &amp;
               \frac{3 + 2\,i }{{\sqrt{55}}} &amp;
               \frac{2+2i}{\sqrt{22}} \\
            \frac{1 - i }{{\sqrt{5}}} &amp;
               \frac{2 + 2\,i }{{\sqrt{55}}} &amp;
               \frac{-3 + i }{{\sqrt{22}}} \\
            \frac{i }{{\sqrt{5}}} &amp;
               \frac{3 - 5\,i }{{\sqrt{55}}} &amp;
               -\frac{2}{\sqrt{22}}
            \end{bmatrix}</me>from <xref ref="example-UM3" acro="UM3"/> is a unitary matrix.  By <xref ref="theorem-CUMOS" acro="CUMOS"/>, its columns<me>\set{
            \colvector{
            \frac{1 + i }{{\sqrt{5}}}\\
               \frac{1 - i }{{\sqrt{5}}}\\
               \frac{i }{{\sqrt{5}}}
            },\,
            \colvector{
            \frac{3 + 2\,i }{{\sqrt{55}}}\\
               \frac{2 + 2\,i }{{\sqrt{55}}}\\
               \frac{3 - 5\,i }{{\sqrt{55}}}
            },\,
            \colvector{
            \frac{2+2i}{\sqrt{22}}\\
               \frac{-3 + i }{{\sqrt{22}}}\\
               -\frac{2}{\sqrt{22}}
            }
            }</me>form an orthonormal set.  You might find checking the six inner products of pairs of these vectors easier than doing the matrix product <m>\adjoint{U}U</m>.  Or, because the inner product is anti-commutative (<xref ref="theorem-IPAC" acro="IPAC"/>) you only need check  three inner products (see <xref ref="exercise-MINM-T12" acro="MINM.T12"/>).</p>
        </example>
        <p>When using vectors and matrices that only have real number entries, orthogonal matrices are those matrices with inverses that equal their transpose.  Similarly, the inner product is the familiar dot product.  Keep this special case in mind as you read the next theorem.</p>
        <theorem xml:id="theorem-UMPIP" acro="UMPIP">
            <title>Unitary Matrices Preserve Inner Products</title>
            <idx>
                <h>unitary matrix</h>
                <h>inner product</h>
            </idx>
            <statement>
                <p>Suppose that <m>U</m> is a unitary matrix of size <m>n</m> and <m>\vect{u}</m> and <m>\vect{v}</m> are two vectors from <m>\complex{n}</m>.  Then<md>
                    <mrow>\innerproduct{U\vect{u}}{U\vect{v}}&amp;=\innerproduct{\vect{u}}{\vect{v}}
                    &amp;
                    &amp;\text{and}
                    &amp;
                    \norm{U\vect{v}}&amp;=\norm{\vect{v}}</mrow>
                </md>.</p>
            </statement>
            <proof>
                <p><md>
                    <mrow>\innerproduct{U\vect{u}}{U\vect{v}}
                    &amp;=\transpose{\left(\conjugate{U\vect{u}}\right)}U\vect{v}&amp;&amp;
                        <xref ref="theorem-MMIP" acro="MMIP"/></mrow>
                    <mrow>&amp;=\transpose{\left(\conjugate{U}\conjugate{\vect{u}}\right)}U\vect{v}&amp;&amp;
                        <xref ref="theorem-MMCC" acro="MMCC"/></mrow>
                    <mrow>&amp;=\transpose{\conjugate{\vect{u}}}\transpose{\conjugate{U}}U\vect{v}&amp;&amp;
                        <xref ref="theorem-MMT" acro="MMT"/></mrow>
                    <mrow>&amp;=\transpose{\conjugate{\vect{u}}}\adjoint{U}U\vect{v}&amp;&amp;
                        <xref ref="definition-A" acro="A"/></mrow>
                    <mrow>&amp;=\transpose{\conjugate{\vect{u}}}I_n\vect{v}&amp;&amp;
                        <xref ref="definition-UM" acro="UM"/></mrow>
                    <mrow>&amp;=\transpose{\conjugate{\vect{u}}}\vect{v}&amp;&amp;
                        <xref ref="theorem-MMIM" acro="MMIM"/></mrow>
                    <mrow>&amp;=\innerproduct{\vect{u}}{\vect{v}}&amp;&amp;
                        <xref ref="theorem-MMIP" acro="MMIP"/></mrow>
                </md></p>
                <p>The second conclusion is just a specialization of the first conclusion.<md>
                    <mrow>\norm{U\vect{v}}
                    &amp;=\sqrt{\norm{U\vect{v}}^2}</mrow>
                    <mrow>&amp;=\sqrt{\innerproduct{U\vect{v}}{U\vect{v}}}&amp;&amp;
                        <xref ref="theorem-IPN" acro="IPN"/></mrow>
                    <mrow>&amp;=\sqrt{\innerproduct{\vect{v}}{\vect{v}}}</mrow>
                    <mrow>&amp;=\sqrt{\norm{\vect{v}}^2}&amp;&amp;
                        <xref ref="theorem-IPN" acro="IPN"/></mrow>
                    <!-- push qed to the right margin -->
                    <mrow>&amp;=\norm{\vect{v}}&amp;&amp;</mrow>
                </md></p>
            </proof>
        </theorem>
        <p>Aside from the inherent interest in this theorem, it makes a bigger statement about unitary matrices.  When we view vectors geometrically as directions or forces, then the norm equates to a notion of length.  If we transform a vector by multiplication with a unitary matrix, then the length (norm) of that vector stays the same.  If we consider column vectors with two or three slots containing only real numbers, then the inner product of two such vectors is just the dot product, and this quantity can be used to compute the angle between two vectors.  When two vectors are multiplied (transformed) by the same unitary matrix, their dot product is unchanged and their individual lengths are unchanged.  This results in the angle between the two vectors remaining unchanged.</p>
        <p>A <term>unitary transformation</term> (matrix-vector products with unitary matrices) thus preserve geometrical relationships among vectors representing directions, forces, or other physical quantities.  In the case of a two-slot vector with real entries, this is simply a rotation.  These sorts of computations are exceedingly important in computer graphics such as games and real-time simulations, especially when increased realism is achieved by performing many such computations quickly.  We will see unitary matrices again in subsequent sections (especially <xref ref="theorem-OD" acro="OD"/>) and in each instance, consider the interpretation of the unitary matrix as a sort of geometry-preserving transformation.  Some authors use the term <term>isometry</term> to highlight this behavior.  We will speak loosely of a unitary matrix as being a sort of generalized rotation.</p>
        <computation xml:id="sage-UM" acro="UM">
            <title>Unitary Matrices</title>
            <idx>unitary matrices</idx>
            <p>No surprise about how we check if a matrix is unitary.  Here is <xref ref="example-UM3" acro="UM3"/>,</p>
            <sage xml:id="sagecell-UM-1">
                <input>
                A = matrix(QQbar, [
                    [(1+I)/sqrt(5), (3+2*I)/sqrt(55), (2+2*I)/sqrt(22)],
                    [(1-I)/sqrt(5), (2+2*I)/sqrt(55),  (-3+I)/sqrt(22)],
                    [    I/sqrt(5), (3-5*I)/sqrt(55),    (-2)/sqrt(22)]
                                  ])
                A.is_unitary()
                </input>
                <output>
                True
                </output>
            </sage>
            <sage xml:id="sagecell-UM-2">
                <input>
                A.conjugate_transpose() == A.inverse()
                </input>
                <output>
                True
                </output>
            </sage>
            <p>We can verify <xref ref="theorem-UMPIP" acro="UMPIP"/>, where the vectors <c>u</c> and <c>v</c> are created randomly.  Try evaluating this compute cell with your own choices.</p>
            <sage xml:id="sagecell-UM-3">
                <input>
                u = random_vector(QQ, 3) + QQbar(I)*random_vector(QQ, 3)
                v = random_vector(QQ, 3) + QQbar(I)*random_vector(QQ, 3)
                (A*u).hermitian_inner_product(A*v) == u.hermitian_inner_product(v)
                </input>
                <output>
                True
                </output>
            </sage>
            <p>If you want to experiment with permutation matrices, Sage has these too.  We can create a permutation matrix from a list that indicates for each column the row with a one in it.  Notice that the product here of two permutation matrices is again a permutation matrix.</p>
            <sage xml:id="sagecell-UM-4">
                <input>
                sigma = Permutation([2,3,1])
                S = sigma.to_matrix(); S
                </input>
                <output>
                [0 0 1]
                [1 0 0]
                [0 1 0]
                </output>
            </sage>
            <sage xml:id="sagecell-UM-5">
                <input>
                tau = Permutation([1,3,2])
                T = tau.to_matrix(); T
                </input>
                <output>
                [1 0 0]
                [0 0 1]
                [0 1 0]
                </output>
            </sage>
            <sage xml:id="sagecell-UM-6">
                <input>
                S*T
                </input>
                <output>
                [0 1 0]
                [1 0 0]
                [0 0 1]
                </output>
            </sage>
            <sage xml:id="sagecell-UM-7">
                <input>
                rho = Permutation([2, 1, 3])
                S*T == rho.to_matrix()
                </input>
                <output>
                True
                </output>
            </sage>
        </computation>
        <p>A final reminder:  the terms <q>dot product,</q> <q>symmetric matrix</q> and <q>orthogonal matrix</q> used in reference to vectors or matrices with real number entries are special cases of the terms <q>inner product,</q> <q>Hermitian matrix</q> and <q>unitary matrix</q> that we use for vectors or matrices with complex number entries, so keep that in mind as you read elsewhere.</p>
    </subsection>
    <exercises xml:id="readingquestions-MINM">
        <title>Reading Questions</title>
        <exercise xml:id="reading-MINM-1">
            <statement>
                <p>Compute the inverse of the coefficient matrix of the system of equations below and use the inverse to solve the system.<md>
                    <mrow>4x_1 + 10x_2 &amp;= 12</mrow>
                    <mrow>2x_1 + 6x_2 &amp;= 4</mrow>
                </md></p>
            </statement>
        </exercise>
        <exercise xml:id="reading-MINM-2">
            <statement>
                <p>In the reading questions for <xref ref="section-MISLE" acro="MISLE"/> you were asked to find the  inverse of the <m>3\times 3</m> matrix below.<me>\begin{bmatrix}
                2 &amp; 3 &amp; 1\\
                1 &amp; -2 &amp; -3\\
                -2 &amp; 4 &amp; 6
                \end{bmatrix}</me>Because the matrix was not nonsingular, you had no theorems at that point that would allow you to compute the inverse.  Explain why you now know that the inverse does not exist (which is different than not being able to compute it) by quoting the relevant theorem's acronym.</p>
            </statement>
        </exercise>
        <exercise xml:id="reading-MINM-3">
            <statement>
                <p>Is the matrix <m>A</m> unitary?  Why?<me>A=\begin{bmatrix}
                \frac{1}{\sqrt{22}}\left(4+2i\right) &amp; \frac{1}{\sqrt{374}}\left(5+3i\right) \\
                \frac{1}{\sqrt{22}}\left(-1-i\right) &amp; \frac{1}{\sqrt{374}}\left(12+14i\right) \\
                \end{bmatrix}</me></p>
            </statement>
        </exercise>
    </exercises>
    <exercises xml:id="exercises-MINM">
        <title>Exercises</title>
        <exercise number="C20" xml:id="exercise-MINM-C20">
            <statement contributor="chrisblack">
                <p>Verify that <m>AB</m> is nonsingular.<md>
                    <mrow>A&amp;=
                    \begin{bmatrix}
                    1 &amp; 2 &amp; 1 \\
                    0 &amp; 1 &amp; 1\\
                    1 &amp; 0 &amp; 2
                    \end{bmatrix}
                    &amp;
                    B &amp;=
                    \begin{bmatrix}
                    -1 &amp; 1 &amp; 0\\
                    1 &amp; 2 &amp; 1\\
                    0 &amp; 1 &amp; 1
                    \end{bmatrix}</mrow>
                </md></p>
            </statement>
        </exercise>
        <exercise number="C40" xml:id="exercise-MINM-C40">
            <statement>
                <p>Solve the system of equations below using the inverse of a matrix.<md>
                <mrow>x_1+x_2+3x_3+x_4&amp;=5</mrow>
                <mrow>-2x_1-x_2-4x_3-x_4&amp;=-7</mrow>
                <mrow>x_1+4x_2+10x_3+2x_4&amp;=9</mrow>
                <mrow>-2x_1-4x_3+5x_4&amp;=9</mrow>
                </md></p>
            </statement>
            <solution xml:id="solution-MINM-C40">
                <p>The coefficient matrix and vector of constants for the system are<md>
                <mrow>\begin{bmatrix}
                1 &amp; 1 &amp; 3 &amp; 1\\
                -2 &amp; -1 &amp; -4 &amp; -1\\
                1 &amp; 4 &amp; 10 &amp; 2\\
                -2 &amp; 0 &amp; -4 &amp; 5
                \end{bmatrix}&amp;&amp;
                \vect{b}=\colvector{5\\-7\\9\\9}</mrow>
                </md>.  <m>\inverse{A}</m> can be computed by using a calculator, or by the method of <xref ref="theorem-CINM" acro="CINM"/>.  Then <xref ref="theorem-SNCM" acro="SNCM"/> says the unique solution is<me>\inverse{A}\vect{b}=
                \begin{bmatrix}
                38 &amp; 18 &amp; -5 &amp; -2\\
                96 &amp; 47 &amp; -12 &amp; -5\\
                -39 &amp; -19 &amp; 5 &amp; 2\\
                -16 &amp; -8 &amp; 2 &amp; 1
                \end{bmatrix}
                \colvector{5\\-7\\9\\9}
                =
                \colvector{1\\-2\\1\\3}</me>.</p>
            </solution>
        </exercise>
        <exercise number="M10" xml:id="exercise-MINM-M10">
            <statement contributor="chrisblack">
                <p>Find values of <m>x</m>, <m>y</m>, <m>z</m> so that matrix <m>A</m> is invertible.<me>A =
                \begin{bmatrix}
                1 &amp; 2 &amp; x\\
                3 &amp; 0 &amp; y \\
                1 &amp; 1 &amp; z
                \end{bmatrix}</me></p>
            </statement>
            <solution xml:id="solution-MINM-M10" contributor="chrisblack">
                <p>There are an infinite number of possible answers.  We want to find a vector
                <m>\colvector{x \\ y \\ z}</m> so that the set<md>
                    <mrow>S = \set{
                    \begin{bmatrix} 1 \\ 3 \\ 1 \end{bmatrix},
                    \begin{bmatrix} 2 \\ 0 \\ 1 \end{bmatrix},
                    \begin{bmatrix} x \\ y \\ z \end{bmatrix}
                    }</mrow>
                </md>is a linearly independent set.  We need a vector not in the span of the first two columns, which geometrically means that we need it to not be in the same plane as the first two columns of <m>A</m>.  We can choose any values we want for <m>x</m> and <m>y</m>, and then choose a value of <m>z</m> that makes the three vectors independent.</p>
                <p>I will (arbitrarily) choose <m>x = 1</m>, <m>y = 1</m>.  Then, we have<md>
                <mrow>A = \begin{bmatrix}
                1 &amp; 2 &amp; 1\\
                3 &amp; 0 &amp; 1 \\
                1 &amp; 1 &amp; z
                \end{bmatrix}
                &amp;\rref
                \begin{bmatrix}
                \leading{1} &amp; 0 &amp; 2z-1
                \\ 0 &amp; \leading{1} &amp; 1-z
                \\ 0 &amp; 0 &amp; 4 - 6z
                \end{bmatrix}</mrow>
                </md>which is invertible if and only if <m>4-6z \ne 0</m>.  Thus, we can choose any value as long as <m>z \ne \frac{2}{3}</m>, so we choose <m>z = 0</m>, and we have found a matrix <m>A = \begin{bmatrix} 1 &amp; 2 &amp; 1\\ 3 &amp; 0 &amp; 1 \\ 1 &amp; 1 &amp; 0 \end{bmatrix}</m> that is invertible.</p>
            </solution>
        </exercise>
        <exercise number="M11" xml:id="exercise-MINM-M11">
            <statement contributor="chrisblack">
                <p>Find values of <m>x</m>, <m>y</m> <m>z</m> so that matrix <m>A</m>  is singular.<me>A =
                \begin{bmatrix}
                1 &amp; x &amp; 1\\
                1 &amp; y &amp; 4 \\
                0 &amp; z &amp; 5
                \end{bmatrix}</me></p>
            </statement>
            <solution xml:id="solution-MINM-M11" contributor="chrisblack">
                <p>There are an infinite number of possible answers. We need the set of vectors<md>
                <mrow>S &amp;=
                \set{
                \begin{bmatrix}1 \\ 1 \\ 0 \end{bmatrix},
                \begin{bmatrix}x \\ y \\ z \end{bmatrix},
                \begin{bmatrix} 1 \\ 4 \\ 5 \end{bmatrix}
                }</mrow>
                </md> to be linearly dependent.  One  way to do this  by inspection is to have<m>\colvector{x \\ y \\ z} =\colvector{1 \\ 4 \\ 5}</m>.  Thus, if we let <m>x = 1</m>, <m>y = 4</m>, <m> z = 5</m>, then the matrix <m>A = \begin{bmatrix} 1 &amp; 1 &amp; 1\\ 1 &amp; 4 &amp; 4 \\ 0 &amp; 5 &amp; 5 \end{bmatrix}</m> is singular.</p>
            </solution>
        </exercise>
        <exercise number="M15" xml:id="exercise-MINM-M15">
            <statement contributor="chrisblack">
                <p>If <m>A</m> and <m>B</m> are <m>n \times n</m> matrices, <m>A</m> is nonsingular, and <m>B</m> is singular, show directly that <m>AB</m> is singular, without using <xref ref="theorem-NPNT" acro="NPNT"/>.</p>
            </statement>
            <solution xml:id="solution-MINM-M15" contributor="chrisblack">
                <p>If <m>B</m> is singular, then there exists a vector <m>\vect{x}\ne\zerovector</m> so that <m>\vect{x}\in \nsp{B}</m>.  Thus, <m>B\vect{x} = \vect{0}</m>, so <m>A(B\vect{x}) = (AB)\vect{x} = \zerovector</m>, so <m>\vect{x}\in\nsp{AB}</m>.  Since the null space of <m>AB</m> is not trivial, <m>AB</m> is a singular matrix.</p>
            </solution>
        </exercise>
        <exercise number="M20" xml:id="exercise-MINM-M20">
            <statement>
                <p>Construct an example of a <m>4\times 4</m> unitary matrix.</p>
            </statement>
            <solution xml:id="solution-MINM-M20">
                <p>The <m>4\times 4</m> identity matrix, <m>I_4</m>, would be one example (<xref ref="definition-IM" acro="IM"/>).  Any of the 23 other rearrangements of the columns of <m>I_4</m> would be a simple, but less trivial, example.  See <xref ref="example-UPM" acro="UPM"/>.</p>
            </solution>
        </exercise>
        <exercise number="M80" xml:id="exercise-MINM-M80">
            <statement contributor="markhamrick">
                <p>Matrix multiplication interacts nicely with many operations.  But not always with transforming a matrix to reduced row-echelon form.  Suppose that <m>A</m> is an <m>m\times n</m> matrix and <m>B</m> is an <m>n\times p</m> matrix.  Let <m>P</m> be a matrix that is row-equivalent to <m>A</m> and in reduced row-echelon form, <m>Q</m> be a matrix that is row-equivalent to <m>B</m> and in reduced row-echelon form, and let <m>R</m> be a matrix that is row-equivalent to <m>AB</m> and in reduced row-echelon form.  Is <m>PQ=R</m>?  (In other words, with nonstandard notation, is <m>\text{rref}(A)\text{rref}(B)=\text{rref}(AB)</m>?)</p>
                <p>Construct a counterexample to show that, in general, this statement is false.  Then find a large class of matrices where if <m>A</m> and <m>B</m> are in the class, then the statement is true.</p>
            </statement>
            <solution xml:id="solution-MINM-M80">
                <p>Take<md>
                    <mrow>A&amp;=
                    \begin{bmatrix}
                    1&amp;0\\0&amp;0
                    \end{bmatrix}
                    &amp;
                    B&amp;=
                    \begin{bmatrix}
                    0&amp;0\\1&amp;0
                    \end{bmatrix}</mrow>
                </md>.Then <m>A</m> is already in reduced row-echelon form, and by swapping rows, <m>B</m> row-reduces to <m>A</m>.  So the product of the row-echelon forms of <m>A</m> is <m>AA=A\neq\zeromatrix</m>.  However, the product <m>AB</m> is the <m>2\times 2</m> zero matrix, which is in reduced-echelon form, and not equal to <m>AA</m>.  When you get there, <xref ref="theorem-PEEF" acro="PEEF"/> or <xref ref="theorem-EMDRO" acro="EMDRO"/> might shed some light on why we would not expect this statement to be true in general.</p>
                <p>If <m>A</m> and <m>B</m> are nonsingular, then <m>AB</m> is nonsingular (<xref ref="theorem-NPNT" acro="NPNT"/>), and all three matrices <m>A</m>, <m>B</m> and <m>AB</m> row-reduce to the identity matrix (<xref ref="theorem-NMRRI" acro="NMRRI"/>).  By <xref ref="theorem-MMIM" acro="MMIM"/>, the desired relationship is true.</p>
            </solution>
        </exercise>
        <exercise number="T10" xml:id="exercise-MINM-T10">
            <statement>
                <p>Suppose that <m>Q</m> and <m>P</m> are unitary matrices of size <m>n</m>.  Prove that <m>QP</m> is a unitary matrix.</p>
            </statement>
        </exercise>
        <exercise number="T11" xml:id="exercise-MINM-T11">
            <statement>
                <p>Prove that Hermitian matrices (<xref ref="definition-HM" acro="HM"/>) have real entries on the diagonal.  More precisely, suppose that <m>A</m> is a Hermitian matrix of size <m>n</m>.  Then <m>\matrixentry{A}{ii}\in\reals</m>, <m>1\leq i\leq n</m>.</p>
            </statement>
        </exercise>
        <exercise number="T12" xml:id="exercise-MINM-T12">
            <statement>
                <p>Suppose that we are checking if a square matrix of size <m>n</m> is unitary.  Show that a straightforward application of <xref ref="theorem-CUMOS" acro="CUMOS"/> requires the computation of <m>n^2</m> inner products when the matrix is unitary, and fewer when the matrix is not orthogonal.  Then show that this maximum number of inner products can be reduced to <m>\frac{1}{2}n(n+1)</m> in light of <xref ref="theorem-IPAC" acro="IPAC"/>.</p>
            </statement>
        </exercise>
        <exercise number="T25" xml:id="exercise-MINM-T25">
            <statement contributor="manleyperkel">
                <p>The notation <m>A^k</m> means a repeated matrix product between <m>k</m> copies of the square matrix <m>A</m>.<ol>
                    <li>Assume <m>A</m> is an <m>n\times n</m> matrix where <m>A^2=\zeromatrix</m> (which does not imply that <m>A=\zeromatrix</m>.)  Prove that <m>I_n-A</m> is invertible by showing that <m>I_n+A</m> is an inverse of <m>I_n-A</m>.</li><li>Assume that <m>A</m> is an <m>n\times n</m> matrix where <m>A^3=\zeromatrix</m>.  Prove that <m>I_n-A</m> is invertible.</li>
                    <li>Form a general theorem based on your observations from parts (1) and (2) and provide a proof.</li>
                </ol></p>
            </statement>
        </exercise>
    </exercises>
</section>
