<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the book                -->
<!--                                              -->
<!--      A First Course in Linear Algebra        -->
<!--                                              -->
<!-- Copyright (C) 2004-2017  Robert A. Beezer    -->
<!-- See the file COPYING for copying conditions. -->

<section xml:id="section-MR" acro="MR">
    <title>Matrix Representations</title>
    <introduction>
        <p>We have seen that linear transformations whose domain and codomain are vector spaces of columns vectors have a close relationship with matrices (<xref ref="theorem-MBLT" acro="MBLT"/>, <xref ref="theorem-MLTCV" acro="MLTCV"/>).  In this section, we will extend the relationship between matrices and linear transformations to the setting of linear transformations between abstract vector spaces.</p>
    </introduction>
    <subsection xml:id="subsection-MR-MR" acro="MR">
        <title>Matrix Representations</title>
        <p>This is a fundamental definition.</p>
        <definition xml:id="definition-MR" acro="MR">
            <title>Matrix Representation</title>
            <idx>matrix representation</idx>
            <statement>
                <p>Suppose that <m>\ltdefn{T}{U}{V}</m> is a linear transformation, <m>B=\set{\vectorlist{u}{n}}</m> is a basis for <m>U</m> of size <m>n</m>, and <m>C</m> is a basis for <m>V</m> of size <m>m</m>.  Then the <term>matrix representation</term> of <m>T</m> relative to <m>B</m> and <m>C</m> is the <m>m\times n</m> matrix,<me>\matrixrep{T}{B}{C}=\left[
                \left.\vectrep{C}{\lteval{T}{\vect{u}_1}}\right|
                \left.\vectrep{C}{\lteval{T}{\vect{u}_2}}\right|
                \left.\vectrep{C}{\lteval{T}{\vect{u}_3}}\right|
                \ldots
                \left|\vectrep{C}{\lteval{T}{\vect{u}_n}}\right.
                \right]</me>.</p>
            </statement>
            <notation xml:id="notation-MR" acro="MR">
                <idx>
                    <h>matrix representation</h>
                    <h>notation</h>
                </idx>
                <description>Matrix Representation</description>
                <usage>\matrixrep{T}{B}{C}</usage>
            </notation>
        </definition>
        <example xml:id="example-OLTTR" acro="OLTTR">
            <title>One linear transformation, three representations</title>
            <idx>matrix representations</idx>
            <p>Consider the linear transformation, <m>\ltdefn{S}{P_3}{M_{22}}</m>, given by<md>
                <mrow>\lteval{S}{a+bx+cx^2+dx^3}=
                \begin{bmatrix}
                3a+7b-2c-5d &amp; 8a+14b-2c-11d\\
                -4a-8b+2c+6d &amp; 12a+22b-4c-17d
                \end{bmatrix}</mrow>
            </md>.</p>
            <p>First, we build a representation relative to the bases,<md>
                <mrow>B&amp;=\set{1+2x+x^2-x^3,\,1+3x+x^2+x^3,\,-1-2x+2x^3,\,2+3x+2x^2-5x^3}</mrow>
                <mrow>C&amp;=\set{
                \begin{bmatrix}1 &amp; 1 \\ 1 &amp; 2\end{bmatrix},\,
                \begin{bmatrix}2 &amp; 3 \\ 2 &amp; 5\end{bmatrix},\,
                \begin{bmatrix}-1 &amp; -1 \\ 0 &amp; -2\end{bmatrix},\,
                \begin{bmatrix}-1 &amp; -4 \\ -2 &amp; -4\end{bmatrix}
                }</mrow>
            </md>.</p>
            <p>We evaluate <m>S</m> with each element of the basis for the domain, <m>B</m>, and coordinatize the result relative to the vectors in the basis for the codomain, <m>C</m>.  Notice here how we take elements of vector spaces and decompose them into linear combinations of basis elements as the key step in constructing coordinatizations of vectors.  There is a system of equations involved almost every time, but we will omit these details since this should be a routine exercise at this stage.  We have<md>
                <mrow>&amp;\vectrep{C}{\lteval{S}{1+2x+x^2-x^3}}
                =\vectrep{C}{\begin{bmatrix}20 &amp; 45 \\ -24 &amp; 69\end{bmatrix}}</mrow>
                <mrow>&amp;\quad=\vectrep{C}{
                (-90)\begin{bmatrix}1 &amp; 1 \\ 1 &amp; 2\end{bmatrix}+
                37\begin{bmatrix}2 &amp; 3 \\ 2 &amp; 5\end{bmatrix}+
                (-40)\begin{bmatrix}-1 &amp; -1 \\ 0 &amp; -2\end{bmatrix}+
                4\begin{bmatrix}-1 &amp; -4 \\ -2 &amp; -4\end{bmatrix}
                }
                =\colvector{-90\\37\\-40\\4}</mrow>
                <mrow>&amp;\vectrep{C}{\lteval{S}{1+3x+x^2+x^3}}
                =\vectrep{C}{\begin{bmatrix}17 &amp; 37 \\ -20 &amp; 57\end{bmatrix}}</mrow>
                <mrow>&amp;\quad=\vectrep{C}{
                (-72)\begin{bmatrix}1 &amp; 1 \\ 1 &amp; 2\end{bmatrix}+
                29\begin{bmatrix}2 &amp; 3 \\ 2 &amp; 5\end{bmatrix}+
                (-34)\begin{bmatrix}-1 &amp; -1 \\ 0 &amp; -2\end{bmatrix}+
                3\begin{bmatrix}-1 &amp; -4 \\ -2 &amp; -4\end{bmatrix}
                }
                =\colvector{-72\\29\\-34\\3}</mrow>
                <mrow>&amp;\vectrep{C}{\lteval{S}{-1-2x+2x^3}}
                =\vectrep{C}{\begin{bmatrix}-27 &amp; -58 \\ 32 &amp; -90\end{bmatrix}}</mrow>
                <mrow>&amp;\quad=\vectrep{C}{
                114\begin{bmatrix}1 &amp; 1 \\ 1 &amp; 2\end{bmatrix}+
                (-46)\begin{bmatrix}2 &amp; 3 \\ 2 &amp; 5\end{bmatrix}+
                54\begin{bmatrix}-1 &amp; -1 \\ 0 &amp; -2\end{bmatrix}+
                (-5)\begin{bmatrix}-1 &amp; -4 \\ -2 &amp; -4\end{bmatrix}
                }
                =\colvector{114\\-46\\54\\-5}</mrow>
                <mrow>&amp;\vectrep{C}{\lteval{S}{2+3x+2x^2-5x^3}}
                =\vectrep{C}{\begin{bmatrix}48 &amp; 109 \\ -58 &amp; 167\end{bmatrix}}</mrow>
                <mrow>&amp;\quad=\vectrep{C}{
                -220\begin{bmatrix}1 &amp; 1 \\ 1 &amp; 2\end{bmatrix}+
                91\begin{bmatrix}2 &amp; 3 \\ 2 &amp; 5\end{bmatrix}+
                (-96)\begin{bmatrix}-1 &amp; -1 \\ 0 &amp; -2\end{bmatrix}+
                10\begin{bmatrix}-1 &amp; -4 \\ -2 &amp; -4\end{bmatrix}
                }
                =\colvector{-220\\91\\-96\\10}</mrow>
            </md>.</p>
            <p>Thus, employing <xref ref="definition-MR" acro="MR"/><me>\matrixrep{S}{B}{C}=
                \begin{bmatrix}
                 -90 &amp; -72 &amp; 114 &amp; -220 \\
                 37 &amp; 29 &amp; -46 &amp; 91 \\
                 -40 &amp; -34 &amp; 54 &amp; -96 \\
                 4 &amp; 3 &amp; -5 &amp; 10
                \end{bmatrix}</me>.</p>
            <p>Often we use <q>nice</q> bases to build matrix representations and the work involved is much easier.  Suppose we take bases<md>
                <mrow>D&amp;=\set{1,\,x,\,x^2,\,x^3}
                &amp;
                E&amp;=\set{
                \begin{bmatrix}1&amp;0\\0&amp;0\end{bmatrix},\,
                \begin{bmatrix}0&amp;1\\0&amp;0\end{bmatrix},\,
                \begin{bmatrix}0&amp;0\\1&amp;0\end{bmatrix},\,
                \begin{bmatrix}0&amp;0\\0&amp;1\end{bmatrix}}</mrow>
            </md>.</p>
            <p>The evaluation of <m>S</m> at the elements of <m>D</m> is easy and coordinatization relative to <m>E</m> can be done on sight,<md>
                <mrow>\vectrep{E}{\lteval{S}{1}}
                &amp;=\vectrep{E}{\begin{bmatrix}3 &amp; 8 \\ -4 &amp; 12\end{bmatrix}}</mrow>
                <mrow>&amp;=\vectrep{E}{
                3\begin{bmatrix}1&amp;0\\0&amp;0\end{bmatrix}+
                8\begin{bmatrix}0&amp;1\\0&amp;0\end{bmatrix}+
                (-4)\begin{bmatrix}0&amp;0\\1&amp;0\end{bmatrix}+
                12\begin{bmatrix}0&amp;0\\0&amp;1\end{bmatrix}
                }
                =\colvector{3 \\ 8 \\ -4 \\ 12}</mrow>
                <mrow>\vectrep{E}{\lteval{S}{x}}
                &amp;=\vectrep{E}{\begin{bmatrix}7 &amp; 14 \\ -8 &amp; 22\end{bmatrix}}</mrow>
                <mrow>&amp;=\vectrep{E}{
                7\begin{bmatrix}1&amp;0\\0&amp;0\end{bmatrix}+
                14\begin{bmatrix}0&amp;1\\0&amp;0\end{bmatrix}+
                (-8)\begin{bmatrix}0&amp;0\\1&amp;0\end{bmatrix}+
                22\begin{bmatrix}0&amp;0\\0&amp;1\end{bmatrix}
                }
                =\colvector{7 \\ 14 \\ -8 \\ 22}</mrow>
                <mrow>\vectrep{E}{\lteval{S}{x^2}}
                &amp;=\vectrep{E}{\begin{bmatrix}-2 &amp; -2 \\ 2 &amp; -4\end{bmatrix}}</mrow>
                <mrow>&amp;=\vectrep{E}{
                (-2)\begin{bmatrix}1&amp;0\\0&amp;0\end{bmatrix}+
                (-2)\begin{bmatrix}0&amp;1\\0&amp;0\end{bmatrix}+
                2\begin{bmatrix}0&amp;0\\1&amp;0\end{bmatrix}+
                (-4)\begin{bmatrix}0&amp;0\\0&amp;1\end{bmatrix}
                }
                =\colvector{-2 \\ -2 \\ 2 \\ -4}</mrow>
                <mrow>\vectrep{E}{\lteval{S}{x^3}}
                &amp;=\vectrep{E}{\begin{bmatrix}-5 &amp; -11 \\ 6 &amp; -17\end{bmatrix}}</mrow>
                <mrow>&amp;=\vectrep{E}{
                (-5)\begin{bmatrix}1&amp;0\\0&amp;0\end{bmatrix}+
                (-11)\begin{bmatrix}0&amp;1\\0&amp;0\end{bmatrix}+
                6\begin{bmatrix}0&amp;0\\1&amp;0\end{bmatrix}+
                (-17)\begin{bmatrix}0&amp;0\\0&amp;1\end{bmatrix}
                }</mrow>
                <mrow>&amp;=\colvector{-5 \\ -11 \\ 6 \\ -17}</mrow>
            </md>.</p>
            <p>So the matrix representation of <m>S</m> relative to <m>D</m> and <m>E</m> is<me>\matrixrep{S}{D}{E}=
                \begin{bmatrix}
                 3 &amp; 7 &amp; -2 &amp; -5 \\
                 8 &amp; 14 &amp; -2 &amp; -11 \\
                 -4 &amp; -8 &amp; 2 &amp; 6 \\
                 12 &amp; 22 &amp; -4 &amp; -17
                \end{bmatrix}</me>.</p>
            <p>One more time, but now let us use bases<md>
                <mrow>F&amp;=\set{1+x-x^2+2x^3,\,-1+2x+2x^3,\,2+x-2x^2+3x^3,\,1+x+2x^3}</mrow>
                <mrow>G&amp;=\set{
                \begin{bmatrix}1&amp;1\\-1&amp;2\end{bmatrix},\,
                \begin{bmatrix}-1&amp;2\\0&amp;2\end{bmatrix},\,
                \begin{bmatrix}2&amp;1\\-2&amp;3\end{bmatrix},\,
                \begin{bmatrix}1&amp;1\\0&amp;2\end{bmatrix}
                }</mrow>
            </md>and evaluate <m>S</m> with the elements of <m>F</m>, then coordinatize the results relative to <m>G</m>,<md>
                <mrow>\vectrep{G}{\lteval{S}{1+x-x^2+2x^3}}
                &amp;=\vectrep{G}{\begin{bmatrix}2 &amp; 2 \\ -2 &amp; 4\end{bmatrix}}
                =\vectrep{G}{
                2\begin{bmatrix}1&amp;1\\-1&amp;2\end{bmatrix}
                }
                =\colvector{2\\0\\0\\0}</mrow>
                <mrow>\vectrep{G}{\lteval{S}{-1+2x+2x^3}}
                &amp;=\vectrep{G}{\begin{bmatrix}1 &amp; -2 \\ 0 &amp; -2\end{bmatrix}}
                =\vectrep{G}{
                (-1)\begin{bmatrix}-1&amp;2\\0&amp;2\end{bmatrix}
                }
                =\colvector{0\\-1\\0\\0}</mrow>
                <mrow>\vectrep{G}{\lteval{S}{2+x-2x^2+3x^3}}
                &amp;=\vectrep{G}{\begin{bmatrix}2 &amp; 1 \\ -2 &amp; 3\end{bmatrix}}
                =\vectrep{G}{
                \begin{bmatrix}2&amp;1\\-2&amp;3\end{bmatrix}
                }
                =\colvector{0\\0\\1\\0}</mrow>
                <mrow>\vectrep{G}{\lteval{S}{1+x+2x^3}}
                &amp;=\vectrep{G}{\begin{bmatrix}0 &amp; 0 \\ 0 &amp; 0\end{bmatrix}}
                =\vectrep{G}{
                0\begin{bmatrix}1&amp;1\\0&amp;2\end{bmatrix}
                }
                =\colvector{0\\0\\0\\0}</mrow>
            </md>.</p>
            <p>So we arrive at an especially economical matrix representation,<me>\matrixrep{S}{F}{G}=
            \begin{bmatrix}
            2 &amp; 0 &amp; 0 &amp; 0 \\
            0 &amp; -1 &amp; 0 &amp; 0 \\
            0 &amp; 0 &amp; 1 &amp; 0 \\
            0 &amp; 0 &amp; 0 &amp; 0
            \end{bmatrix}</me>.</p>
        </example>
        <p>We may choose to use whatever terms we want when we make a definition.  Some are arbitrary, while others make sense, but only in light of subsequent theorems.  Matrix representation is in the latter category.  We begin with a linear transformation and produce a matrix.  So what?  Here is the theorem that <em>justifies</em> the term <term>matrix representation</term>.</p>
        <theorem xml:id="theorem-FTMR" acro="FTMR">
            <title>Fundamental Theorem of Matrix Representation</title>
            <idx>matrix representation</idx>
            <statement>
                <p>Suppose that <m>\ltdefn{T}{U}{V}</m> is a linear transformation, <m>B</m> is a basis for <m>U</m>, <m>C</m> is a basis for <m>V</m> and <m>\matrixrep{T}{B}{C}</m> is the matrix representation of <m>T</m> relative to <m>B</m> and <m>C</m>.  Then, for any <m>\vect{u}\in U</m>,<me>\vectrep{C}{\lteval{T}{\vect{u}}}=\matrixrep{T}{B}{C}\left(\vectrep{B}{\vect{u}}\right)</me>or equivalently<me>\lteval{T}{\vect{u}}=\vectrepinv{C}{\matrixrep{T}{B}{C}\left(\vectrep{B}{\vect{u}}\right)}</me>.</p>
            </statement>
            <proof>
                <p>Let <m>B=\set{\vectorlist{u}{n}}</m> be the basis of <m>U</m>.  Since <m>\vect{u}\in U</m>, there are scalars <m>\scalarlist{a}{n}</m> such that<me>\vect{u}=\lincombo{a}{u}{n}</me>.</p>
                <p>Then,<md>
                    <mrow>&amp;\matrixrep{T}{B}{C}\vectrep{B}{\vect{u}}</mrow>
                    <mrow>&amp;=
                    \left[
                    \left.\vectrep{C}{\lteval{T}{\vect{u}_1}}\right|
                    \left.\vectrep{C}{\lteval{T}{\vect{u}_2}}\right|
                    \left.\vectrep{C}{\lteval{T}{\vect{u}_3}}\right|
                    \ldots
                    \left|\vectrep{C}{\lteval{T}{\vect{u}_n}}\right.
                    \right]
                    \vectrep{B}{\vect{u}}&amp;&amp;
                        <xref ref="definition-MR" acro="MR"/></mrow>
                    <mrow>&amp;=
                    \left[
                    \left.\vectrep{C}{\lteval{T}{\vect{u}_1}}\right|
                    \left.\vectrep{C}{\lteval{T}{\vect{u}_2}}\right|
                    \left.\vectrep{C}{\lteval{T}{\vect{u}_3}}\right|
                    \ldots
                    \left|\vectrep{C}{\lteval{T}{\vect{u}_n}}\right.
                    \right]
                    \colvector{a_1\\a_2\\a_3\\\vdots\\a_n}&amp;&amp;
                        <xref ref="definition-VR" acro="VR"/></mrow>
                    <mrow>&amp;=
                    a_1\vectrep{C}{\lteval{T}{\vect{u}_1}}+
                    a_2\vectrep{C}{\lteval{T}{\vect{u}_2}}+
                    \cdots+
                    a_n\vectrep{C}{\lteval{T}{\vect{u}_n}}&amp;&amp;
                        <xref ref="definition-MVP" acro="MVP"/></mrow>
                    <mrow>&amp;=
                    \vectrep{C}{
                    a_1\lteval{T}{\vect{u}_1}+
                    a_2\lteval{T}{\vect{u}_2}+
                    a_3\lteval{T}{\vect{u}_3}+\cdots+
                    a_n\lteval{T}{\vect{u}_n}
                    }&amp;&amp;
                        <xref ref="theorem-LTLC" acro="LTLC"/></mrow>
                    <mrow>&amp;=
                    \vectrep{C}{
                    \lteval{T}{
                    \lincombo{a}{u}{n}
                    }}&amp;&amp;
                        <xref ref="theorem-LTLC" acro="LTLC"/></mrow>
                    <mrow>&amp;=
                    \vectrep{C}{\lteval{T}{\vect{u}}}</mrow>
                </md>.</p>
                <p>The alternative conclusion is obtained as<md>
                    <mrow>\lteval{T}{\vect{u}}
                    &amp;=\lteval{I_V}{\lteval{T}{\vect{u}}}&amp;&amp;
                        <xref ref="definition-IDLT" acro="IDLT"/></mrow>
                    <mrow>&amp;=\lteval{\left(\compose{\vectrepinvname{C}}{\vectrepname{C}}\right)}{\lteval{T}{\vect{u}}}&amp;&amp;
                        <xref ref="definition-IVLT" acro="IVLT"/></mrow>
                    <mrow>&amp;=\vectrepinv{C}{\vectrep{C}{\lteval{T}{\vect{u}}}}&amp;&amp;
                        <xref ref="definition-LTC" acro="LTC"/></mrow>
                    <mrow>&amp;=\vectrepinv{C}{\matrixrep{T}{B}{C}\left(\vectrep{B}{\vect{u}}\right)}</mrow>
                </md>.</p>
            </proof>
        </theorem>
        <p>This theorem says that we can apply <m>T</m> to <m>\vect{u}</m> and coordinatize the result relative to <m>C</m> in <m>V</m>, or we can first coordinatize <m>\vect{u}</m> relative to <m>B</m> in <m>U</m>, then multiply by the matrix representation.  Either way, the result is the same.  So the effect of a linear transformation can always be accomplished by a matrix-vector product (<xref ref="definition-MVP" acro="MVP"/>).  That is important enough to say again.  The effect of a linear transformation is a matrix-vector product.</p>
        <figure xml:id="figure-FTMR" acro="FTMR">
            <caption>Fundamental Theorem of Matrix Representations</caption>
            <image xml:id="image-FTMR">
                <latex-image>
                \begin{tikzpicture}
                \matrix (m) [matrix of math nodes, row sep=5em, column sep=10em, text height=1.5ex, text depth=0.25ex]
                { \vect{u} \amp \lteval{T}{\vect{u}} \\
                \vectrep{B}{\vect{u}} \amp \matrixrep{T}{B}{C}\,\vectrep{B}{\vect{u}}=\vectrep{C}{\lteval{T}{\vect{u}}}\\};
                \path[->]
                (m-1-1) edge[thick] node[auto] {\(T\)}                   (m-1-2)
                (m-1-2) edge[thick] node[auto] {\(\vectrepname{C}\)}     (m-2-2)
                (m-1-1) edge[thick] node[auto] {\(\vectrepname{B}\)}     (m-2-1)
                (m-2-1) edge[thick] node[auto] {\(\matrixrep{T}{B}{C}\)} (m-2-2);
                \end{tikzpicture}
                </latex-image>
            </image>
        </figure>
        <p>The alternative conclusion of this result might be even more striking.  It says that to effect a linear transformation (<m>T</m>) of a vector (<m>\vect{u}</m>), coordinatize the input (with <m>\vectrepname{B}</m>), do a matrix-vector product (with <m>\matrixrep{T}{B}{C}</m>), and un-coordinatize the result (with <m>\vectrepinvname{C}</m>).  So, absent some bookkeeping about vector representations, a linear transformation <em>is</em> a matrix.  To adjust the diagram, we <q>reverse</q> the arrow on the right, which means inverting the vector representation <m>\vectrepname{C}</m> on <m>V</m>.  Now we can go directly across the top of the diagram, computing the linear transformation between the abstract vector spaces.  Or, we can around the other three sides, using vector representation, a matrix-vector product, followed by un-coordinatization.</p>
        <figure xml:id="figure-FTMRA" acro="FTMRA">
            <caption>Fundamental Theorem of Matrix Representations (Alternate)</caption>
            <image xml:id="image-FTMRA">
                <latex-image>
                \begin{tikzpicture}
                \matrix (m) [matrix of math nodes, row sep=5em, column sep=10em, text height=1.5ex, text depth=0.25ex]
                {\vect{u} \amp \lteval{T}{\vect{u}}=\vectrepinv{C}{\matrixrep{T}{B}{C}\,\vectrep{B}{\vect{u}}}\\
                \vectrep{B}{\vect{u}} \amp \matrixrep{T}{B}{C}\,\vectrep{B}{\vect{u}}\\};
                \path[->]
                (m-1-1) edge[thick] node[auto] {\(T\)}                   (m-1-2)
                (m-2-2) edge[thick] node[auto] {\(\vectrepinvname{C}\)}  (m-1-2) % reversed direction
                (m-1-1) edge[thick] node[auto] {\(\vectrepname{B}\)}     (m-2-1)
                (m-2-1) edge[thick] node[auto] {\(\matrixrep{T}{B}{C}\)} (m-2-2);
                \end{tikzpicture}
                </latex-image>
            </image>
        </figure>
        <p>Here is an example to illustrate how the <q>action</q> of a linear transformation can be effected by matrix multiplication.</p>
        <example xml:id="example-ALTMM" acro="ALTMM">
            <title>A linear transformation as matrix multiplication</title>
            <idx>
                <h>linear transformation</h>
                <h>as matrix multiplication</h>
            </idx>
            <p>In <xref ref="example-OLTTR" acro="OLTTR"/> we found three representations of the linear transformation <m>S</m>.  In this example, we will compute a single output of <m>S</m> in four different ways.  First <q>normally,</q> then three times over using <xref ref="theorem-FTMR" acro="FTMR"/>.</p>
            <p>Choose <m>p(x)=3-x+2x^2-5x^3</m>, for no particular reason.  Then the straightforward application of <m>S</m> to <m>p(x)</m> yields<md>
                <mrow>\lteval{S}{p(x)}
                &amp;=\lteval{S}{3-x+2x^2-5x^3}</mrow>
                <mrow>&amp;=
                \begin{bmatrix}
                3(3)+7(-1)-2(2)-5(-5) &amp; 8(3)+14(-1)-2(2)-11(-5)\\
                -4(3)-8(-1)+2(2)+6(-5) &amp; 12(3)+22(-1)-4(2)-17(-5)
                \end{bmatrix}</mrow>
                <mrow>&amp;=
                \begin{bmatrix}
                23 &amp; 61 \\ -30 &amp; 91
                \end{bmatrix}</mrow>
            </md>.</p>
            <p>Now use the representation of <m>S</m> relative to the bases <m>B</m> and <m>C</m> and <xref ref="theorem-FTMR" acro="FTMR"/>.  Note that we will employ the following linear combination in moving from the second line to the third,<md>
                <mrow>3-x+2x^2-5x^3
                &amp;= 48(1+2x+x^2-x^3)+(-20)(1+3x+x^2+x^3)+</mrow>
                <mrow>&amp;\quad\quad (-1)(-1-2x+2x^3)+(-13)(2+3x+2x^2-5x^3)</mrow>
            </md>.</p>
            <p>We have<md>
                <mrow>\lteval{S}{p(x)}
                &amp;=\vectrepinv{C}{\matrixrep{S}{B}{C}\vectrep{B}{p(x)}}\\
                &amp;=\vectrepinv{C}{\matrixrep{S}{B}{C}\vectrep{B}{3-x+2x^2-5x^3}}\\
                &amp;=\vectrepinv{C}{\matrixrep{S}{B}{C}\colvector{48\\-20\\-1\\-13}}\\
                &amp;=\vectrepinv{C}{
                \begin{bmatrix}
                 -90 &amp; -72 &amp; 114 &amp; -220 \\
                 37 &amp; 29 &amp; -46 &amp; 91 \\
                 -40 &amp; -34 &amp; 54 &amp; -96 \\
                 4 &amp; 3 &amp; -5 &amp; 10
                \end{bmatrix}
                \colvector{48\\-20\\-1\\-13}}</mrow>
                <mrow>&amp;=\vectrepinv{C}{\colvector{-134\\59\\-46\\7}}\\
                &amp;=
                (-134)\begin{bmatrix}1 &amp; 1 \\ 1 &amp; 2\end{bmatrix}+
                59\begin{bmatrix}2 &amp; 3 \\ 2 &amp; 5\end{bmatrix}+
                (-46)\begin{bmatrix}-1 &amp; -1 \\ 0 &amp; -2\end{bmatrix}+
                7\begin{bmatrix}-1 &amp; -4 \\ -2 &amp; -4\end{bmatrix}</mrow>
                <mrow>&amp;=
                \begin{bmatrix}
                23 &amp; 61 \\ -30 &amp; 91
                \end{bmatrix}</mrow>
            </md>.</p>
            <p>Again, but now with <q>nice</q> bases like <m>D</m> and <m>E</m>, and the computations are more transparent.  We have<md>
                <mrow>\lteval{S}{p(x)}
                &amp;=\vectrepinv{E}{\matrixrep{S}{D}{E}\vectrep{D}{p(x)}}</mrow>
                <mrow>&amp;=\vectrepinv{E}{\matrixrep{S}{D}{E}\vectrep{D}{3-x+2x^2-5x^3}}</mrow>
                <mrow>&amp;=\vectrepinv{E}{\matrixrep{S}{D}{E}\vectrep{D}{3(1)+(-1)(x)+2(x^2)+(-5)(x^3)}}</mrow>
                <mrow>&amp;=\vectrepinv{E}{\matrixrep{S}{D}{E}\colvector{3\\-1\\2\\-5}}</mrow>
                <mrow>&amp;=\vectrepinv{E}{
                \begin{bmatrix}
                 3 &amp; 7 &amp; -2 &amp; -5 \\
                 8 &amp; 14 &amp; -2 &amp; -11 \\
                 -4 &amp; -8 &amp; 2 &amp; 6 \\
                 12 &amp; 22 &amp; -4 &amp; -17
                \end{bmatrix}
                \colvector{3\\-1\\2\\-5}
                }</mrow>
                <mrow>&amp;=\vectrepinv{E}{\colvector{23 \\ 61 \\ -30 \\ 91}}</mrow>
                <mrow>&amp;=
                23\begin{bmatrix}1&amp;0\\0&amp;0\end{bmatrix}+
                61\begin{bmatrix}0&amp;1\\0&amp;0\end{bmatrix}+
                (-30)\begin{bmatrix}0&amp;0\\1&amp;0\end{bmatrix}+
                91\begin{bmatrix}0&amp;0\\0&amp;1\end{bmatrix}</mrow>
                <mrow>&amp;=
                \begin{bmatrix}
                23 &amp; 61 \\ -30 &amp; 91
                \end{bmatrix}</mrow>
            </md>.</p>
            <p>OK, last time, now with the bases <m>F</m> and <m>G</m>.  The coordinatizations will take some work this time, but the matrix-vector product (<xref ref="definition-MVP" acro="MVP"/>) (which is the actual action of the linear transformation) will be especially easy, given the diagonal nature of the matrix representation, <m>\matrixrep{S}{F}{G}</m>.  Here we go,<md>
                <mrow>\lteval{S}{p(x)}
                &amp;=\vectrepinv{G}{\matrixrep{S}{F}{G}\vectrep{F}{p(x)}}</mrow>
                <mrow>&amp;=\vectrepinv{G}{\matrixrep{S}{F}{G}\vectrep{F}{3-x+2x^2-5x^3}}</mrow>
                <mrow>&amp;=\vectrepinvname{G}\left(\matrixrep{S}{F}{G}\vectrepname{F}\left(
                32(1+x-x^2+2x^3)
                -7(-1+2x+2x^3)\right.\right.</mrow>
                <mrow>&amp;\left.\left.\quad\quad\quad\quad\quad\quad\quad -17(2+x-2x^2+3x^3)
                -2(1+x+2x^3)
                \right)\right)</mrow>
                <mrow>&amp;=\vectrepinv{G}{\matrixrep{S}{F}{G}\colvector{32\\-7\\-17\\-2}}</mrow>
                <mrow>&amp;=\vectrepinv{G}{
                \begin{bmatrix}
                2 &amp; 0 &amp; 0 &amp; 0 \\
                0 &amp; -1 &amp; 0 &amp; 0 \\
                0 &amp; 0 &amp; 1 &amp; 0 \\
                0 &amp; 0 &amp; 0 &amp; 0
                \end{bmatrix}
                \colvector{32\\-7\\-17\\-2}
                }</mrow>
                <mrow>&amp;=\vectrepinv{G}{\colvector{64 \\ 7 \\ -17 \\ 0}}</mrow>
                <mrow>&amp;=
                64\begin{bmatrix}1&amp;1\\-1&amp;2\end{bmatrix}+
                7\begin{bmatrix}-1&amp;2\\0&amp;2\end{bmatrix}+
                (-17)\begin{bmatrix}2&amp;1\\-2&amp;3\end{bmatrix}+
                0\begin{bmatrix}1&amp;1\\0&amp;2\end{bmatrix}</mrow>
                <mrow>&amp;=
                \begin{bmatrix}
                23 &amp; 61 \\ -30 &amp; 91
                \end{bmatrix}</mrow>
            </md>.</p>
            <p>This example is not meant to necessarily illustrate that any one of these four computations is simpler than the others.  Instead, it is meant to illustrate the many different ways we can arrive at the same result, with the last three all employing a matrix representation to effect the linear transformation.</p>
        </example>
        <p>We will use <xref ref="theorem-FTMR" acro="FTMR"/> frequently in the next few sections.  A typical application will feel like the linear transformation <m>T</m> <q>commutes</q> with a vector representation, <m>\vectrepname{C}</m>, and as it does the transformation morphs into a matrix, <m>\matrixrep{T}{B}{C}</m>, while the vector representation changes to a new basis, <m>\vectrepname{B}</m>.  Or vice-versa.</p>
        <computation xml:id="sage-MR" acro="MR">
            <title>Matrix Representations</title>
            <idx>matrix representations</idx>
            <p>It is very easy to create matrix representations of linear transformations in Sage.  Our examples in the text have used abstract vector spaces to make it a little easier to keep track of the domain and codomain.  With Sage we will have to consistently use variants of <c>QQ^n</c>, but we will use non-obvious bases to make it nontrivial and interesting.  Here are the components of our first example, one linear function, two vector spaces, four bases.</p>
            <sage xml:id="sagecell-MR-1">
                <input>
                x1, x2, x3, x4 = var('x1, x2, x3, x4')
                outputs = [3*x1 + 7*x2 + x3 - 4*x4,
                           2*x1 + 5*x2 + x3 - 3*x4,
                            -x1 - 2*x2      +   x4]
                T_symbolic(x1, x2, x3, x4) = outputs
                U = QQ^4
                V = QQ^3
                b0 = vector(QQ, [ 1, 1, -1, 0])
                b1 = vector(QQ, [-1, 0, -2, 7])
                b2 = vector(QQ, [ 0, 1, -2, 4])
                b3 = vector(QQ, [-2, 0, -1, 6])
                B = [b0, b1, b2, b3]
                c0 = vector(QQ, [ 1,  6, -6])
                c1 = vector(QQ, [ 0,  1, -1])
                c2 = vector(QQ, [-2, -3,  4])
                C = [c0, c1, c2]
                d0 = vector(QQ, [ 1, -3,  2, -1])
                d1 = vector(QQ, [ 0,  1,  0,  1])
                d2 = vector(QQ, [-1,  2, -1, -1])
                d3 = vector(QQ, [ 2, -8,  4, -3])
                D = [d0, d1, d2, d3]
                e0 = vector(QQ, [ 0,  1, -3])
                e1 = vector(QQ, [-1,  2, -1])
                e2 = vector(QQ, [ 2, -4,  3])
                E = [e0, e1, e2]
                </input>
            </sage>
            <p>We create alternate versions of the domain and codomain by providing them with bases other than the standard ones.  Then we build the linear transformation and ask for its <c>.matrix()</c>.  We will use numerals to distinguish our two examples.</p>
            <sage xml:id="sagecell-MR-2">
                <input>
                U1 = U.subspace_with_basis(B)
                V1 = V.subspace_with_basis(C)
                T1 = linear_transformation(U1, V1, T_symbolic)
                T1.matrix(side='right')
                </input>
                <output>
                [ 15 -67 -25 -61]
                [-75 326 120 298]
                [  3 -17  -7 -15]
                </output>
            </sage>
            <p>Now, we do it again, but with the bases <c>D</c> and <c>E</c>.</p>
            <sage xml:id="sagecell-MR-3">
                <input>
                U2 = U.subspace_with_basis(D)
                V2 = V.subspace_with_basis(E)
                T2 = linear_transformation(U2, V2, T_symbolic)
                T2.matrix(side='right')
                </input>
                <output>
                [ -32    8   38  -91]
                [-148   37  178 -422]
                [ -80   20   96 -228]
                </output>
            </sage>
            <p>So once the pieces are in place, it is quite easy to obtain a matrix representation.  You might experiment with examples from the text, by first mentally converting elements of abstract vector spaces into column vectors via vector representations using simple bases for the abstract spaces.</p>
            <p>The linear transformations <c>T1</c> and <c>T2</c> are not different as functions, despite the fact that Sage treats them as unequal (since they have different matrix representations).  We can check that the two functions behave identically, first with random testing.  Repeated execution of the following compute cell should always produce <c>True</c>.</p>
            <sage xml:id="sagecell-MR-4">
                <input>
                u = random_vector(QQ, 4)
                T1(u) == T2(u)
                </input>
                <output>
                True
                </output>
            </sage>
            <p>A better approach would be to see if  <c>T1</c> and <c>T2</c> agree on a basis for the domain, and to avoid any favoritism, we will use the basis of <c>U</c> itself.  Convince yourself that this is a proper application of <xref ref="theorem-LTDB" acro="LTDB"/> to demonstrate equality.</p>
            <sage xml:id="sagecell-MR-5">
                <input>
                all([T1(u) == T2(u) for u in U.basis()])
                </input>
                <output>
                True
                </output>
            </sage>
            <p>Or we can just ask if they are equal functions (a method that is implemented using the previous idea about agreeing on a basis).</p>
            <sage xml:id="sagecell-MR-6">
                <input>
                T1.is_equal_function(T2)
                </input>
                <output>
                True
                </output>
            </sage>
            <p>We can also demonstrate <xref ref="theorem-FTMR" acro="FTMR"/> <mdash/> we will use the representation for <c>T1</c>.  We need a couple of vector representation linear transformations.</p>
            <sage xml:id="sagecell-MR-7">
                <input>
                rhoB = linear_transformation(U1, U, U.basis())
                rhoC = linear_transformation(V1, V, V.basis())
                </input>
            </sage>
            <p>Now we experiment with a <q>random</q> element of the domain.  Study the third computation carefully, as it is the Sage version of <xref ref="theorem-FTMR" acro="FTMR"/>.  You might compute some of the pieces of this expression to build up the final expression, and you might duplicate the experiment with a different input and/or with the <c>T2</c> version.</p>
            <sage xml:id="sagecell-MR-8">
                <input>
                T_symbolic(-3, 4, -5, 2)
                </input>
                <output>
                (6, 3, -3)
                </output>
            </sage>
            <sage xml:id="sagecell-MR-9">
                <input>
                u = vector(QQ, [-3, 4, -5, 2])
                T1(u)
                </input>
                <output>
                (6, 3, -3)
                </output>
            </sage>
            <sage xml:id="sagecell-MR-10">
                <input>
                (rhoC^-1)(T1.matrix(side='right')*rhoB(u))
                </input>
                <output>
                (6, 3, -3)
                </output>
            </sage>
            <p>One more time, but we will replace the vector representation linear transformations with Sage conveniences.  Recognize that the <c>.linear_combination_of_basis()</c> method is the inverse of vector representation (it is <q>un-coordinatization</q>).</p>
            <sage xml:id="sagecell-MR-11">
                <input>
                output_coord = T1.matrix(side='right')*U1.coordinate_vector(u)
                V1.linear_combination_of_basis(output_coord)
                </input>
                <output>
                (6, 3, -3)
                </output>
            </sage>
            <p>We have concentrated here on the first version of the conclusion of <xref ref="theorem-FTMR" acro="FTMR"/>.  You could experiment with the second version in a similar manner.  Extra credit: what changes do you need to make in any of these experiments if you remove the <c>side='right'</c> option?</p>
        </computation>
    </subsection>
    <subsection xml:id="subsection-MR-NRFO" acro="NRFO">
        <title>New Representations from Old</title>
        <p>In <xref ref="subsection-LT-NLTFO" acro="LT.NLTFO"/> we built new linear transformations from other linear transformations.  Sums, scalar multiples and compositions.  These new linear transformations will have matrix representations as well.  How do the new matrix representations relate to the old matrix representations?  Here are the three theorems.</p>
        <theorem xml:id="theorem-MRSLT" acro="MRSLT">
            <title>Matrix Representation of a Sum of Linear Transformations</title>
            <idx>
                <h>matrix representation</h>
                <h>sum of linear transformations</h>
            </idx>
            <statement>
                <p>Suppose that <m>\ltdefn{T}{U}{V}</m> and <m>\ltdefn{S}{U}{V}</m> are linear transformations, <m>B</m> is a basis of <m>U</m> and <m>C</m> is a basis of <m>V</m>.  Then<me>\matrixrep{T+S}{B}{C}=\matrixrep{T}{B}{C}+\matrixrep{S}{B}{C}</me>.</p>
            </statement>
            <proof>
                <p>Let <m>\vect{x}</m> be any vector in <m>\complex{n}</m>.  Define <m>\vect{u}\in U</m> by <m>\vect{u}=\vectrepinv{B}{\vect{x}}</m>, so <m>\vect{x}=\vectrep{B}{\vect{u}}</m>.  Then,<md>
                    <mrow>\matrixrep{T+S}{B}{C}\vect{x}&amp;=
                    \matrixrep{T+S}{B}{C}\vectrep{B}{\vect{u}}&amp;&amp;\text{Substitution}</mrow>
                    <mrow>&amp;=\vectrep{C}{\lteval{\left(T+S\right)}{\vect{u}}}&amp;&amp;
                        <xref ref="theorem-FTMR" acro="FTMR"/></mrow>
                    <mrow>&amp;=\vectrep{C}{\lteval{T}{\vect{u}}+\lteval{S}{\vect{u}}}&amp;&amp;
                        <xref ref="definition-LTA" acro="LTA"/></mrow>
                    <mrow>&amp;=\vectrep{C}{\lteval{T}{\vect{u}}}+\vectrep{C}{\lteval{S}{\vect{u}}}&amp;&amp;
                        <xref ref="definition-LT" acro="LT"/></mrow>
                    <mrow>&amp;=\matrixrep{T}{B}{C}\left(\vectrep{B}{\vect{u}}\right)+\matrixrep{S}{B}{C}\left(\vectrep{B}{\vect{u}}\right)&amp;&amp;
                        <xref ref="theorem-FTMR" acro="FTMR"/></mrow>
                    <mrow>&amp;=\left(\matrixrep{T}{B}{C}+\matrixrep{S}{B}{C}\right)\vectrep{B}{\vect{u}}&amp;&amp;
                        <xref ref="theorem-MMDAA" acro="MMDAA"/></mrow>
                    <mrow>&amp;=\left(\matrixrep{T}{B}{C}+\matrixrep{S}{B}{C}\right)\vect{x}&amp;&amp;\text{Substitution}</mrow>
                </md>.</p>
                <p>Since the matrices <m>\matrixrep{T+S}{B}{C}</m> and <m>\matrixrep{T}{B}{C}+\matrixrep{S}{B}{C}</m> have equal matrix-vector products for <em>every</em> vector in <m>\complex{n}</m>, by <xref ref="theorem-EMMVP" acro="EMMVP"/> they are equal matrices.  (Now would be a good time to double-back and study the proof of <xref ref="theorem-EMMVP" acro="EMMVP"/>.  You did promise to come back to this theorem sometime, didn't you?)</p>
            </proof>
        </theorem>
        <theorem xml:id="theorem-MRMLT" acro="MRMLT">
            <title>Matrix Representation of a Multiple of a Linear Transformation</title>
            <idx>
                <h>matrix representation</h>
                <h>multiple of a linear transformation</h>
            </idx>
            <statement>
                <p>Suppose that <m>\ltdefn{T}{U}{V}</m> is a linear transformation, <m>\alpha\in\complexes</m>, <m>B</m> is a basis of <m>U</m> and <m>C</m> is a basis of <m>V</m>.  Then<me>\matrixrep{\alpha T}{B}{C}=\alpha\matrixrep{T}{B}{C}</me>.</p>
            </statement>
            <proof>
                <p>Let <m>\vect{x}</m> be any vector in <m>\complex{n}</m>.  Define <m>\vect{u}\in U</m> by <m>\vect{u}=\vectrepinv{B}{\vect{x}}</m>, so <m>\vect{x}=\vectrep{B}{\vect{u}}</m>.  Then,<md>
                    <mrow>\matrixrep{\alpha T}{B}{C}\vect{x}&amp;=
                    \matrixrep{\alpha T}{B}{C}\vectrep{B}{\vect{u}}&amp;&amp;\text{Substitution}</mrow>
                    <mrow>&amp;=\vectrep{C}{\lteval{\left(\alpha T\right)}{\vect{u}}}&amp;&amp;
                        <xref ref="theorem-FTMR" acro="FTMR"/></mrow>
                    <mrow>&amp;=\vectrep{C}{\alpha\lteval{T}{\vect{u}}}&amp;&amp;
                        <xref ref="definition-LTSM" acro="LTSM"/></mrow>
                    <mrow>&amp;=\alpha\vectrep{C}{\lteval{T}{\vect{u}}}&amp;&amp;
                        <xref ref="definition-LT" acro="LT"/></mrow>
                    <mrow>&amp;=\alpha\left(\matrixrep{T}{B}{C}\vectrep{B}{\vect{u}}\right)&amp;&amp;
                        <xref ref="theorem-FTMR" acro="FTMR"/></mrow>
                    <mrow>&amp;=\left(\alpha\matrixrep{T}{B}{C}\right)\vectrep{B}{\vect{u}}&amp;&amp;
                        <xref ref="theorem-MMSMM" acro="MMSMM"/></mrow>
                    <mrow>&amp;=\left(\alpha\matrixrep{T}{B}{C}\right)\vect{x}&amp;&amp;\text{Substitution}</mrow>
                </md>.</p>
                <p>Since the matrices <m>\matrixrep{\alpha T}{B}{C}</m> and <m>\alpha\matrixrep{T}{B}{C}</m> have equal matrix-vector products for <em>every</em> vector in <m>\complex{n}</m>,  by <xref ref="theorem-EMMVP" acro="EMMVP"/> they are equal matrices.</p>
            </proof>
        </theorem>
        <p>The vector space of all linear transformations from <m>U</m> to <m>V</m> is now isomorphic to the vector space of all <m>m\times n</m> matrices.</p>
        <theorem xml:id="theorem-MRCLT" acro="MRCLT">
            <title>Matrix Representation of a Composition of Linear Transformations</title>
            <idx>
                <h>matrix representation</h>
                <h>composition of linear transformations</h>
            </idx>
            <statement>
                <p>Suppose that <m>\ltdefn{T}{U}{V}</m> and <m>\ltdefn{S}{V}{W}</m> are linear transformations, <m>B</m> is a basis of <m>U</m>, <m>C</m> is a basis of <m>V</m>, and <m>D</m> is a basis of <m>W</m>.  Then<me>\matrixrep{\compose{S}{T}}{B}{D}=\matrixrep{S}{C}{D}\matrixrep{T}{B}{C}</me>.</p>
            </statement>
            <proof>
                <p>Let <m>\vect{x}</m> be any vector in <m>\complex{n}</m>.  Define <m>\vect{u}\in U</m> by <m>\vect{u}=\vectrepinv{B}{\vect{x}}</m>, so <m>\vect{x}=\vectrep{B}{\vect{u}}</m>.  Then,<md>
                    <mrow>\matrixrep{\compose{S}{T}}{B}{D}\vect{x}&amp;=
                    \matrixrep{\compose{S}{T}}{B}{D}\vectrep{B}{\vect{u}}&amp;&amp;\text{Substitution}</mrow>
                    <mrow>&amp;=\vectrep{D}{\lteval{\left(\compose{S}{T}\right)}{\vect{u}}}&amp;&amp;
                        <xref ref="theorem-FTMR" acro="FTMR"/></mrow>
                    <mrow>&amp;=\vectrep{D}{\lteval{S}{\lteval{T}{\vect{u}}}}&amp;&amp;
                        <xref ref="definition-LTC" acro="LTC"/></mrow>
                    <mrow>&amp;=\matrixrep{S}{C}{D}\vectrep{C}{\lteval{T}{\vect{u}}}&amp;&amp;
                        <xref ref="theorem-FTMR" acro="FTMR"/></mrow>
                    <mrow>&amp;=\matrixrep{S}{C}{D}\left(\matrixrep{T}{B}{C}\vectrep{B}{\vect{u}}\right)&amp;&amp;
                        <xref ref="theorem-FTMR" acro="FTMR"/></mrow>
                    <mrow>&amp;=\left(\matrixrep{S}{C}{D}\matrixrep{T}{B}{C}\right)\vectrep{B}{\vect{u}}&amp;&amp;
                        <xref ref="theorem-MMA" acro="MMA"/></mrow>
                    <mrow>&amp;=\left(\matrixrep{S}{C}{D}\matrixrep{T}{B}{C}\right)\vect{x}&amp;&amp;\text{Substitution}</mrow>
                </md>.</p>
                <p>Since the matrices <m>\matrixrep{\compose{S}{T}}{B}{D}</m> and <m>\matrixrep{S}{C}{D}\matrixrep{T}{B}{C}</m> have equal matrix-vector products for <em>every</em> vector in <m>\complex{n}</m>, by <xref ref="theorem-EMMVP" acro="EMMVP"/> they are equal matrices.</p>
            </proof>
        </theorem>
        <p>This is the second great surprise of introductory linear algebra.  Matrices are linear transformations (functions, really), and matrix multiplication is function composition!  We can form the composition of two linear transformations, then form the matrix representation of the result.  Or we can form the matrix representation of each linear transformation separately, then <em>multiply</em> the two representations together via <xref ref="definition-MM" acro="MM"/>.  In either case, we arrive at the same result.</p>
        <example xml:id="example-MPMR" acro="MPMR">
            <title>Matrix product of matrix representations</title>
            <idx>
                <h>matrix product</h>
                <h>as composition of linear transformations</h>
            </idx>
            <p>Consider the two linear transformations,<md>
                <mrow>\ltdefn{T}{\complex{2}}{P_2}&amp;\quad
                \lteval{T}{\colvector{a\\b}}=(-a + 3b)+(2a + 4b)x+(a - 2b)x^2</mrow>
                <mrow>\ltdefn{S}{P_2}{M_{22}}&amp;\quad\
                \lteval{S}{a+bx+cx^2}=
                \begin{bmatrix}
                2a + b + 2c &amp; a + 4b - c\\
                -a + 3c &amp; 3a + b + 2 c\end{bmatrix}</mrow>
            </md>and bases for <m>\complex{2}</m>, <m>P_2</m> and <m>M_{22}</m> (respectively),<md>
                <mrow>B&amp;=\set{\colvector{3\\1},\,\colvector{2\\1}}</mrow>
                <mrow>C&amp;=\set{1-2x+x^2,\,-1+3x,\,2x+3x^2}</mrow>
                <mrow>D&amp;=\set{
                \begin{bmatrix} 1 &amp; -2\\ 1 &amp; -1 \end{bmatrix},\,
                \begin{bmatrix} 1 &amp; -1\\ 1 &amp; 2\end{bmatrix},\,
                \begin{bmatrix} -1 &amp; 2 \\ 0 &amp; 0 \end{bmatrix},\,
                \begin{bmatrix} 2 &amp; -3 \\ 2 &amp; 2\end{bmatrix}
                }</mrow>
            </md>.</p>
            <p>Begin by computing the new linear transformation that is the composition of <m>T</m> and <m>S</m> (<xref ref="definition-LTC" acro="LTC"/>, <xref ref="theorem-CLTLT" acro="CLTLT"/>),  <m>\ltdefn{\left(\compose{S}{T}\right)}{\complex{2}}{M_{22}}</m>,<md>
                <mrow>&amp;\lteval{\left(\compose{S}{T}\right)}{\colvector{a\\b}}=\lteval{S}{\lteval{T}{\colvector{a\\b}}}</mrow>
                <mrow>&amp;\quad\quad=\lteval{S}{(-a + 3b)+(2a + 4b)x+(a - 2b)x^2}</mrow>
                <mrow>&amp;\quad\quad=
                \begin{bmatrix}
                2(-a + 3b) + (2a + 4b) + 2(a - 2b) &amp; (-a + 3b) + 4(2a + 4b) - (a - 2b)\\
                -(-a + 3b) + 3(a - 2b) &amp; 3(-a + 3b) + (2a + 4b) + 2(a - 2b)
                \end{bmatrix}</mrow>
                <mrow>&amp;\quad\quad=
                \begin{bmatrix}
                2a + 6b &amp; 6a + 21b\\
                4a - 9b &amp;a + 9b
                \end{bmatrix}</mrow>
            </md>.</p>
            <p>Now compute the matrix representations (<xref ref="definition-MR" acro="MR"/>) for each of these three linear transformations (<m>T</m>, <m>S</m>, <m>\compose{S}{T}</m>), relative to the appropriate bases.  First for <m>T</m>,<md>
                <mrow>\vectrep{C}{\lteval{T}{\colvector{3\\1}}}&amp;=\vectrep{C}{10x+x^2}</mrow>
                <mrow>&amp;=\vectrep{C}{28(1-2x+x^2)+28(-1+3x)+(-9)(2x+3x^2)}</mrow>
                <mrow>&amp;=\colvector{28\\28\\-9}</mrow>
                <mrow>\vectrep{C}{\lteval{T}{\colvector{2\\1}}}&amp;=\vectrep{C}{1+8x}</mrow>
                <mrow>&amp;=\vectrep{C}{33(1-2x+x^2)+32(-1+3x)+(-11)(2x+3x^2)}</mrow>
                <mrow>&amp;=\colvector{33\\32\\-11}</mrow>
            </md>.</p>
            <p>So we have the matrix representation of <m>T</m>,<me>\matrixrep{T}{B}{C}=
            \begin{bmatrix}
             28 &amp; 33 \\
             28 &amp; 32 \\
             -9 &amp; -11
            \end{bmatrix}</me>.</p>
            <p>Now, a representation of <m>S</m>,<md>
                <mrow>&amp;\vectrep{D}{\lteval{S}{1-2x+x^2}}=\vectrep{D}{\begin{bmatrix}2 &amp; -8 \\ 2 &amp; 3 \end{bmatrix}}</mrow>
                <mrow>&amp;\quad\quad=\vectrep{D}{
                (-11)\begin{bmatrix} 1 &amp; -2\\ 1 &amp; -1 \end{bmatrix}+
                (-21)\begin{bmatrix} 1 &amp; -1\\ 1 &amp; 2\end{bmatrix}+
                0\begin{bmatrix} -1 &amp; 2 \\ 0 &amp; 0 \end{bmatrix}+
                (17)\begin{bmatrix} 2 &amp; -3 \\ 2 &amp; 2\end{bmatrix}
                }</mrow>
                <mrow>&amp;\quad\quad=\colvector{-11\\-21\\0\\17}</mrow>
                <mrow>&amp;\vectrep{D}{\lteval{S}{-1+3x}}=\vectrep{D}{\begin{bmatrix}1 &amp; 11 \\ 1 &amp; 0\end{bmatrix}}</mrow>
                <mrow>&amp;\quad\quad=\vectrep{D}{
                26\begin{bmatrix} 1 &amp; -2\\ 1 &amp; -1 \end{bmatrix}+
                51\begin{bmatrix} 1 &amp; -1\\ 1 &amp; 2\end{bmatrix}+
                0\begin{bmatrix} -1 &amp; 2 \\ 0 &amp; 0 \end{bmatrix}+
                (-38)\begin{bmatrix} 2 &amp; -3 \\ 2 &amp; 2\end{bmatrix}
                }</mrow>
                <mrow>&amp;\quad\quad=\colvector{26\\51\\0\\-38}</mrow>
                <mrow>&amp;\vectrep{D}{\lteval{S}{2x+3x^2}}=\vectrep{D}{\begin{bmatrix}8 &amp; 5 \\ 9 &amp; 8\end{bmatrix}}</mrow>
                <mrow>&amp;\quad\quad=\vectrep{D}{
                34\begin{bmatrix} 1 &amp; -2\\ 1 &amp; -1 \end{bmatrix}+
                67\begin{bmatrix} 1 &amp; -1\\ 1 &amp; 2\end{bmatrix}+
                1\begin{bmatrix} -1 &amp; 2 \\ 0 &amp; 0 \end{bmatrix}+
                (-46)\begin{bmatrix} 2 &amp; -3 \\ 2 &amp; 2\end{bmatrix}
                }</mrow>
                <mrow>&amp;\quad\quad=\colvector{34\\67\\1\\-46}</mrow>
            </md>.</p>
            <p>So we have the matrix representation of <m>S</m>,<me>\matrixrep{S}{C}{D}=
            \begin{bmatrix}
             -11 &amp; 26 &amp; 34 \\
             -21 &amp; 51 &amp; 67 \\
             0 &amp; 0 &amp; 1 \\
             17 &amp; -38 &amp; -46
            \end{bmatrix}</me>.</p>
            <p>Finally, a representation of <m>\compose{S}{T}</m>,<md>
                <mrow>&amp;\vectrep{D}{\lteval{\left(\compose{S}{T}\right)}{\colvector{3\\1}}}=\vectrep{D}{\begin{bmatrix}12 &amp; 39\\3 &amp;12\end{bmatrix}}</mrow>
                <mrow>&amp;\quad\quad=\vectrep{D}{
                114\begin{bmatrix} 1 &amp; -2\\ 1 &amp; -1 \end{bmatrix}+
                237\begin{bmatrix} 1 &amp; -1\\ 1 &amp; 2\end{bmatrix}+
                (-9)\begin{bmatrix} -1 &amp; 2 \\ 0 &amp; 0 \end{bmatrix}+
                (-174)\begin{bmatrix} 2 &amp; -3 \\ 2 &amp; 2\end{bmatrix}
                }</mrow>
                <mrow>&amp;\quad\quad=\colvector{114\\237\\-9\\-174}</mrow>
                <mrow>&amp;\vectrep{D}{\lteval{\left(\compose{S}{T}\right)}{\colvector{2\\1}}}=\vectrep{D}{\begin{bmatrix}10 &amp; 33\\-1 &amp; 11\end{bmatrix}}</mrow>
                <mrow>&amp;\quad\quad=\vectrep{D}{
                95\begin{bmatrix} 1 &amp; -2\\ 1 &amp; -1 \end{bmatrix}+
                202\begin{bmatrix} 1 &amp; -1\\ 1 &amp; 2\end{bmatrix}+
                (-11)\begin{bmatrix} -1 &amp; 2 \\ 0 &amp; 0 \end{bmatrix}+
                (-149)\begin{bmatrix} 2 &amp; -3 \\ 2 &amp; 2\end{bmatrix}
                }</mrow>
                <mrow>&amp;\quad\quad=\colvector{95\\202\\-11\\-149}</mrow>
            </md>.</p>
            <p>So we have the matrix representation of <m>\compose{S}{T}</m>,<me>\matrixrep{\compose{S}{T}}{B}{D}=
            \begin{bmatrix}
             114 &amp; 95 \\
             237 &amp; 202 \\
             -9 &amp; -11 \\
             -174 &amp; -149
            \end{bmatrix}</me>.</p>
            <p>Now, we are all set to verify the conclusion of <xref ref="theorem-MRCLT" acro="MRCLT"/>,<md>
                <mrow>\matrixrep{S}{C}{D}\matrixrep{T}{B}{C}
                &amp;=
                \begin{bmatrix}
                 -11 &amp; 26 &amp; 34 \\
                 -21 &amp; 51 &amp; 67 \\
                 0 &amp; 0 &amp; 1 \\
                 17 &amp; -38 &amp; -46
                \end{bmatrix}
                \begin{bmatrix}
                 28 &amp; 33 \\
                 28 &amp; 32 \\
                 -9 &amp; -11
                \end{bmatrix}</mrow>
                <mrow>&amp;=
                \begin{bmatrix}
                 114 &amp; 95 \\
                 237 &amp; 202 \\
                 -9 &amp; -11 \\
                 -174 &amp; -149
                \end{bmatrix}</mrow>
                <mrow>&amp;=\matrixrep{\compose{S}{T}}{B}{D}</mrow>
            </md>.</p>
            <p>We have intentionally used nonstandard bases.  If you were to choose <q>nice</q> bases for the three vector spaces, then the result of the theorem might be rather transparent.  But this would still be a worthwhile exercise <mdash/> give it a go.</p>
        </example>
        <p>A diagram, similar to ones we have seen earlier, might make the importance of this theorem clearer.</p>
        <figure xml:id="figure-MRCLT" acro="MRCLT">
            <caption>Matrix Representation and Composition of Linear Transformations</caption>
            <image xml:id="image-MRCLT">
                <latex-image>
                \begin{tikzpicture}
                \matrix (m) [matrix of math nodes, row sep=5em, column sep=10em, text height=1.5ex, text depth=0.25ex]
                { S, T \amp \matrixrep{S}{C}{D},\ \matrixrep{T}{B}{C} \\
                \compose{S}{T} \amp \matrixrep{\compose{S}{T}}{B}{D}=\matrixrep{S}{C}{D}\,\matrixrep{T}{B}{C}\\};
                \path[->]
                (m-1-1) edge[thick] node[auto]  {Definition MR}  (m-1-2)
                (m-1-2) edge[thick] node[right] {Definition MM}  (m-2-2)
                (m-1-1) edge[thick] node[left]  {Definition LTC} (m-2-1)
                (m-2-1) edge[thick] node[auto]  {Definition MR}  (m-2-2);
                \end{tikzpicture}
                </latex-image>
            </image>
        </figure>
        <p>One of our goals in the first part of this book is to make the definition of matrix multiplication (<xref ref="definition-MVP" acro="MVP"/>, <xref ref="definition-MM" acro="MM"/>) seem as natural as possible.  However, many of us are brought up with an entry-by-entry description of matrix multiplication (<xref ref="theorem-EMP" acro="EMP"/>) as the <em>definition</em> of matrix multiplication, and then theorems about columns of matrices and linear combinations follow from that definition.  With this unmotivated definition, the realization that matrix multiplication is function composition is quite remarkable.  It is an interesting exercise to begin with the question, <q>What is the matrix representation of the composition of two linear transformations?</q> and then, without using any theorems about matrix multiplication, finally arrive at the entry-by-entry description of matrix multiplication.  Try it yourself (<xref ref="exercise-MR-T80" acro="MR.T80"/>).</p>
        <computation xml:id="sage-SUTH3" acro="SUTH3">
            <title>Sage Under The Hood, Round 3</title>
            <idx>
                <h>sage under the hood</h>
                <h>round 3</h>
            </idx>
            <p>We have seen that Sage is able to add two linear transformations, or multiply a single linear transformation by a scalar.  Under the hood, this is accomplished by simply adding matrix representations, or multiplying a matrix by a scalar, according to <xref ref="theorem-MRSLT" acro="MRSLT"/> and <xref ref="theorem-MRMLT" acro="MRMLT"/> (respectively).  <xref ref="theorem-MRCLT" acro="MRCLT"/> says linear transformation composition is matrix representation multiplication.  Is it still a mystery why we use the symbol <c>*</c> for linear transformation composition in Sage?</p>
            <p>We could do several examples here, but you should now be able to construct them yourselves.  We will do just one, linear transformation composition is matrix representation multiplication.</p>
            <sage xml:id="sagecell-SUTH3-1">
                <input>
                x1, x2, x3, x4 = var('x1, x2, x3, x4')
                outputs = [-5*x1 - 2*x2 +   x3,
                            4*x1 - 3*x2 - 3*x3,
                            4*x1 - 6*x2 - 4*x3,
                            5*x1 + 3*x2       ]
                T_symbolic(x1, x2, x3) = outputs
                outputs = [-3*x1 - x2 + x3 + 2*x4,
                            7*x1 + x2 + x3 -   x4]
                S_symbolic(x1, x2, x3, x4) = outputs
                b0 = vector(QQ, [-1, -2,  2])
                b1 = vector(QQ, [ 1,  1,  0])
                b2 = vector(QQ, [ 0,  3, -5])
                U = (QQ^3).subspace_with_basis([b0, b1, b2])
                c0 = vector(QQ, [ 0,  0,  2,  1])
                c1 = vector(QQ, [ 2, -3, -1, -6])
                c2 = vector(QQ, [-2,  3,  2,  7])
                c3 = vector(QQ, [ 1, -2, -4, -6])
                V = (QQ^4).subspace_with_basis([c0, c1, c2, c3])
                d0 = vector(QQ, [3, 4])
                d1 = vector(QQ, [2, 3])
                W = (QQ^2).subspace_with_basis([d0, d1])
                T = linear_transformation(U, V, T_symbolic)
                S = linear_transformation(V, W, S_symbolic)
                (S*T).matrix('right')
                </input>
                <output>
                [-321  218  297]
                [ 456 -310 -422]
                </output>
            </sage>
            <sage xml:id="sagecell-SUTH3-2">
                <input>
                S.matrix(side='right')*T.matrix(side='right')
                </input>
                                <output>
                [-321  218  297]
                [ 456 -310 -422]
                </output>
            </sage>
            <p>Extra credit: what changes do you need to make if you dropped the <c>side='right'</c> option on these three matrix representations?</p>
        </computation>
    </subsection>
    <subsection xml:id="subsection-MR-PMR" acro="PMR">
        <title>Properties of Matrix Representations</title>
        <p>It will not be a surprise to discover that the kernel and range of a linear transformation are closely related to the null space and column space of the transformation's matrix representation.  Perhaps this idea has been bouncing around in your head already, even before seeing the definition of a matrix representation.  However, with a formal definition of a matrix representation (<xref ref="definition-MR" acro="MR"/>), and a fundamental theorem to go with it (<xref ref="theorem-FTMR" acro="FTMR"/>) we can be formal about the relationship, using the idea of isomorphic vector spaces (<xref ref="definition-IVS" acro="IVS"/>).  Here are the twin theorems.</p>
        <theorem xml:id="theorem-KNSI" acro="KNSI">
            <title>Kernel and Null Space Isomorphism</title>
            <idx>
                <h>kernel</h>
                <h>isomorphic to null space</h>
            </idx>
            <statement>
                <idx>
                    <h>null space</h>
                    <h>isomorphic to kernel</h>
                </idx>
                <p>Suppose that <m>\ltdefn{T}{U}{V}</m> is a linear transformation, <m>B</m> is a basis for <m>U</m> of size <m>n</m>, and <m>C</m> is a basis for <m>V</m>.  Then the kernel of <m>T</m> is isomorphic to the null space of <m>\matrixrep{T}{B}{C}</m>,<me>\krn{T}\isomorphic\nsp{\matrixrep{T}{B}{C}}</me>.</p>
            </statement>
            <proof>
                <p>To establish that two vector spaces are isomorphic, we must find an isomorphism between them, an invertible linear transformation (<xref ref="definition-IVS" acro="IVS"/>).  The kernel of the linear transformation <m>T</m>, <m>\krn{T}</m>, is a subspace of <m>U</m>, while the null space of the matrix representation, <m>\nsp{\matrixrep{T}{B}{C}}</m> is a subspace of <m>\complex{n}</m>.  The function <m>\vectrepname{B}</m> is defined as a function from <m>U</m> to <m>\complex{n}</m>, but we can just as well employ the definition of <m>\vectrepname{B}</m> as a function from <m>\krn{T}</m> to <m>\nsp{\matrixrep{T}{B}{C}}</m>.</p>
                <p>We must first insure that if we choose an input for <m>\vectrepname{B}</m> from <m>\krn{T}</m> that then the output will be an element of <m>\nsp{\matrixrep{T}{B}{C}}</m>.  So suppose that <m>\vect{u}\in\krn{T}</m>.  Then<md>
                    <mrow>\matrixrep{T}{B}{C}\vectrep{B}{\vect{u}}
                    &amp;=\vectrep{C}{\lteval{T}{\vect{u}}}&amp;&amp;
                        <xref ref="theorem-FTMR" acro="FTMR"/></mrow>
                    <mrow>&amp;=\vectrep{C}{\zerovector}&amp;&amp;
                        <xref ref="definition-KLT" acro="KLT"/></mrow>
                    <mrow>&amp;=\zerovector&amp;&amp;
                        <xref ref="theorem-LTTZZ" acro="LTTZZ"/></mrow>
                </md>.</p>
                <p>This says that <m>\vectrep{B}{\vect{u}}\in\nsp{\matrixrep{T}{B}{C}}</m>, as desired.</p>
                <p>The restriction in the size of the domain and codomain <m>\vectrepname{B}</m> will not affect the fact that <m>\vectrepname{B}</m> is a linear transformation (<xref ref="theorem-VRLT" acro="VRLT"/>), nor will it affect the fact that <m>\vectrepname{B}</m> is injective (<xref ref="theorem-VRI" acro="VRI"/>).  Something must be done though to verify that <m>\vectrepname{B}</m> is surjective.  To this end, appeal to the definition of surjective (<xref ref="definition-SLT" acro="SLT"/>), and suppose that we have an element of the codomain, <m>\vect{x}\in\nsp{\matrixrep{T}{B}{C}}\subseteq\complex{n}</m> and we wish to find an element of the domain with <m>\vect{x}</m> as its image.  We now show that the desired element of the domain is <m>\vect{u}=\vectrepinv{B}{\vect{x}}</m>.  First, verify that <m>\vect{u}\in\krn{T}</m>,<md>
                    <mrow>\lteval{T}{\vect{u}}
                    &amp;=\lteval{T}{\vectrepinv{B}{\vect{x}}}</mrow>
                    <mrow>&amp;=\vectrepinv{C}{\matrixrep{T}{B}{C}\left(\vectrep{B}{\vectrepinv{B}{\vect{x}}}\right)}&amp;&amp;
                        <xref ref="theorem-FTMR" acro="FTMR"/></mrow>
                    <mrow>&amp;=\vectrepinv{C}{\matrixrep{T}{B}{C}\left(\lteval{I_{\complex{n}}}{\vect{x}}\right)}&amp;&amp;
                        <xref ref="definition-IVLT" acro="IVLT"/></mrow>
                    <mrow>&amp;=\vectrepinv{C}{\matrixrep{T}{B}{C}\vect{x}}&amp;&amp;
                        <xref ref="definition-IDLT" acro="IDLT"/></mrow>
                    <mrow>&amp;=\vectrepinv{C}{\zerovector_{\complex{n}}}&amp;&amp;
                        <xref ref="definition-KLT" acro="KLT"/></mrow>
                    <mrow>&amp;=\zerovector_{V}&amp;&amp;
                        <xref ref="theorem-LTTZZ" acro="LTTZZ"/></mrow>
                    <intertext>Second, verify that the proposed isomorphism, <m>\vectrepname{B}</m>, takes <m>\vect{u}</m> to <m>\vect{x}</m>,</intertext>
                    <mrow>\vectrep{B}{\vect{u}}&amp;=\vectrep{B}{\vectrepinv{B}{\vect{x}}}&amp;&amp;\text{Substitution}</mrow>
                    <mrow>&amp;=\lteval{I_{\complex{n}}}{\vect{x}}&amp;&amp;
                        <xref ref="definition-IVLT" acro="IVLT"/></mrow>
                    <mrow>&amp;=\vect{x}&amp;&amp;
                        <xref ref="definition-IDLT" acro="IDLT"/></mrow>
                </md>.</p>
                <p>With <m>\vectrepname{B}</m> demonstrated to be an injective and surjective linear transformation from <m>\krn{T}</m> to <m>\nsp{\matrixrep{T}{B}{C}}</m>, <xref ref="theorem-ILTIS" acro="ILTIS"/> tells us <m>\vectrepname{B}</m> is invertible, and so by <xref ref="definition-IVS" acro="IVS"/>, we say <m>\krn{T}</m> and <m>\nsp{\matrixrep{T}{B}{C}}</m> are isomorphic.</p>
            </proof>
        </theorem>
        <example xml:id="example-KVMR" acro="KVMR">
            <title>Kernel via matrix representation</title>
            <idx>
                <h>kernel</h>
                <h>via matrix representation</h>
            </idx>
            <p>Consider the kernel of the linear transformation, <m>\ltdefn{T}{M_{22}}{P_2}</m>, given by<me>\lteval{T}{\begin{bmatrix}a&amp;b\\c&amp;d\end{bmatrix}}=
            (2a-b+c-5d)+(a+4b+5c+2d)x+(3a-2b+c-8d)x^2</me>.</p>
            <p>We will begin with a matrix representation of <m>T</m> relative to the bases for <m>M_{22}</m> and <m>P_2</m> (respectively),<md>
                <mrow>B&amp;=\set{
                \begin{bmatrix}1 &amp; 2 \\ -1 &amp; -1\end{bmatrix},\,
                \begin{bmatrix}1 &amp; 3 \\ -1 &amp; -4\end{bmatrix},\,
                \begin{bmatrix}1 &amp; 2 \\ 0 &amp; -2\end{bmatrix},\,
                \begin{bmatrix}2 &amp; 5 \\ -2 &amp; -4\end{bmatrix}
                }</mrow>
                <mrow>C&amp;=\set{1+x+x^2,\,2+3x,\,-1-2x^2}</mrow>
            </md>.</p>
            <p>Then,<md>
                <mrow>\vectrep{C}{\lteval{T}{\begin{bmatrix}1 &amp; 2 \\ -1 &amp; -1\end{bmatrix}}}
                &amp;=\vectrep{C}{4+2x+6x^2}</mrow>
                <mrow>&amp;=\vectrep{C}{2(1+x+x^2)+0(2+3x)+(-2)(-1-2x^2)}</mrow>
                <mrow>&amp;=\colvector{2\\0\\-2}</mrow>
                <mrow>\vectrep{C}{\lteval{T}{\begin{bmatrix}1 &amp; 3 \\ -1 &amp; -4\end{bmatrix}}}
                &amp;=\vectrep{C}{18+28x^2}</mrow>
                <mrow>&amp;=\vectrep{C}{(-24)(1+x+x^2)+8(2+3x)+(-26)(-1-2x^2)}</mrow>
                <mrow>&amp;=\colvector{-24\\8\\-26}</mrow>
                <mrow>\vectrep{C}{\lteval{T}{\begin{bmatrix}1 &amp; 2 \\ 0 &amp; -2\end{bmatrix}}}
                &amp;=\vectrep{C}{10+5x+15x^2}</mrow>
                <mrow>&amp;=\vectrep{C}{5(1+x+x^2)+0(2+3x)+(-5)(-1-2x^2)}</mrow>
                <mrow>&amp;=\colvector{5\\0\\-5}</mrow>
                <mrow>\vectrep{C}{\lteval{T}{\begin{bmatrix}2 &amp; 5 \\ -2 &amp; -4\end{bmatrix}}}
                &amp;=\vectrep{C}{17+4x+26x^2}</mrow>
                <mrow>&amp;=\vectrep{C}{(-8)(1+x+x^2)+(4)(2+3x)+(-17)(-1-2x^2)}</mrow>
                <mrow>&amp;=\colvector{-8\\4\\-17}</mrow>
            </md>.</p>
            <p>So the matrix representation of <m>T</m> (relative to <m>B</m> and <m>C</m>) is<me>\matrixrep{T}{B}{C}
            =
            \begin{bmatrix}
             2 &amp; -24 &amp; 5 &amp; -8 \\
             0 &amp; 8 &amp; 0 &amp; 4 \\
             -2 &amp; -26 &amp; -5 &amp; -17
            \end{bmatrix}</me>.</p>
            <p>We know from <xref ref="theorem-KNSI" acro="KNSI"/> that the kernel of the linear transformation <m>T</m> is isomorphic to the null space of the matrix representation <m>\matrixrep{T}{B}{C}</m> and by studying the proof of <xref ref="theorem-KNSI" acro="KNSI"/> we learn that <m>\vectrepname{B}</m> is an isomorphism between these null spaces. Rather than trying to compute the kernel of <m>T</m> using definitions and techniques from <xref ref="chapter-LT" acro="LT"/> we will instead analyze the null space of <m>\matrixrep{T}{B}{C}</m> using techniques from way back in <xref ref="chapter-V" acro="V"/>.  First row-reduce <m>\matrixrep{T}{B}{C}</m>,<me>\begin{bmatrix}
            2 &amp; -24 &amp; 5 &amp; -8 \\
            0 &amp; 8 &amp; 0 &amp; 4 \\
            -2 &amp; -26 &amp; -5 &amp; -17
            \end{bmatrix}
            \rref
            \begin{bmatrix}
             \leading{1} &amp; 0 &amp; \frac{5}{2} &amp; 2 \\
             0 &amp; \leading{1} &amp; 0 &amp; \frac{1}{2} \\
             0 &amp; 0 &amp; 0 &amp; 0
            \end{bmatrix}</me>.</p>
            <p>So, by <xref ref="theorem-BNS" acro="BNS"/>, a basis for <m>\nsp{\matrixrep{T}{B}{C}}</m> is<me>\set{\colvector{-\frac{5}{2}\\0\\1\\0},\,\colvector{-2\\-\frac{1}{2}\\0\\1}}</me>.</p>
            <p>We can  now convert this basis of <m>\nsp{\matrixrep{T}{B}{C}}</m> into a basis of <m>\krn{T}</m> by applying <m>\vectrepinvname{B}</m> to each element of the basis,<md>
                <mrow>\vectrepinv{B}{\colvector{-\frac{5}{2}\\0\\1\\0}}&amp;=
                (-\frac{5}{2})\begin{bmatrix}1 &amp; 2 \\ -1 &amp; -1\end{bmatrix}+
                0\begin{bmatrix}1 &amp; 3 \\ -1 &amp; -4\end{bmatrix}+
                1\begin{bmatrix}1 &amp; 2 \\ 0 &amp; -2\end{bmatrix}+
                0\begin{bmatrix}2 &amp; 5 \\ -2 &amp; -4\end{bmatrix}</mrow>
                <mrow>&amp;=\begin{bmatrix}-\frac{3}{2} &amp; -3 \\ \frac{5}{2} &amp; \frac{1}{2}\end{bmatrix}</mrow>
                <mrow>\vectrepinv{B}{\colvector{-2\\-\frac{1}{2}\\0\\1}}&amp;=
                (-2)\begin{bmatrix}1 &amp; 2 \\ -1 &amp; -1\end{bmatrix}+
                (-\frac{1}{2})\begin{bmatrix}1 &amp; 3 \\ -1 &amp; -4\end{bmatrix}+
                0\begin{bmatrix}1 &amp; 2 \\ 0 &amp; -2\end{bmatrix}+
                1\begin{bmatrix}2 &amp; 5 \\ -2 &amp; -4\end{bmatrix}</mrow>
                <mrow>&amp;=\begin{bmatrix}-\frac{1}{2} &amp; -\frac{1}{2} \\ \frac{1}{2} &amp;0\end{bmatrix}</mrow>
            </md>.</p>
            <p>So the set<me>\set{
            \begin{bmatrix}-\frac{3}{2} &amp; -3 \\ \frac{5}{2} &amp; \frac{1}{2}\end{bmatrix},\,
            \begin{bmatrix}-\frac{1}{2} &amp; -\frac{1}{2} \\ \frac{1}{2} &amp;0\end{bmatrix}
            }</me>is a basis for <m>\krn{T}</m>.  Just for fun, you might evaluate <m>T</m> with each of these two basis vectors and verify that the output is the zero polynomial (<xref ref="exercise-MR-C10" acro="MR.C10"/>).</p>
        </example>
        <p>An entirely similar result applies to the range of a linear transformation and the column space of a matrix representation of the linear transformation.</p>
        <theorem xml:id="theorem-RCSI" acro="RCSI">
            <title>Range and Column Space Isomorphism</title>
            <idx>
                <h>range</h>
                <h>isomorphic to column space</h>
            </idx>
            <statement>
                <idx>
                    <h>column space</h>
                    <h>isomorphic to range</h>
                </idx>
                <p>Suppose that <m>\ltdefn{T}{U}{V}</m> is a linear transformation, <m>B</m> is a basis for <m>U</m> of size <m>n</m>, and <m>C</m> is a basis for <m>V</m> of size <m>m</m>.  Then the range of <m>T</m> is isomorphic to the column space of <m>\matrixrep{T}{B}{C}</m>,<me>\rng{T}\isomorphic\csp{\matrixrep{T}{B}{C}}</me>.</p>
            </statement>
            <proof>
                <p>To establish that two vector spaces are isomorphic, we must find an isomorphism between them, an invertible linear transformation (<xref ref="definition-IVS" acro="IVS"/>).  The range of the linear transformation <m>T</m>, <m>\rng{T}</m>, is a subspace of <m>V</m>, while the column space of the matrix representation, <m>\csp{\matrixrep{T}{B}{C}}</m> is a subspace of <m>\complex{m}</m>.  The function <m>\vectrepname{C}</m> is defined as a function from <m>V</m> to <m>\complex{m}</m>, but we can just as well employ the definition of <m>\vectrepname{C}</m> as a function from <m>\rng{T}</m> to <m>\csp{\matrixrep{T}{B}{C}}</m>.</p>
                <p>We must first insure that if we choose an input for <m>\vectrepname{C}</m> from <m>\rng{T}</m> that then the output will be an element of <m>\csp{\matrixrep{T}{B}{C}}</m>.  So suppose that <m>\vect{v}\in\rng{T}</m>.  Then there is a vector <m>\vect{u}\in U</m>, such that <m>\lteval{T}{\vect{u}}=\vect{v}</m>.  Consider<md>
                    <mrow>\matrixrep{T}{B}{C}\vectrep{B}{\vect{u}}
                    &amp;=\vectrep{C}{\lteval{T}{\vect{u}}}&amp;&amp;
                        <xref ref="theorem-FTMR" acro="FTMR"/></mrow>
                    <mrow>&amp;=\vectrep{C}{\vect{v}}&amp;&amp;
                        <xref ref="definition-RLT" acro="RLT"/></mrow>
                </md>.</p>
                <p>This says that <m>\vectrep{C}{\vect{v}}\in\csp{\matrixrep{T}{B}{C}}</m>, as desired.</p>
                <p>The restriction in the size of the domain and codomain will not affect the fact that <m>\vectrepname{C}</m> is a linear transformation (<xref ref="theorem-VRLT" acro="VRLT"/>), nor will it affect the fact that <m>\vectrepname{C}</m> is injective (<xref ref="theorem-VRI" acro="VRI"/>).  Something must be done though to verify that <m>\vectrepname{C}</m> is surjective.  This all gets a bit confusing, since the domain of our isomorphism is the range of the linear transformation, so think about your objects as you go.  To establish that <m>\vectrepname{C}</m> is surjective, appeal to the definition of a surjective linear transformation (<xref ref="definition-SLT" acro="SLT"/>), and suppose that we have an element of the codomain, <m>\vect{y}\in\csp{\matrixrep{T}{B}{C}}\subseteq\complex{m}</m> and we wish to find an element of the domain with <m>\vect{y}</m> as its image.  Since <m>\vect{y}\in\csp{\matrixrep{T}{B}{C}}</m>, there exists a vector, <m>\vect{x}\in\complex{n}</m> with <m>\matrixrep{T}{B}{C}\vect{x}=\vect{y}</m>.</p>
                <p>We now show that the desired element of the domain is <m>\vect{v}=\vectrepinv{C}{\vect{y}}</m>.     First, verify that <m>\vect{v}\in\rng{T}</m> by applying <m>T</m> to <m>\vect{u}=\vectrepinv{B}{\vect{x}}</m>,<md>
                    <mrow>\lteval{T}{\vect{u}}&amp;=\lteval{T}{\vectrepinv{B}{\vect{x}}}</mrow>
                    <mrow>&amp;=\vectrepinv{C}{\matrixrep{T}{B}{C}\left(\vectrep{B}{\vectrepinv{B}{\vect{x}}}\right)}&amp;&amp;
                        <xref ref="theorem-FTMR" acro="FTMR"/></mrow>
                    <mrow>&amp;=\vectrepinv{C}{\matrixrep{T}{B}{C}\left(\lteval{I_{\complex{n}}}{\vect{x}}\right)}&amp;&amp;
                        <xref ref="definition-IVLT" acro="IVLT"/></mrow>
                    <mrow>&amp;=\vectrepinv{C}{\matrixrep{T}{B}{C}\vect{x}}&amp;&amp;
                        <xref ref="definition-IDLT" acro="IDLT"/></mrow>
                    <mrow>&amp;=\vectrepinv{C}{\vect{y}}&amp;&amp;
                        <xref ref="definition-CSM" acro="CSM"/></mrow>
                    <mrow>&amp;=\vect{v}&amp;&amp;\text{Substitution}</mrow>
                    <intertext>Second, verify that the proposed isomorphism, <m>\vectrepname{C}</m>, takes <m>\vect{v}</m> to <m>\vect{y}</m>,</intertext>
                    <mrow>\vectrep{C}{\vect{v}}&amp;=\vectrep{C}{\vectrepinv{C}{\vect{y}}}&amp;&amp;\text{Substitution}</mrow>
                    <mrow>&amp;=\lteval{I_{\complex{m}}}{\vect{y}}&amp;&amp;
                        <xref ref="definition-IVLT" acro="IVLT"/></mrow>
                    <mrow>&amp;=\vect{y}&amp;&amp;
                        <xref ref="definition-IDLT" acro="IDLT"/></mrow>
                </md>.</p>
                <p>With <m>\vectrepname{C}</m> demonstrated to be an injective and surjective linear transformation from <m>\rng{T}</m> to <m>\csp{\matrixrep{T}{B}{C}}</m>, <xref ref="theorem-ILTIS" acro="ILTIS"/> tells us <m>\vectrepname{C}</m> is invertible, and so by <xref ref="definition-IVS" acro="IVS"/>, we say <m>\rng{T}</m> and <m>\csp{\matrixrep{T}{B}{C}}</m> are isomorphic.</p>
            </proof>
        </theorem>
        <example xml:id="example-RVMR" acro="RVMR">
            <title>Range via matrix representation</title>
            <idx>
                <h>range</h>
                <h>via matrix representation</h>
            </idx>
            <p>In this example, we will recycle the linear transformation <m>T</m> and the bases <m>B</m> and <m>C</m> of <xref ref="example-KVMR" acro="KVMR"/> but now we will compute the range of <m>\ltdefn{T}{M_{22}}{P_2}</m>, given by<me>\lteval{T}{\begin{bmatrix}a&amp;b\\c&amp;d\end{bmatrix}}=
            (2a-b+c-5d)+(a+4b+5c+2d)x+(3a-2b+c-8d)x^2</me>.</p>
            <p>With bases <m>B</m> and <m>C</m>,<md>
                <mrow>B&amp;=\set{
                \begin{bmatrix}1 &amp; 2 \\ -1 &amp; -1\end{bmatrix},\,
                \begin{bmatrix}1 &amp; 3 \\ -1 &amp; -4\end{bmatrix},\,
                \begin{bmatrix}1 &amp; 2 \\ 0 &amp; -2\end{bmatrix},\,
                \begin{bmatrix}2 &amp; 5 \\ -2 &amp; -4\end{bmatrix}
                }</mrow>
                <mrow>C&amp;=\set{1+x+x^2,\,2+3x,\,-1-2x^2}</mrow>
            </md>we obtain the matrix representation<me>\matrixrep{T}{B}{C}
            =
            \begin{bmatrix}
             2 &amp; -24 &amp; 5 &amp; -8 \\
             0 &amp; 8 &amp; 0 &amp; 4 \\
             -2 &amp; -26 &amp; -5 &amp; -17
            \end{bmatrix}</me>.</p>
            <p>We know from <xref ref="theorem-RCSI" acro="RCSI"/> that the range of the linear transformation <m>T</m> is isomorphic to the column space of the matrix representation <m>\matrixrep{T}{B}{C}</m> and by studying the proof of <xref ref="theorem-RCSI" acro="RCSI"/> we learn that <m>\vectrepname{C}</m> is an isomorphism between these subspaces.   Notice that since the range is a subspace of the codomain, we will employ <m>\vectrepname{C}</m> as the isomorphism, rather than <m>\vectrepname{B}</m>, which was the correct choice for an isomorphism between the null spaces of <xref ref="example-KVMR" acro="KVMR"/>.</p>
            <p>Rather than trying to compute the range of <m>T</m> using definitions and techniques from <xref ref="chapter-LT" acro="LT"/> we will instead analyze the column space of <m>\matrixrep{T}{B}{C}</m> using techniques from way back in <xref ref="chapter-M" acro="M"/>.  First row-reduce <m>\transpose{\left(\matrixrep{T}{B}{C}\right)}</m>,<me>\begin{bmatrix}
            2 &amp; 0 &amp; -2 \\
            -24 &amp; 8 &amp; -26 \\
            5 &amp; 0 &amp; -5 \\
            -8 &amp; 4 &amp; -17
            \end{bmatrix}
            \rref
            \begin{bmatrix}
            \leading{1} &amp; 0 &amp; -1 \\
            0 &amp; \leading{1} &amp; -\frac{25}{4} \\
            0 &amp; 0 &amp; 0 \\
            0 &amp; 0 &amp; 0
            \end{bmatrix}</me>.</p>
            <p>Now employ <xref ref="theorem-CSRST" acro="CSRST"/> and <xref ref="theorem-BRS" acro="BRS"/> (there are other methods we could choose here to compute the column space, such as <xref ref="theorem-BCS" acro="BCS"/>) to obtain the basis for <m>\csp{\matrixrep{T}{B}{C}}</m>,<me>\set{
            \colvector{1\\0\\-1},\,
            \colvector{0\\1\\-\frac{25}{4}}
            }</me>.</p>
            <p>We can  now convert this basis of <m>\csp{\matrixrep{T}{B}{C}}</m> into a basis of <m>\rng{T}</m> by applying <m>\vectrepinvname{C}</m> to each element of the basis,<md>
                <mrow>\vectrepinv{C}{\colvector{1\\0\\-1}}&amp;=
                (1+x+x^2)-(-1-2x^2)=2+x+3x^2</mrow>
                <mrow>\vectrepinv{C}{\colvector{0\\1\\-\frac{25}{4}}}&amp;=
                (2+3x)-\frac{25}{4}(-1-2x^2)=\frac{33}{4}+3x+\frac{31}{2}x^2</mrow>
            </md>.</p>
            <p>So the set<me>\set{
                2+3x+3x^2,\,
                \frac{33}{4}+3x+\frac{31}{2}x^2
                }</me>is a basis for <m>\rng{T}</m>.</p>
        </example>
        <p><xref ref="theorem-KNSI" acro="KNSI"/> and <xref ref="theorem-RCSI" acro="RCSI"/> can be viewed as further formal evidence for the <xref ref="principle-CP" acro="CP" text="title" />, though they are not direct consequences.</p>
        <p><xref ref="figure-KRI" acro="KRI"/> is meant to suggest <xref ref="theorem-KNSI" acro="KNSI"/> and <xref ref="theorem-RCSI" acro="RCSI"/>, in addition to their proofs (and so carry the same notation as the statements of these two theorems).  The dashed lines indicate a subspace relationship, with the smaller vector space lower down in the diagram.  The central square is highly reminiscent of <xref ref="figure-FTMR" acro="FTMR"/>.  Each of the four vector representations is an isomorphism, so the inverse linear transformation could be depicted with an arrow pointing in the other direction.  The four vector spaces across the bottom are familiar from the earliest days of the course, while the four vector spaces across the top are completely abstract.  The vector representations that are restrictions (far left and far right) are the functions shown to be invertible representations as the key technique in the proofs of <xref ref="theorem-KNSI" acro="KNSI"/> and <xref ref="theorem-RCSI" acro="RCSI"/>.  So this diagram could be helpful as you study those two proofs.</p>
        <figure xml:id="figure-KRI" acro="KRI">
            <caption>Kernel and Range Isomorphisms</caption>
            <image xml:id="image-KRI">
                <latex-image>
                \begin{tikzpicture}
                \matrix (m) [matrix of math nodes, row sep=2.5em, column sep=5em, text height=1.5ex, text depth=0.25ex]
                {                          \amp U           \amp V           \amp \\
                 \krn{T}                   \amp             \amp             \amp \rng{T} \\
                                           \amp \complex{n} \amp \complex{m} \amp \\
                 \nsp{\matrixrep{T}{B}{C}} \amp             \amp             \amp \csp{\matrixrep{T}{B}{C}} \\
                };
                % linear transformations
                \path[->]
                (m-1-2) edge[thick] node[auto]  {\(T\)}                    (m-1-3)
                (m-3-2) edge[thick] node[auto]  {\(\matrixrep{T}{B}{C}\)}  (m-3-3);
                % vector representations
                \path[->]
                (m-2-1) edge[thick] node[auto]  {\(\restrict{\vectrepname{B}}{\krn{T}}\)}  (m-4-1)
                (m-1-2) edge[thick] node[auto]  {\(\vectrepname{B}\)}  (m-3-2)
                (m-1-3) edge[thick] node[auto]  {\(\vectrepname{C}\)}  (m-3-3)
                (m-2-4) edge[thick] node[auto]  {\(\restrict{\vectrepname{C}}{\rng{T}}\)}  (m-4-4);
                % subsets
                \path[-]
                (m-1-2) edge[thick, densely dashed] (m-2-1)
                (m-1-3) edge[thick, densely dashed] (m-2-4)
                (m-3-2) edge[thick, densely dashed] (m-4-1)
                (m-3-3) edge[thick, densely dashed] (m-4-4);
                \end{tikzpicture}
                </latex-image>
            </image>
        </figure>
        <computation xml:id="sage-LTR" acro="LTR">
            <title>Linear Transformation Restrictions</title>
            <idx>
                <h>linear transformation</h>
                <h>restrictions</h>
            </idx>
            <p><xref ref="theorem-KNSI" acro="KNSI"/> and <xref ref="theorem-RCSI" acro="RCSI"/> have two of the most subtle proofs we have seen so far.  The conclusion that two vector spaces are isomorphic is established by actually constructing an isomorphism between the vector spaces.  To build the isomorphism, we begin with a familar object, a vector representation linear transformation, but the hard work is showing that we can <q>restrict</q> the domain and codomain of this function and still arrive at a legitimate (invertible) linear transformation.  In an effort to make the proofs more concrete, we will walk through a nontrivial example for <xref ref="theorem-KNSI" acro="KNSI"/>, and you might try to do the same for <xref ref="theorem-RCSI" acro="RCSI"/>.  (An understanding of this subsection is not needed for the remainder <mdash/> its second purpose is to demonstrate some of the powerful tools Sage provides.)</p>
            <p>Here are the pieces.  We build a linear transformation with two different representations, one with respect to standard bases, the other with respect to less-obvious bases.</p>
            <sage xml:id="sagecell-LTR-1">
                <input>
                x1, x2, x3, x4, x5 = var('x1, x2, x3, x4, x5')
                outputs = [ x1 - x2 - 5*x3 +   x4 +   x5,
                            x1      - 2*x3 -   x4 -   x5,
                               - x2 - 3*x3 + 2*x4 + 2*x5,
                           -x1 + x2 + 5*x3 -   x4 -   x5]
                T_symbolic(x1, x2, x3, x4, x5) = outputs
                b0 = vector(QQ, [-1,  6,  5,  5,  1])
                b1 = vector(QQ, [-1,  5,  4,  4,  1])
                b2 = vector(QQ, [-2,  4,  3,  2,  5])
                b3 = vector(QQ, [ 1, -1,  0,  1, -5])
                b4 = vector(QQ, [ 3, -7, -6, -5, -4])
                U = (QQ^5).subspace_with_basis([b0, b1, b2, b3, b4])
                c0 = vector(QQ, [1, 1, 1, -3])
                c1 = vector(QQ, [-2, 3, -6, -7])
                c2 = vector(QQ, [0, -1, 1, 2])
                c3 = vector(QQ, [-1, 3, -4, -7])
                V = (QQ^4).subspace_with_basis([c0, c1, c2, c3])
                T_plain = linear_transformation(QQ^5, QQ^4, T_symbolic)
                T_fancy = linear_transformation(   U,    V, T_symbolic)
                </input>
            </sage>
            <p>Now we compute the kernel of the linear transformation using the <q>plain</q> version, and the null space of a matrix representation coming from the <q>fancy</q> version.</p>
            <sage xml:id="sagecell-LTR-2">
                <input>
                K = T_plain.kernel()
                K
                </input>
                <output>
                Vector space of degree 5 and dimension 3 over Rational Field
                Basis matrix:
                [   1    0  2/7    0  3/7]
                [   0    1 -1/7    0  2/7]
                [   0    0    0    1   -1]
                </output>
            </sage>
            <sage xml:id="sagecell-LTR-3">
                <input>
                MK = T_fancy.matrix(side='right').right_kernel()
                MK
                </input>
                <output>
                Vector space of degree 5 and dimension 3 over Rational Field
                Basis matrix:
                [      1       0       0 -97/203 164/203]
                [      0       1       0  -10/29   19/29]
                [      0       0       1 129/203 100/203]
                </output>
            </sage>
            <p>So quite obviously, the kernel of the linear transformation is quite different looking from the null space of the matrix representation.  Though it is no accident that they have the same dimension.  Now we build the necessary vector representation, and use two Sage commands to <q>restrict</q> the function to a smaller domain (the kernel of the linear transformation) and a smaller codomain (the null space of the matrix representation relative to nonstandard bases).</p>
            <sage xml:id="sagecell-LTR-4">
                <input>
                rhoB = linear_transformation(U, QQ^5, (QQ^5).basis())
                rho_restrict = rhoB.restrict_domain(K).restrict_codomain(MK)
                rho_restrict
                </input>
                <output>
                Vector space morphism represented by the matrix:
                [ 33/7 -37/7 -11/7]
                [-13/7  22/7 -12/7]
                [   -4     5     6]
                Domain: Vector space of degree 5 and dimension 3 over Rational Field
                Basis matrix:
                [   1    0  2/7    0  3/7]
                [   0    1 -1/7    0  2/7]
                [   0    0    0    1   -1]
                Codomain: Vector space of degree 5 and dimension 3 over Rational Field
                Basis matrix:
                [      1       0       0 -97/203 164/203]
                [      0       1       0  -10/29   19/29]
                [      0       0       1 129/203 100/203]
                </output>
            </sage>
            <p>The first success is that the restriction was even created.  Sage would recognize if the original linear transformation ever carried an input from the restricted domain to an output that was not contained in the proposed codomain, and would have raised an error in that event.  Phew!  Guaranteeing this success was the first big step in the proof of <xref ref="theorem-KNSI" acro="KNSI"/>.  Notice that the matrix representation of the restriction is a <m>3\times 3</m> matrix, since the restriction runs between a domain and codomain that each have dimension 3.  These two vector spaces (the domain and codomain of the restriction) have dimension 3 but still contain vectors with 5 entries in their un-coordinatized versions.</p>
            <p>The next two steps of the proof show that the restriction is injective (easy in the proof) and surjective (hard in the proof).  In Sage, here is the second success,</p>
            <sage xml:id="sagecell-LTR-5">
                <input>
                rho_restrict.is_injective()
                </input>
                <output>
                True
                </output>
            </sage>
            <sage xml:id="sagecell-LTR-6">
                <input>
                rho_restrict.is_surjective()
                </input>
                <output>
                True
                </output>
            </sage>
            <p>Verified as invertible, <c>rho_restrict</c> qualifies as an isomorphism between the linear transformation kernel, <c>K</c>, and the matrix representation null space, <c>MK</c>.  Only an example, but still very nice.  Your turn <mdash/> can you create a verfication of <xref ref="theorem-RCSI" acro="RCSI"/> (for this example, or some other nontrivial example you might create yourself)?</p>
        </computation>
    </subsection>
    <subsection xml:id="subsection-MR-IVLT" acro="IVLT">
        <title>Invertible Linear Transformations</title>
        <p>We have seen, both in theorems and in examples, that questions about linear transformations are often equivalent to questions about matrices.  It is the matrix representation of a linear transformation that makes this idea precise.  Here is our final theorem that solidifies this connection.</p>
        <theorem xml:id="theorem-IMR" acro="IMR">
            <title>Invertible Matrix Representations</title>
            <idx>
                <h>matrix representation</h>
                <h>invertible</h>
            </idx>
            <statement>
                <p>Suppose that <m>\ltdefn{T}{U}{V}</m> is a linear transformation, <m>B</m> is a basis for <m>U</m> and <m>C</m> is a basis for <m>V</m>. Then <m>T</m> is an invertible linear transformation if and only if the matrix representation of <m>T</m> relative to <m>B</m> and <m>C</m>, <m>\matrixrep{T}{B}{C}</m> is an invertible matrix.  When <m>T</m> is invertible,<me>\matrixrep{\ltinverse{T}}{C}{B}=\inverse{\left(\matrixrep{T}{B}{C}\right)}</me>.</p>
            </statement>
            <proof>
                <case direction="forward">
                    <!-- MBX: move end of case -->
                </case>
                <p>  Suppose <m>T</m> is invertible, so the inverse linear transformation <m>\ltdefn{\ltinverse{T}}{V}{U}</m> exists (<xref ref="definition-IVLT" acro="IVLT"/>).  Both linear transformations have matrix representations relative to the bases of <m>U</m> and <m>V</m>, namely <m>\matrixrep{T}{B}{C}</m> and <m>\matrixrep{\ltinverse{T}}{C}{B}</m> (<xref ref="definition-MR" acro="MR"/>).  </p>
                <p>Then<md>
                    <mrow>\matrixrep{\ltinverse{T}}{C}{B}\matrixrep{T}{B}{C}
                    &amp;=\matrixrep{\compose{\ltinverse{T}}{T}}{B}{B}&amp;&amp;
                        <xref ref="theorem-MRCLT" acro="MRCLT"/></mrow>
                    <mrow>&amp;=\matrixrep{I_U}{B}{B}&amp;&amp;
                        <xref ref="definition-IVLT" acro="IVLT"/></mrow>
                    <mrow>&amp;=\left[
                    \left.\vectrep{B}{\lteval{I_U}{\vect{u}_1}}\right|
                    \left.\vectrep{B}{\lteval{I_U}{\vect{u}_2}}\right|
                    \ldots
                    \left|\vectrep{B}{\lteval{I_U}{\vect{u}_n}}\right.
                    \right]&amp;&amp;
                        <xref ref="definition-MR" acro="MR"/></mrow>
                    <mrow>&amp;=\left[
                    \left.\vectrep{B}{\vect{u}_1}\right|
                    \left.\vectrep{B}{\vect{u}_2}\right|
                    \left.\vectrep{B}{\vect{u}_3}\right|
                    \ldots
                    \left|\vectrep{B}{\vect{u}_n}\right.
                    \right]&amp;&amp;
                        <xref ref="definition-IDLT" acro="IDLT"/></mrow>
                    <mrow>&amp;=\matrixcolumns{e}{n}&amp;&amp;
                        <xref ref="definition-VR" acro="VR"/></mrow>
                    <mrow>&amp;=I_n&amp;&amp;
                        <xref ref="definition-IM" acro="IM"/></mrow>
                </md>.</p>
                <p>And<md>
                    <mrow>\matrixrep{T}{B}{C}\matrixrep{\ltinverse{T}}{C}{B}&amp;=
                    \matrixrep{\compose{T}{\ltinverse{T}}}{C}{C}&amp;&amp;
                        <xref ref="theorem-MRCLT" acro="MRCLT"/></mrow>
                    <mrow>&amp;=\matrixrep{I_V}{C}{C}&amp;&amp;
                        <xref ref="definition-IVLT" acro="IVLT"/></mrow>
                    <mrow>&amp;=\left[
                    \left.\vectrep{C}{\lteval{I_V}{\vect{v}_1}}\right|
                    \left.\vectrep{C}{\lteval{I_V}{\vect{v}_2}}\right|
                    \ldots
                    \left|\vectrep{C}{\lteval{I_V}{\vect{v}_n}}\right.
                    \right]&amp;&amp;
                        <xref ref="definition-MR" acro="MR"/></mrow>
                    <mrow>&amp;=\left[
                    \left.\vectrep{C}{\vect{v}_1}\right|
                    \left.\vectrep{C}{\vect{v}_2}\right|
                    \left.\vectrep{C}{\vect{v}_3}\right|
                    \ldots
                    \left|\vectrep{C}{\vect{v}_n}\right.
                    \right]&amp;&amp;
                        <xref ref="definition-IDLT" acro="IDLT"/></mrow>
                    <mrow>&amp;=\matrixcolumns{e}{n}&amp;&amp;
                        <xref ref="definition-VR" acro="VR"/></mrow>
                    <mrow>&amp;=I_n&amp;&amp;
                        <xref ref="definition-IM" acro="IM"/></mrow>
                </md>.</p>
                <p>These two equations show that <m>\matrixrep{T}{B}{C}</m> and <m>\matrixrep{\ltinverse{T}}{C}{B}</m> are inverse matrices (<xref ref="definition-MI" acro="MI"/>) and establish that when <m>T</m> is invertible, then <m>\matrixrep{\ltinverse{T}}{C}{B}=\inverse{\left(\matrixrep{T}{B}{C}\right)}</m>.</p>
                <case direction="backward">
                    <!-- MBX: move end of case -->
                </case>
                <p>Suppose now that <m>\matrixrep{T}{B}{C}</m> is an invertible matrix and hence nonsingular (<xref ref="theorem-NI" acro="NI"/>).  We compute the nullity of <m>T</m>,<md>
                    <mrow>\nullity{T}
                    &amp;=\dimension{\krn{T}}&amp;&amp;
                        <xref ref="definition-KLT" acro="KLT"/></mrow>
                    <mrow>&amp;=\dimension{\nsp{\matrixrep{T}{B}{C}}}&amp;&amp;
                        <xref ref="theorem-KNSI" acro="KNSI"/></mrow>
                    <mrow>&amp;=\nullity{\matrixrep{T}{B}{C}}&amp;&amp;
                        <xref ref="definition-NOM" acro="NOM"/></mrow>
                    <mrow>&amp;=0&amp;&amp;
                        <xref ref="theorem-RNNM" acro="RNNM"/></mrow>
                </md>.</p>
                <p>So the kernel of <m>T</m> is trivial, and by <xref ref="theorem-KILT" acro="KILT"/>, <m>T</m> is injective.</p>
                <p>We now compute the rank of <m>T</m>,<md>
                    <mrow>\rank{T}
                    &amp;=\dimension{\rng{T}}&amp;&amp;
                        <xref ref="definition-RLT" acro="RLT"/></mrow>
                    <mrow>&amp;=\dimension{\csp{\matrixrep{T}{B}{C}}}&amp;&amp;
                        <xref ref="theorem-RCSI" acro="RCSI"/></mrow>
                    <mrow>&amp;=\rank{\matrixrep{T}{B}{C}}&amp;&amp;
                        <xref ref="definition-ROM" acro="ROM"/></mrow>
                    <mrow>&amp;=\dimension{V}&amp;&amp;
                        <xref ref="theorem-RNNM" acro="RNNM"/></mrow>
                </md>.</p>
                <p>Since the dimension of the range of <m>T</m> equals the dimension of the codomain <m>V</m>, by <xref ref="theorem-EDYES" acro="EDYES"/>, <m>\rng{T}=V</m>.  Which says that <m>T</m> is surjective by <xref ref="theorem-RSLT" acro="RSLT"/>.</p>
                <p>Because <m>T</m> is both injective and surjective, by <xref ref="theorem-ILTIS" acro="ILTIS"/>, <m>T</m> is invertible.</p>
            </proof>
        </theorem>
        <p>By now, the connections between matrices and linear transformations should be starting to become more transparent, and you may have already recognized the invertibility of a matrix as being  tantamount to the invertibility of the associated matrix representation.  The next example shows how to apply this theorem to the problem of actually building a formula for the inverse of an invertible linear transformation.</p>
        <example xml:id="example-ILTVR" acro="ILTVR">
            <title>Inverse of a linear transformation via a representation</title>
            <idx>
                <h>linear transformation inverse</h>
                <h>via matrix representation</h>
            </idx>
            <p>Consider the linear transformation<me>\ltdefn{R}{P_3}{M_{22}},\quad
            \lteval{R}{a+bx+cx^2+x^3}=
            \begin{bmatrix}
            a +b - c + 2d &amp; 2a + 3b - 2c + 3d\\
            a + b + 2 d &amp; -a + b + 2c - 5d
            \end{bmatrix}</me>.</p>
            <p>If we wish to quickly find a formula for the inverse of <m>R</m> (presuming it exists), then choosing <q>nice</q> bases will work best.  So build a matrix representation of <m>R</m> relative to the bases <m>B</m> and <m>C</m>,<md>
                <mrow>B&amp;=\set{1,\,x,\,x^2,\,x^3}</mrow>
                <mrow>C&amp;=\set{
                \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 0 \end{bmatrix},\,
                \begin{bmatrix} 0 &amp; 1 \\ 0 &amp; 0 \end{bmatrix},\,
                \begin{bmatrix} 0 &amp; 0 \\ 1 &amp; 0 \end{bmatrix},\,
                \begin{bmatrix} 0 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}
                }</mrow>
            </md>.</p>
            <p>Then,<md>
                <mrow>\vectrep{C}{\lteval{R}{1}}
                &amp;=\vectrep{C}{\begin{bmatrix} 1 &amp; 2 \\ 1 &amp; -1 \end{bmatrix}}
                =\colvector{1\\2\\1\\-1}</mrow>
                <mrow>\vectrep{C}{\lteval{R}{x}}
                &amp;=\vectrep{C}{\begin{bmatrix} 1 &amp; 3 \\ 1 &amp; 1 \end{bmatrix}}
                =\colvector{1\\3\\1\\1}</mrow>
                <mrow>\vectrep{C}{\lteval{R}{x^2}}
                &amp;=\vectrep{C}{\begin{bmatrix} -1 &amp; -2 \\ 0 &amp; 2 \end{bmatrix}}
                =\colvector{-1\\-2\\0\\2}</mrow>
                <mrow>\vectrep{C}{\lteval{R}{x^3}}
                &amp;=\vectrep{C}{\begin{bmatrix} 2 &amp; 3 \\ 2 &amp; -5 \end{bmatrix}}
                =\colvector{2\\3\\2\\-5}</mrow>
            </md>.</p>
            <p>So a representation of <m>R</m> is<me>\matrixrep{R}{B}{C}=
            \begin{bmatrix}
             1 &amp; 1 &amp; -1 &amp; 2 \\
             2 &amp; 3 &amp; -2 &amp; 3 \\
             1 &amp; 1 &amp; 0 &amp; 2 \\
             -1 &amp; 1 &amp; 2 &amp; -5
            \end{bmatrix}</me>.</p>
            <p>The matrix <m>\matrixrep{R}{B}{C}</m> is invertible (as you can check) so we know for sure that <m>R</m> is invertible by <xref ref="theorem-IMR" acro="IMR"/>.  Furthermore,<me>\matrixrep{\ltinverse{R}}{C}{B}
            =\inverse{\left(\matrixrep{R}{B}{C}\right)}
            =
            \inverse{
            \begin{bmatrix}
             1 &amp; 1 &amp; -1 &amp; 2 \\
             2 &amp; 3 &amp; -2 &amp; 3 \\
             1 &amp; 1 &amp; 0 &amp; 2 \\
             -1 &amp; 1 &amp; 2 &amp; -5
            \end{bmatrix}
            }
            =
            \begin{bmatrix}
            20 &amp; -7 &amp; -2 &amp; 3 \\
            -8 &amp; 3 &amp; 1 &amp; -1 \\
            -1 &amp; 0 &amp; 1 &amp; 0 \\
            -6 &amp; 2 &amp; 1 &amp; -1
            \end{bmatrix}</me>.</p>
            <p>We can use this representation of the inverse linear transformation, in concert with <xref ref="theorem-FTMR" acro="FTMR"/>, to determine an explicit formula for the inverse itself,<md>
                <mrow>\lteval{\ltinverse{R}}{\begin{bmatrix}a&amp;b\\c&amp;d\end{bmatrix}}
                &amp;=\vectrepinv{B}{\matrixrep{\ltinverse{R}}{C}{B}\vectrep{C}{\begin{bmatrix}a&amp;b\\c&amp;d\end{bmatrix}}}&amp;&amp;
                    <xref ref="theorem-FTMR" acro="FTMR"/></mrow>
                <mrow>&amp;=\vectrepinv{B}{\inverse{\left(\matrixrep{R}{B}{C}\right)}\vectrep{C}{\begin{bmatrix}a&amp;b\\c&amp;d\end{bmatrix}}}&amp;&amp;
                    <xref ref="theorem-IMR" acro="IMR"/></mrow>
                <mrow>&amp;=\vectrepinv{B}{\inverse{\left(\matrixrep{R}{B}{C}\right)}\colvector{a\\b\\c\\d}}&amp;&amp;
                    <xref ref="definition-VR" acro="VR"/></mrow>
                <mrow>&amp;=\vectrepinv{B}{
                \begin{bmatrix}
                20 &amp; -7 &amp; -2 &amp; 3 \\
                -8 &amp; 3 &amp; 1 &amp; -1 \\
                -1 &amp; 0 &amp; 1 &amp; 0 \\
                -6 &amp; 2 &amp; 1 &amp; -1
                \end{bmatrix}
                \colvector{a\\b\\c\\d}}&amp;&amp;
                    <xref ref="definition-MI" acro="MI"/></mrow>
                <mrow>&amp;=\vectrepinv{B}{
                \colvector{
                20a - 7b - 2c + 3d</mrow>
                <mrow>-8a + 3b + c -d</mrow>
                <mrow>-a + c</mrow>
                <mrow>-6a + 2b + c - d}}&amp;&amp;
                    <xref ref="definition-MVP" acro="MVP"/></mrow>
                <mrow>&amp;=(20a - 7b - 2c + 3d)+(-8a + 3b + c -d)x</mrow>
                <mrow>&amp;\quad\quad +(-a + c)x^2+(-6a + 2b + c - d)x^3&amp;&amp;
                    <xref ref="definition-VR" acro="VR"/></mrow>
            </md>.</p>
        </example>
        <p>You might look back at <xref ref="example-AIVLT" acro="AIVLT"/>, where we first witnessed the inverse of a linear transformation and recognize that the inverse (<m>S</m>) was built from using the method of <xref ref="example-ILTVR" acro="ILTVR"/> with a matrix representation of <m>T</m>.</p>
        <theorem xml:id="theorem-IMILT" acro="IMILT">
            <title>Invertible Matrices, Invertible Linear Transformation</title>
            <idx>
                <h>invertible linear transformation</h>
                <h>defined by invertible matrix</h>
            </idx>
            <statement>
                <p>Suppose that <m>A</m> is a square matrix of size <m>n</m> and <m>\ltdefn{T}{\complex{n}}{\complex{n}}</m> is the linear transformation defined by <m>\lteval{T}{\vect{x}}=A\vect{x}</m>.  Then <m>A</m> is an invertible matrix if and only if <m>T</m> is an invertible linear transformation.</p>
            </statement>
            <proof>
                <p>Choose bases <m>B=C=\set{\vectorlist{e}{n}}</m> consisting of the standard unit vectors as a basis of <m>\complex{n}</m> (<xref ref="theorem-SUVB" acro="SUVB"/>) and build a matrix representation of <m>T</m> relative to <m>B</m> and <m>C</m>.  Then<md>
                    <mrow>\vectrep{C}{\lteval{T}{\vect{e}_i}}
                    &amp;=\vectrep{C}{A\vect{e}_i}</mrow>
                    <mrow>&amp;=\vectrep{C}{\vect{A}_i}</mrow>
                    <mrow>&amp;=\vect{A}_i</mrow>
                </md>.</p>
                <p>So then the matrix representation of <m>T</m>, relative to <m>B</m> and <m>C</m>, is simply <m>\matrixrep{T}{B}{C}=A</m>.  With this observation, the proof becomes a specialization of <xref ref="theorem-IMR" acro="IMR"/>,<md>
                    <mrow>T\text{ is invertible}
                    \iff\matrixrep{T}{B}{C}\text{ is invertible}
                    \iff A\text{ is invertible}</mrow>
                </md>.</p>
            </proof>
        </theorem>
        <p>This theorem may seem gratuitous.  Why state such a special case of <xref ref="theorem-IMR" acro="IMR"/>?  Because it adds another condition to our NMEx series of theorems, and in some ways it is the most fundamental expression of what it means for a matrix to be nonsingular <mdash/> the associated linear transformation is invertible.  This is our final update.</p>
        <theorem xml:id="theorem-NME9" acro="NME9">
            <title>Nonsingular Matrix Equivalences, Round 9</title>
            <idx>
                <h>nonsingular matrix</h>
                <h>equivalences</h>
            </idx>
            <statement>
                <p>Suppose that <m>A</m> is a square matrix of size <m>n</m>.  The following are equivalent.<ol>
                    <li><m>A</m> is nonsingular.</li>
                    <li><m>A</m> row-reduces to the identity matrix.</li>
                    <li>The null space of <m>A</m> contains only the zero vector, <m>\nsp{A}=\set{\zerovector}</m>.</li>
                    <li>The linear system <m>\linearsystem{A}{\vect{b}}</m> has a unique solution for every possible choice of <m>\vect{b}</m>.</li>
                    <li>The columns of <m>A</m> are a linearly independent set.</li>
                    <li><m>A</m> is invertible.</li>
                    <li>The column space of <m>A</m> is <m>\complex{n}</m>, <m>\csp{A}=\complex{n}</m>.</li>
                    <li>The columns of <m>A</m> are a basis for <m>\complex{n}</m>.</li>
                    <li>The rank of <m>A</m> is <m>n</m>, <m>\rank{A}=n</m>.</li>
                    <li>The nullity of <m>A</m> is zero, <m>\nullity{A}=0</m>.</li>
                    <li>The determinant of <m>A</m> is nonzero, <m>\detname{A}\neq 0</m>.</li>
                    <li><m>\lambda=0</m> is not an eigenvalue of <m>A</m>.</li>
                    <li>The linear transformation <m>\ltdefn{T}{\complex{n}}{\complex{n}}</m> defined by <m>\lteval{T}{\vect{x}}=A\vect{x}</m> is invertible.</li>
                </ol></p>
            </statement>
            <proof>
                <p>By <xref ref="theorem-IMILT" acro="IMILT"/>, the new addition to this list is equivalent to the statement that <m>A</m> is invertible, so we can expand <xref ref="theorem-NME8" acro="NME8"/>.</p>
            </proof>
        </theorem>
        <computation xml:id="sage-NME9" acro="NME9">
            <title>Nonsingular Matrix Equivalences, Round 9</title>
            <idx>nonsingular matrix equivalences, round 9</idx>
            <p>Our final fact about nonsingular matrices expresses the correspondence between invertible matrices and invertible linear transformations.  As a Sage demonstration, we will begin with an invertible linear transformation and examine two matrix representations.  We will create the linear transformation with nonstandard bases and then compute its representation relative to standard bases.</p>
            <sage xml:id="sagecell-NME9-1">
                <input>
                x1, x2, x3, x4 = var('x1, x2, x3, x4')
                outputs = [ x1        - 2*x3 - 4*x4,
                                   x2 -   x3 - 5*x4,
                           -x1 - 2*x2 + 2*x3 + 7*x4,
                               -   x2        +   x4]
                T_symbolic(x1, x2, x3, x4) = outputs
                b0 = vector(QQ, [ 1, -2, -1,  8])
                b1 = vector(QQ, [ 0,  1,  0, -2])
                b2 = vector(QQ, [-1, -2,  2, -5])
                b3 = vector(QQ, [-1, -3,  2, -2])
                U = (QQ^4).subspace_with_basis([b0, b1, b2, b3])
                c0 = vector(QQ, [ 3, -1,  4, -8])
                c1 = vector(QQ, [ 1,  0,  1, -1])
                c2 = vector(QQ, [ 0,  2, -1,  6])
                c3 = vector(QQ, [-1,  2, -2,  8])
                V = (QQ^4).subspace_with_basis([c0, c1, c2, c3])
                T = linear_transformation(U, V, T_symbolic)
                T
                </input>
                <output>
                Vector space morphism represented by the matrix:
                [ 131  -56 -321  366]
                [ -37   17   89 -102]
                [ -61   25  153 -173]
                [  -7   -1   24  -25]
                Domain: Vector space of degree 4 and dimension 4 over Rational Field
                User basis matrix:
                [ 1 -2 -1  8]
                [ 0  1  0 -2]
                [-1 -2  2 -5]
                [-1 -3  2 -2]
                Codomain: Vector space of degree 4 and dimension 4 over Rational Field
                User basis matrix:
                [ 3 -1  4 -8]
                [ 1  0  1 -1]
                [ 0  2 -1  6]
                [-1  2 -2  8]
                </output>
            </sage>
            <sage xml:id="sagecell-NME9-2">
                <input>
                T.is_invertible()
                </input>
                <output>
                True
                </output>
            </sage>
            <sage xml:id="sagecell-NME9-3">
                <input>
                T.matrix(side='right').is_invertible()
                </input>
                <output>
                True
                </output>
            </sage>
            <sage xml:id="sagecell-NME9-4">
                <input>
                (T^-1).matrix(side='right') == (T.matrix(side='right'))^-1
                </input>
                <output>
                True
                </output>
            </sage>
            <p>We can convert <c>T</c> to a new representation using standard bases for <c>QQ^4</c> by computing images of the standard basis.</p>
            <sage xml:id="sagecell-NME9-5">
                <input>
                images = [T(u) for u in (QQ^4).basis()]
                T_standard = linear_transformation(QQ^4, QQ^4, images)
                T_standard
                </input>
                <output>
                Vector space morphism represented by the matrix:
                [ 1  0 -1  0]
                [ 0  1 -2 -1]
                [-2 -1  2  0]
                [-4 -5  7  1]
                Domain: Vector space of dimension 4 over Rational Field
                Codomain: Vector space of dimension 4 over Rational Field
                </output>
            </sage>
            <sage xml:id="sagecell-NME9-6">
                <input>
                T_standard.matrix(side='right').is_invertible()
                </input>
                <output>
                True
                </output>
            </sage>
            <p>Understand that <em>any</em> matrix representation of <c>T_symbolic</c> will have an invertible matrix representation, no matter which bases are used.  If you look at the matrix representation of <c>T_standard</c> and the definition of <c>T_symbolic</c> the construction of this example will be transparent, especially if you know the random matrix constructor,</p>
            <sage xml:id="sagecell-NME9-7">
                <input>
                A = random_matrix(QQ, 4, algorithm='unimodular', upper_bound=9)
                A                                                      # random
                </input>
                <output>
                [-1 -1  2  1]
                [ 1  1 -1  0]
                [-2 -1  5  6]
                [ 1  0 -4 -5]
                </output>
            </sage>
        </computation>
    </subsection>
    <exercises xml:id="readingquestions-MR">
        <title>Reading Questions</title>
        <exercise xml:id="reading-MR-1">
            <statement>
                <p>Why does <xref ref="theorem-FTMR" acro="FTMR"/> deserve the moniker <q>fundamental</q>?</p>
            </statement>
        </exercise>
        <exercise xml:id="reading-MR-2">
            <statement>
                <p>Find the matrix representation, <m>\matrixrep{T}{B}{C}</m> of the linear transformation<me>\ltdefn{T}{\complex{2}}{\complex{2}},\quad\lteval{T}{\colvector{x_1\\x_2}}=\colvector{2x_1-x_2\\3x_1+2x_2}</me>relative to the bases<md>
                    <mrow>B&amp;=\set{\colvector{2\\3},\,\colvector{-1\\2}}&amp;
                    C&amp;=\set{\colvector{1\\0},\,\colvector{1\\1}}</mrow>
                </md>.</p>
            </statement>
        </exercise>
        <exercise xml:id="reading-MR-3">
            <statement>
                <p>What is the second <q>surprise,</q> and why is it surprising?</p>
            </statement>
        </exercise>
    </exercises>
    <exercises xml:id="exercises-MR">
        <title>Exercises</title>
        <exercise number="C10" xml:id="exercise-MR-C10">
            <statement>
                <p><xref ref="example-KVMR" acro="KVMR"/> concludes with a basis for the kernel of the linear transformation <m>T</m>.  Compute the value of <m>T</m> for each of these two basis vectors.  Did you get what you expected?</p>
            </statement>
        </exercise>
        <exercise number="C20" xml:id="exercise-MR-C20">
            <statement>
                <!-- MBX: was a gather-->
                <p>Compute the matrix representation of <m>T</m> relative to the bases <m>B</m> and <m>C</m>,<md alignment="gather">
                    <mrow>\ltdefn{T}{P_3}{\complex{3}},\quad \lteval{T}{a+bx+cx^2+dx^3}=
                    \colvector{2a-3b+4c-2d\\a+b-c+d\\3a+2c-3d}</mrow>
                    <mrow>B=\set{1,\,x,\,x^2,\,x^3}\quad\quad
                    C=\set{\colvector{1\\0\\0},\,\colvector{1\\1\\0},\,\colvector{1\\1\\1}}</mrow>
                </md>.</p>
            </statement>
            <solution xml:id="solution-MR-C20">
                <p>Apply <xref ref="definition-MR" acro="MR"/>,<md>
                    <mrow>\vectrep{C}{\lteval{T}{1}}&amp;=\vectrep{C}{\colvector{2\\1\\3}}
                    =\vectrep{C}{1\colvector{1\\0\\0}+(-2)\colvector{1\\1\\0}+3\colvector{1\\1\\1}}=\colvector{1\\-2\\3}</mrow>
                    <mrow>\vectrep{C}{\lteval{T}{x}}&amp;=\vectrep{C}{\colvector{-3\\1\\0}}
                    =\vectrep{C}{(-4)\colvector{1\\0\\0}+1\colvector{1\\1\\0}+0\colvector{1\\1\\1}}=\colvector{-4\\1\\0}</mrow>
                    <mrow>\vectrep{C}{\lteval{T}{x^2}}&amp;=\vectrep{C}{\colvector{4\\-1\\2}}
                    =\vectrep{C}{5\colvector{1\\0\\0}+(-3)\colvector{1\\1\\0}+2\colvector{1\\1\\1}}=\colvector{5\\-3\\2}</mrow>
                    <mrow>\vectrep{C}{\lteval{T}{x^3}}&amp;=\vectrep{C}{\colvector{-2\\1\\-3}}
                    =\vectrep{C}{(-3)\colvector{1\\0\\0}+4\colvector{1\\1\\0}+(-3)\colvector{1\\1\\1}}=\colvector{-3\\4\\-3}</mrow>
                </md>.  These four vectors are the columns of the matrix representation,<me>\matrixrep{T}{B}{C}=
                \begin{bmatrix}
                1 &amp; -4 &amp; 5 &amp; -3\\
                -2 &amp; 1 &amp; -3 &amp; 4\\
                3 &amp; 0 &amp; 2 &amp; -3
                \end{bmatrix}</me>.</p>
            </solution>
        </exercise>
        <exercise number="C21" xml:id="exercise-MR-C21">
            <statement>
                <p>Find a matrix representation of the linear transformation <m>T</m> relative to the bases <m>B</m> and <m>C</m>,<md>
                    <mrow>&amp;\ltdefn{T}{P_2}{\complex{2}},\quad \lteval{T}{p(x)}=\colvector{p(1)\\p(3)}</mrow>
                    <mrow>&amp;B=\set{2-5x+x^2,\,1+x-x^2,\,x^2}</mrow>
                    <mrow>&amp;C=\set{\colvector{3\\4},\,\colvector{2\\3}}</mrow>
                </md>.</p>
            </statement>
            <solution xml:id="solution-MR-C21">
                <p>Applying <xref ref="definition-MR" acro="MR"/>,<md>
                <mrow>\vectrep{C}{\lteval{T}{2-5x+x^2}}&amp;=\vectrep{C}{\colvector{-2\\-4}}
                =\vectrep{C}{2\colvector{3\\4}+(-4)\colvector{2\\3}}=\colvector{2\\-4}</mrow>
                <mrow>\vectrep{C}{\lteval{T}{1+x-x^2}}&amp;=\vectrep{C}{\colvector{1\\-5}}
                =\vectrep{C}{13\colvector{3\\4}+(-19)\colvector{2\\3}}=\colvector{13\\-19}</mrow>
                <mrow>\vectrep{C}{\lteval{T}{x^2}}&amp;=\vectrep{C}{\colvector{1\\9}}
                =\vectrep{C}{(-15)\colvector{3\\4}+23\colvector{2\\3}}=\colvector{-15\\23}</mrow>
            </md>.  So the resulting matrix representation is<md>
                <mrow>\matrixrep{T}{B}{C}
                &amp;=
                \begin{bmatrix}
                 2 &amp; 13 &amp; -15 \\
                 -4 &amp; -19 &amp; 23
                \end{bmatrix}</mrow>
            </md>.</p>
            </solution>
        </exercise>
        <exercise number="C22" xml:id="exercise-MR-C22">
            <statement>
                <p>Let <m>S_{22}</m> be the vector space of <m>2\times 2</m> symmetric matrices.  Build the matrix representation of the linear transformation <m>\ltdefn{T}{P_2}{S_{22}}</m> relative to the bases <m>B</m> and <m>C</m> and then use this matrix representation to compute <m>\lteval{T}{3+5x-2x^2}</m>,<md>
                    <mrow>B&amp;=\set{1,\,1+x,\,1+x+x^2}
                    &amp;
                    C&amp;=\set{
                    \begin{bmatrix}1 &amp; 0 \\ 0 &amp; 0\end{bmatrix},\,
                    \begin{bmatrix}0 &amp; 1 \\ 1 &amp; 0\end{bmatrix},\,
                    \begin{bmatrix}0 &amp; 0 \\ 0 &amp; 1\end{bmatrix}
                    }</mrow>
                    <mrow>\lteval{T}{a+bx+cx^2}&amp;=
                    \begin{bmatrix}
                    2a-b+c &amp; a+3b-c \\ a+3b-c &amp; a-c
                    \end{bmatrix}</mrow>
                </md>.</p>
            </statement>
            <solution xml:id="solution-MR-C22">
                <p>Input to <m>T</m> the vectors of the basis <m>B</m> and coordinatize the outputs relative to <m>C</m>,<md>
                    <mrow>\vectrep{C}{\lteval{T}{1}}&amp;=
                    \vectrep{C}{
                    \begin{bmatrix}2 &amp; 1 \\ 1 &amp; 1\end{bmatrix}
                    }
                    =
                    \vectrep{C}{
                    2\begin{bmatrix}1 &amp; 0 \\ 0 &amp; 0\end{bmatrix}+
                    1\begin{bmatrix}0 &amp; 1 \\ 1 &amp; 0\end{bmatrix}+
                    1\begin{bmatrix}0 &amp; 0 \\ 0 &amp; 1\end{bmatrix}
                    }
                    =
                    \colvector{2\\1\\1}</mrow>
                    <mrow>\vectrep{C}{\lteval{T}{1+x}}&amp;=
                    \vectrep{C}{
                    \begin{bmatrix}1 &amp; 4 \\ 4 &amp; 1\end{bmatrix}
                    }
                    =
                    \vectrep{C}{
                    1\begin{bmatrix}1 &amp; 0 \\ 0 &amp; 0\end{bmatrix}+
                    4\begin{bmatrix}0 &amp; 1 \\ 1 &amp; 0\end{bmatrix}+
                    1\begin{bmatrix}0 &amp; 0 \\ 0 &amp; 1\end{bmatrix}
                    }
                    =
                    \colvector{1\\4\\1}</mrow>
                    <mrow>\vectrep{C}{\lteval{T}{1+x+x^2}}&amp;=
                    \vectrep{C}{
                    \begin{bmatrix}2 &amp; 3 \\ 3 &amp; 0\end{bmatrix}
                    }
                    =
                    \vectrep{C}{
                    2\begin{bmatrix}1 &amp; 0 \\ 0 &amp; 0\end{bmatrix}+
                    3\begin{bmatrix}0 &amp; 1 \\ 1 &amp; 0\end{bmatrix}+
                    0\begin{bmatrix}0 &amp; 0 \\ 0 &amp; 1\end{bmatrix}
                    }
                    =
                    \colvector{2\\3\\0}</mrow>
                </md>.  Applying <xref ref="definition-MR" acro="MR"/> we have the matrix representation<me>\matrixrep{T}{B}{C}=
                \begin{bmatrix}2 &amp; 1  &amp; 2 \\ 1 &amp; 4 &amp; 3 \\ 1 &amp; 1 &amp; 0 \end{bmatrix}</me>.  To compute <m>\lteval{T}{3+5x-2x^2}</m> employ <xref ref="theorem-FTMR" acro="FTMR"/>,<md>
                    <mrow>\lteval{T}{3+5x-2x^2}&amp;=
                    \vectrepinv{C}{\matrixrep{T}{B}{C}\vectrep{B}{3+5x-2x^2}}</mrow>
                    <mrow>&amp;=\vectrepinv{C}{\matrixrep{T}{B}{C}\vectrep{B}{(-2)(1)+7(1+x)+(-2)(1+x+x^2)}}</mrow>
                    <mrow>&amp;=\vectrepinv{C}{
                    \begin{bmatrix}2&amp;1&amp;2\\1&amp;4&amp;3\\1&amp;1&amp;0\end{bmatrix}
                    \colvector{-2\\7\\-2}
                    }</mrow>
                    <mrow>&amp;=\vectrepinv{C}{\colvector{-1\\20\\5}}</mrow>
                    <mrow>&amp;=
                    (-1)\begin{bmatrix}1 &amp; 0 \\ 0 &amp; 0\end{bmatrix}+
                    20\begin{bmatrix}0 &amp; 1 \\ 1 &amp; 0\end{bmatrix}+
                    5\begin{bmatrix}0 &amp; 0 \\ 0 &amp; 1\end{bmatrix}</mrow>
                    <mrow>&amp;=
                    \begin{bmatrix}-1 &amp; 20 \\ 20 &amp; 5\end{bmatrix}</mrow>
                </md>.  You can, of course, check your answer by evaluating <m>\lteval{T}{3+5x-2x^2}</m> directly.</p>
            </solution>
        </exercise>
        <exercise number="C25" xml:id="exercise-MR-C25">
            <statement>
                <p>Use a matrix representation to determine if the linear transformation <m>\ltdefn{T}{P_3}{M_{22}}</m> is surjective.<me>\lteval{T}{a+bx+cx^2+dx^3}=
                \begin{bmatrix}
                -a+4b+c+2d &amp; 4a-b+6c-d\\
                a+5b-2c+2d &amp; a+2c+5d
                \end{bmatrix}</me></p>
            </statement>
            <solution xml:id="solution-MR-C25">
                <p>Choose bases <m>B</m> and <m>C</m> for the matrix representation,<md>
                    <mrow>B&amp;=\set{1,\,x,\,x^2,\,x^3}
                    &amp;
                    C&amp;=\set{
                    \begin{bmatrix}1 &amp; 0 \\ 0 &amp; 0\end{bmatrix},\,
                    \begin{bmatrix}0 &amp; 1 \\ 0 &amp; 0\end{bmatrix},\,
                    \begin{bmatrix}0 &amp; 0 \\ 1 &amp; 0\end{bmatrix},\,
                    \begin{bmatrix}0 &amp; 0 \\ 0 &amp; 1\end{bmatrix}
                    }</mrow>
                </md>.  Input to <m>T</m> the vectors of the basis <m>B</m> and coordinatize the outputs relative to <m>C</m>,<md>
                    <mrow>\vectrep{C}{\lteval{T}{1}}&amp;=
                    \vectrep{C}{\begin{bmatrix}-1 &amp; 4 \\ 1 &amp; 1\end{bmatrix}}
                    =
                    \vectrep{C}{
                    (-1)\begin{bmatrix}1 &amp; 0 \\ 0 &amp; 0\end{bmatrix}+
                    4\begin{bmatrix}0 &amp; 1 \\ 0 &amp; 0\end{bmatrix}+
                    1\begin{bmatrix}0 &amp; 0 \\ 1 &amp; 0\end{bmatrix}+
                    1\begin{bmatrix}0 &amp; 0 \\ 0 &amp; 1\end{bmatrix}
                    }
                    =
                    \colvector{-1\\4\\1\\1}</mrow>
                    <mrow>\vectrep{C}{\lteval{T}{x}}&amp;=
                    \vectrep{C}{\begin{bmatrix}4 &amp; -1 \\ 5 &amp; 0\end{bmatrix}}
                    =
                    \vectrep{C}{
                    4\begin{bmatrix}1 &amp; 0 \\ 0 &amp; 0\end{bmatrix}+
                    (-1)\begin{bmatrix}0 &amp; 1 \\ 0 &amp; 0\end{bmatrix}+
                    5\begin{bmatrix}0 &amp; 0 \\ 1 &amp; 0\end{bmatrix}+
                    0\begin{bmatrix}0 &amp; 0 \\ 0 &amp; 1\end{bmatrix}
                    }
                    =
                    \colvector{4\\-1\\5\\0}</mrow>
                    <mrow>\vectrep{C}{\lteval{T}{x^2}}&amp;=
                    \vectrep{C}{\begin{bmatrix}1 &amp; 6 \\ -2 &amp; 2\end{bmatrix}}
                    =
                    \vectrep{C}{
                    1\begin{bmatrix}1 &amp; 0 \\ 0 &amp; 0\end{bmatrix}+
                    6\begin{bmatrix}0 &amp; 1 \\ 0 &amp; 0\end{bmatrix}+
                    (-2)\begin{bmatrix}0 &amp; 0 \\ 1 &amp; 0\end{bmatrix}+
                    2\begin{bmatrix}0 &amp; 0 \\ 0 &amp; 1\end{bmatrix}
                    }
                    =
                    \colvector{1\\6\\-2\\2}</mrow>
                    <mrow>\vectrep{C}{\lteval{T}{x^3}}&amp;=
                    \vectrep{C}{\begin{bmatrix}2 &amp; -1 \\ 2 &amp; 5\end{bmatrix}}
                    =
                    \vectrep{C}{
                    2\begin{bmatrix}1 &amp; 0 \\ 0 &amp; 0\end{bmatrix}+
                    (-1)\begin{bmatrix}0 &amp; 1 \\ 0 &amp; 0\end{bmatrix}+
                    2\begin{bmatrix}0 &amp; 0 \\ 1 &amp; 0\end{bmatrix}+
                    5\begin{bmatrix}0 &amp; 0 \\ 0 &amp; 1\end{bmatrix}
                    }
                    =
                    \colvector{2\\-1\\2\\5}</mrow>
                </md>.  Applying <xref ref="definition-MR" acro="MR"/> we have the matrix representation<me>\matrixrep{T}{B}{C}=
                    \begin{bmatrix}
                    -1 &amp; 4 &amp; 1 &amp; 2 \\
                     4 &amp; -1 &amp; 6 &amp; -1 \\
                     1 &amp; 5 &amp; -2 &amp; 2 \\
                     1 &amp; 0 &amp; 2 &amp; 5
                    \end{bmatrix}</me>.  Properties of this matrix representation will translate to properties of the linear transformation The matrix representation is nonsingular since it row-reduces to the identity matrix (<xref ref="theorem-NMRRI" acro="NMRRI"/>) and therefore has a column space equal to <m>\complex{4}</m> (<xref ref="theorem-CNMB" acro="CNMB"/>).  The column space of the matrix representation is isomorphic to the range of the linear transformation (<xref ref="theorem-RCSI" acro="RCSI"/>).  So the range of <m>T</m> has dimension 4, equal to the dimension of the codomain <m>M_{22}</m>.  By <xref ref="theorem-ROSLT" acro="ROSLT"/>, <m>T</m> is surjective.</p>
            </solution>
        </exercise>
        <exercise number="C30" xml:id="exercise-MR-C30">
            <statement>
                <p>Find bases for the kernel and range of the linear transformation <m>S</m> below.<me>\ltdefn{S}{M_{22}}{P_2},\quad\lteval{S}{\begin{bmatrix}a&amp;b\\c&amp;d\end{bmatrix}}=
                (a+2b+5c-4d)+(3a-b+8c+2d)x+(a+b+4c-2d)x^2</me></p>
            </statement>
            <solution xml:id="solution-MR-C30">
                <p>These subspaces will be easiest to construct by analyzing a matrix representation of <m>S</m>.  Since we can use any matrix representation, we might as well use natural bases that allow us to construct the matrix representation quickly and easily,<md>
                <mrow>B&amp;=\set{
                \begin{bmatrix}1&amp;0\\0&amp;0\end{bmatrix},\,
                \begin{bmatrix}0&amp;1\\0&amp;0\end{bmatrix},\,
                \begin{bmatrix}0&amp;0\\1&amp;0\end{bmatrix},\,
                \begin{bmatrix}0&amp;0\\0&amp;1\end{bmatrix}
                }
                &amp;
                C&amp;=\set{1,\,x,\,x^2}</mrow>
            </md>then we can practically build the matrix representation on sight,<me>\matrixrep{S}{B}{C}=
                \begin{bmatrix}
                1 &amp; 2 &amp; 5 &amp; -4\\
                3 &amp; -1 &amp; 8 &amp; 2\\
                1 &amp; 1 &amp; 4 &amp; -2
                \end{bmatrix}</me>.  The first step is to find bases for the null space and column space of the matrix representation.  Row-reducing the matrix representation we find,<me>\begin{bmatrix}
                \leading{1} &amp; 0 &amp; 3 &amp; 0\\
                0 &amp; \leading{1} &amp; 1 &amp; -2\\
                0 &amp; 0 &amp; 0 &amp; 0
                \end{bmatrix}</me>.  So by <xref ref="theorem-BNS" acro="BNS"/> and <xref ref="theorem-BCS" acro="BCS"/>, we have<md>
                <mrow>\nsp{\matrixrep{S}{B}{C}}&amp;=\spn{\set{\colvector{-3\\-1\\1\\0},\,\colvector{0\\2\\0\\1}}}
                &amp;
                \csp{\matrixrep{S}{B}{C}}&amp;=\spn{\set{\colvector{1\\3\\1},\,\colvector{2\\-1\\1}}}</mrow>
            </md>.  Now, the proofs of <xref ref="theorem-KNSI" acro="KNSI"/> and <xref ref="theorem-RCSI" acro="RCSI"/> tell us that we can apply <m>\ltinverse{\vectrepname{B}}</m> and <m>\vectrepinvname{C}</m> (respectively) to <q>un-coordinatize</q> and get bases for the kernel and range of the linear transformation <m>S</m> itself,<md>
                <mrow>\krn{S}&amp;=\spn{\set{
                \begin{bmatrix}
                -3&amp;-1\\1&amp;0
                \end{bmatrix},\,
                \begin{bmatrix}
                0&amp;2\\0&amp;1
                \end{bmatrix}
                }}
                &amp;
                \rng{S}&amp;=\spn{\set{1+3x+x^2,\,2-x+x^2}}</mrow>
            </md>.</p>
            </solution>
        </exercise>
        <exercise number="C40" xml:id="exercise-MR-C40">
            <statement>
                <p>Let <m>S_{22}</m> be the set of <m>2\times 2</m> symmetric matrices.  Verify that the linear transformation <m>R</m> is invertible and find <m>\ltinverse{R}</m>.<me>\ltdefn{R}{S_{22}}{P_2},\quad\lteval{R}{\begin{bmatrix}a&amp;b\\b&amp;c\end{bmatrix}}=
                (a-b)+(2a-3b-2c)x+(a-b+c)x^2</me></p>
            </statement>
            <solution xml:id="solution-MR-C40">
                <p>The analysis of <m>R</m> will be easiest if we analyze a matrix representation of <m>R</m>.  Since we can use any matrix representation, we might as well use natural bases that allow us to construct the matrix representation quickly and easily,<md>
                    <mrow>B&amp;=\set{
                    \begin{bmatrix}1&amp;0\\0&amp;0\end{bmatrix},\,
                    \begin{bmatrix}0&amp;1\\1&amp;0\end{bmatrix},\,
                    \begin{bmatrix}0&amp;0\\0&amp;1\end{bmatrix}
                    }
                    &amp;
                    C&amp;=\set{1,\,x,\,x^2}</mrow>
                </md>then we can practically build the matrix representation on sight,<me>\matrixrep{R}{B}{C}=
                \begin{bmatrix}
                1 &amp; -1 &amp; 0\\
                2 &amp; -3 &amp; -2\\
                1 &amp; -1 &amp; 1
                \end{bmatrix}</me>.  This matrix representation is invertible (it has a nonzero determinant of <m>-1</m>, <xref ref="theorem-SMZD" acro="SMZD"/>, <xref ref="theorem-NI" acro="NI"/>) so <xref ref="theorem-IMR" acro="IMR"/> tells us that the linear transformation <m>R</m> is also invertible.  To find a formula for <m>\ltinverse{R}</m> we compute,<md>
                    <mrow>\lteval{\ltinverse{R}}{a+bx+cx^2}
                    &amp;=\vectrepinv{B}{\matrixrep{\ltinverse{R}}{C}{B}\vectrep{C}{a+bx+cx^2}}&amp;&amp;
                        <xref ref="theorem-FTMR" acro="FTMR"/></mrow>
                    <mrow>&amp;=\vectrepinv{B}{\inverse{\left(\matrixrep{R}{B}{C}\right)}\vectrep{C}{a+bx+cx^2}}&amp;&amp;
                        <xref ref="theorem-IMR" acro="IMR"/></mrow>
                    <mrow>&amp;=\vectrepinv{B}{\inverse{\left(\matrixrep{R}{B}{C}\right)}\colvector{a\\b\\c}}&amp;&amp;
                        <xref ref="definition-VR" acro="VR"/></mrow>
                    <mrow>&amp;=\vectrepinv{B}{
                    \begin{bmatrix}5&amp;-1&amp;-2\\4&amp;-1&amp;-2\\-1&amp;0&amp;1\end{bmatrix}
                    \colvector{a\\b\\c}}&amp;&amp;
                        <xref ref="definition-MI" acro="MI"/></mrow>
                    <mrow>&amp;=\vectrepinv{B}{\colvector{5a-b-2c\\4a-b-2c\\-a+c}}&amp;&amp;
                        <xref ref="definition-MVP" acro="MVP"/></mrow>
                    <mrow>&amp;=\begin{bmatrix}5a-b-2c&amp;4a-b-2c\\4a-b-2c&amp;-a+c\end{bmatrix}&amp;&amp;
                        <xref ref="definition-VR" acro="VR"/></mrow>
                </md>.</p>
            </solution>
        </exercise>
        <exercise number="C41" xml:id="exercise-MR-C41">
            <statement>
                <p>Prove that the linear transformation <m>S</m> is invertible.  Then find a formula for the inverse linear transformation, <m>\ltinverse{S}</m>, by employing a matrix inverse.<me>\ltdefn{S}{P_1}{M_{12}},\quad \lteval{S}{a+bx}=
                \begin{bmatrix} 3a+b &amp; 2a+b \end{bmatrix}</me></p>
            </statement>
            <solution xml:id="solution-MR-C41">
                <p>First, build a matrix representation of <m>S</m> (<xref ref="definition-MR" acro="MR"/>).  We are free to choose whatever bases we wish, so we should choose ones that are easy to work with, such as<md>
                    <mrow>B&amp;=\set{1,\,x}</mrow>
                    <mrow>C&amp;=\set{\begin{bmatrix}1 &amp; 0\end{bmatrix},\,\begin{bmatrix}0 &amp; 1\end{bmatrix}}</mrow>
                </md>.  The resulting matrix representation is then<me>\matrixrep{T}{B}{C}=
                \begin{bmatrix}
                3 &amp; 1\\
                2 &amp; 1
                \end{bmatrix}</me>.  This matrix is invertible since it has a nonzero determinant, so by <xref ref="theorem-IMR" acro="IMR"/> the linear transformation <m>S</m> is invertible.  We can use the matrix inverse and <xref ref="theorem-IMR" acro="IMR"/> to find a formula for the inverse linear transformation,<md>
                    <mrow>\lteval{\ltinverse{S}}{\begin{bmatrix}a&amp;b\end{bmatrix}}
                    &amp;=\vectrepinv{B}{\matrixrep{\ltinverse{S}}{C}{B}\vectrep{C}{\begin{bmatrix}a&amp;b\end{bmatrix}}}&amp;&amp;
                        <xref ref="theorem-FTMR" acro="FTMR"/></mrow>
                    <mrow>&amp;=\vectrepinv{B}{\inverse{\left(\matrixrep{S}{B}{C}\right)}\vectrep{C}{\begin{bmatrix}a&amp;b\end{bmatrix}}}&amp;&amp;
                        <xref ref="theorem-IMR" acro="IMR"/></mrow>
                    <mrow>&amp;=\vectrepinv{B}{\inverse{\left(\matrixrep{S}{B}{C}\right)}\colvector{a\\b}}&amp;&amp;
                        <xref ref="definition-VR" acro="VR"/></mrow>
                    <mrow>&amp;=\vectrepinv{B}{
                    \inverse{\left(
                    \begin{bmatrix}
                    3 &amp; 1\\
                    2 &amp; 1
                    \end{bmatrix}
                    \right)}
                    \colvector{a\\b}}\\
                    &amp;=\vectrepinv{B}{
                    \begin{bmatrix}
                    1 &amp; -1\\
                    -2 &amp; 3
                    \end{bmatrix}
                    \colvector{a\\b}}&amp;&amp;
                        <xref ref="definition-MI" acro="MI"/></mrow>
                    <mrow>&amp;=\vectrepinv{B}{
                    \colvector{
                    a-b</mrow>
                    <mrow>-2a+3b
                    }}&amp;&amp;
                        <xref ref="definition-MVP" acro="MVP"/></mrow>
                    <mrow>&amp;=(a-b)+(-2a+3b)x&amp;&amp;
                        <xref ref="definition-VR" acro="VR"/></mrow>
                 </md>.</p>
            </solution>
        </exercise>
        <exercise number="C42" xml:id="exercise-MR-C42">
            <statement>
                <p>The linear transformation <m>\ltdefn{R}{M_{12}}{M_{21}}</m> is invertible.  Use a matrix representation to determine a formula for the inverse linear transformation <m>\ltdefn{\ltinverse{R}}{M_{21}}{M_{12}}</m>.<me>\lteval{R}{\begin{bmatrix}a &amp; b\end{bmatrix}}
                =
                \begin{bmatrix}
                a+3b\\
                4a+11b
                \end{bmatrix}</me></p>
            </statement>
            <solution xml:id="solution-MR-C42">
                <p>Choose bases <m>B</m> and <m>C</m> for <m>M_{12}</m> and <m>M_{21}</m> (respectively),<md>
                <mrow>B=\set{
                \begin{bmatrix}1 &amp; 0\end{bmatrix},\,
                \begin{bmatrix}0 &amp; 1\end{bmatrix}
                }
                C=\set{
                \begin{bmatrix}1 \\ 0\end{bmatrix},\,
                \begin{bmatrix}0 \\ 1\end{bmatrix}
                }</mrow>
            </md>.  The resulting matrix representation is<me>\matrixrep{R}{B}{C}
            =
            \begin{bmatrix}
            1 &amp; 3\\
            4 &amp; 11
            \end{bmatrix}</me>.  This matrix is invertible (its determinant is nonzero, <xref ref="theorem-SMZD" acro="SMZD"/>), so by <xref ref="theorem-IMR" acro="IMR"/>, we can compute the matrix representation of <m>\ltinverse{R}</m> with a matrix inverse (<xref ref="theorem-TTMI" acro="TTMI"/>),<me>\matrixrep{\ltinverse{R}}{C}{B}
            =\inverse{\begin{bmatrix}1 &amp; 3\\4 &amp; 11\end{bmatrix}}
            =\begin{bmatrix}-11 &amp; 3\\4 &amp; -1\end{bmatrix}</me>.  To obtain a general formula for <m>\ltinverse{R}</m>, use <xref ref="theorem-FTMR" acro="FTMR"/>,<md>
                <mrow>\lteval{\ltinverse{R}}{\begin{bmatrix}x \\ y\end{bmatrix}}
                &amp;=\vectrepinv{B}{\matrixrep{\ltinverse{R}}{C}{B}\vectrep{C}{\begin{bmatrix}x\\y\end{bmatrix}}}</mrow>
                <mrow>&amp;=\vectrepinv{B}{\begin{bmatrix}-11&amp;3\\4&amp;-1\end{bmatrix}\colvector{x\\y}}</mrow>
                <mrow>&amp;=\vectrepinv{B}{\colvector{-11x+3y\\4x-y}}</mrow>
                <mrow>&amp;=\begin{bmatrix}-11x+3y&amp;4x-y\end{bmatrix}</mrow>
            </md>.</p>
            </solution>
        </exercise>
        <exercise number="C50" xml:id="exercise-MR-C50">
            <statement>
                <p>Use a matrix representation to find a basis for the range of the linear transformation <m>L</m>.<me>\ltdefn{L}{M_{22}}{P_2},\quad
                \lteval{T}{\begin{bmatrix}a&amp;b\\c&amp;d\end{bmatrix}}=
                (a+2b+4c+d)+(3a+c-2d)x+(-a+b+3c+3d)x^2</me></p>
            </statement>
            <solution xml:id="solution-MR-C50">
                <p>As usual, build any matrix representation of <m>L</m>, most likely using <q>nice</q> bases, such as<md>
                    <mrow>B&amp;=\set{
                    \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 0 \end{bmatrix},\,
                    \begin{bmatrix} 0 &amp; 1 \\ 0 &amp; 0 \end{bmatrix},\,
                    \begin{bmatrix} 0 &amp; 0 \\ 1 &amp; 0 \end{bmatrix},\,
                    \begin{bmatrix} 0 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}
                    }</mrow>
                    <mrow>C&amp;=\set{1,\,x,\,x^2}</mrow>
                </md>.  Then the matrix representation (<xref ref="definition-MR" acro="MR"/>) is,<me>\matrixrep{L}{B}{C}=
                \begin{bmatrix}
                 1 &amp; 2 &amp; 4 &amp; 1 \\
                 3 &amp; 0 &amp; 1 &amp; -2 \\
                 -1 &amp; 1 &amp; 3 &amp; 3
                \end{bmatrix}</me>.  <xref ref="theorem-RCSI" acro="RCSI"/> tells us that we can compute the column space of the matrix representation, then use the isomorphism <m>\vectrepinvname{C}</m> to convert the column space of the matrix representation into the range of the linear transformation.  So we first analyze the matrix representation,<me>\begin{bmatrix}
                 1 &amp; 2 &amp; 4 &amp; 1 \\
                 3 &amp; 0 &amp; 1 &amp; -2 \\
                 -1 &amp; 1 &amp; 3 &amp; 3
                \end{bmatrix}
                \rref
                \begin{bmatrix}
                 \leading{1} &amp; 0 &amp; 0 &amp; -1 \\
                 0 &amp; \leading{1} &amp; 0 &amp; -1 \\
                 0 &amp; 0 &amp; \leading{1} &amp; 1
                \end{bmatrix}</me>.  With three nonzero rows in the reduced row-echelon form of the matrix, we know the column space has dimension 3.  Since <m>P_2</m> has dimension 3 (<xref ref="theorem-DP" acro="DP"/>), the range must be all of <m>P_2</m>.  So <em>any</em> basis of <m>P_2</m> would suffice as a basis for the range.  For instance, <m>C</m> itself would be a correct answer.</p>
                <p>A more laborious approach would be to use <xref ref="theorem-BCS" acro="BCS"/> and choose the first three columns of the matrix representation as a basis for the range of the matrix representation.  These could then be <q>un-coordinatized</q> with <m>\vectrepinvname{C}</m> to yield a (<q>not nice</q>) basis for <m>P_2</m>.</p>
            </solution>
        </exercise>
        <exercise number="C51" xml:id="exercise-MR-C51">
            <statement>
                <p>Use a matrix representation to find a basis for the kernel of the linear transformation <m>L</m>.<me>\ltdefn{L}{M_{22}}{P_2},\quad
                \lteval{T}{\begin{bmatrix}a&amp;b\\c&amp;d\end{bmatrix}}=
                (a+2b+4c+d)+(3a+c-2d)x+(-a+b+3c+3d)x^2</me></p>
            </statement>
        </exercise>
        <exercise number="C52" xml:id="exercise-MR-C52">
            <statement>
                <p>Find a basis for the kernel of the linear transformation <m>\ltdefn{T}{P_2}{M_{22}}</m>.<me>\lteval{T}{a+bx+cx^2}=
                \begin{bmatrix}
                a+2b-2c &amp; 2a+2b \\
                -a+b-4c &amp; 3a+2b+2c
                \end{bmatrix}</me></p>
            </statement>
            <solution xml:id="solution-MR-C52">
                <p>Choose bases <m>B</m> and <m>C</m> for the matrix representation,<md>
                    <mrow>B&amp;=\set{1,\,x,\,x^2}
                    &amp;
                    C&amp;=\set{
                    \begin{bmatrix}1 &amp; 0 \\ 0 &amp; 0\end{bmatrix},\,
                    \begin{bmatrix}0 &amp; 1 \\ 0 &amp; 0\end{bmatrix},\,
                    \begin{bmatrix}0 &amp; 0 \\ 1 &amp; 0\end{bmatrix},\,
                    \begin{bmatrix}0 &amp; 0 \\ 0 &amp; 1\end{bmatrix}
                    }</mrow>
                </md>.  Input to <m>T</m> the vectors of the basis <m>B</m> and coordinatize the outputs relative to <m>C</m>,<md>
                    <mrow>\vectrep{C}{\lteval{T}{1}}&amp;=
                    \vectrep{C}{\begin{bmatrix}1 &amp; 2 \\ -1 &amp; 3\end{bmatrix}}
                    =
                    \vectrep{C}{
                    1\begin{bmatrix}1 &amp; 0 \\ 0 &amp; 0\end{bmatrix}+
                    2\begin{bmatrix}0 &amp; 1 \\ 0 &amp; 0\end{bmatrix}+
                    (-1)\begin{bmatrix}0 &amp; 0 \\ 1 &amp; 0\end{bmatrix}+
                    3\begin{bmatrix}0 &amp; 0 \\ 0 &amp; 1\end{bmatrix}
                    }
                    =
                    \colvector{1\\2\\-1\\3}</mrow>
                    <mrow>\vectrep{C}{\lteval{T}{x}}&amp;=
                    \vectrep{C}{\begin{bmatrix}2 &amp; 2 \\ 1 &amp; 2 \end{bmatrix}}
                    =
                    \vectrep{C}{
                    2\begin{bmatrix}1 &amp; 0 \\ 0 &amp; 0\end{bmatrix}+
                    2\begin{bmatrix}0 &amp; 1 \\ 0 &amp; 0\end{bmatrix}+
                    1\begin{bmatrix}0 &amp; 0 \\ 1 &amp; 0\end{bmatrix}+
                    2\begin{bmatrix}0 &amp; 0 \\ 0 &amp; 1\end{bmatrix}
                    }
                    =
                    \colvector{2\\2\\1\\2}</mrow>
                    <mrow>\vectrep{C}{\lteval{T}{x^2}}&amp;=
                    \vectrep{C}{\begin{bmatrix}-2 &amp; 0 \\ -4 &amp; 2\end{bmatrix}}
                    =
                    \vectrep{C}{
                    (-2)\begin{bmatrix}1 &amp; 0 \\ 0 &amp; 0\end{bmatrix}+
                    0\begin{bmatrix}0 &amp; 1 \\ 0 &amp; 0\end{bmatrix}+
                    (-4)\begin{bmatrix}0 &amp; 0 \\ 1 &amp; 0\end{bmatrix}+
                    2\begin{bmatrix}0 &amp; 0 \\ 0 &amp; 1\end{bmatrix}
                    }
                    =
                    \colvector{-2\\0\\-4\\2}</mrow>
                </md>.  Applying <xref ref="definition-MR" acro="MR"/> we have the matrix representation<me>\matrixrep{T}{B}{C}=
                \begin{bmatrix}
                 1 &amp; 2 &amp; -2 \\
                 2 &amp; 2 &amp; 0 \\
                 -1 &amp; 1 &amp; -4 \\
                 3 &amp; 2 &amp; 2
                \end{bmatrix}</me>.  The null space of the matrix representation is isomorphic (via <m>\vectrepname{B}</m>) to the kernel of the linear transformation (<xref ref="theorem-KNSI" acro="KNSI"/>).  So we compute the null space of the matrix representation by first row-reducing the matrix to,<me>\begin{bmatrix}
                 \leading{1} &amp; 0 &amp; 2 \\
                 0 &amp; \leading{1} &amp; -2 \\
                 0 &amp; 0 &amp; 0 \\
                 0 &amp; 0 &amp; 0
                \end{bmatrix}</me>.  Employing <xref ref="theorem-BNS" acro="BNS"/> we have<me>\nsp{\matrixrep{T}{B}{C}}=\spn{\set{\colvector{-2\\2\\1}}}</me>.  We only need to uncoordinatize this one basis vector to get a basis for <m>\krn{T}</m>,<me>\krn{T}
                =\spn{\set{\vectrepinv{B}{\colvector{-2\\2\\1}}}}
                =\spn{\set{-2+2x+x^2}}</me>.</p>
            </solution>
        </exercise>
        <exercise number="M20" xml:id="exercise-MR-M20">
            <statement>
                <p>The linear transformation <m>D</m> performs differentiation on polynomials.  Use a matrix representation of <m>D</m> to find the rank and nullity of <m>D</m>.<me>\ltdefn{D}{P_n}{P_n},\quad \lteval{D}{p(x)}=p^\prime(x)</me></p>
            </statement>
            <solution xml:id="solution-MR-M20">
                <p>Build a matrix representation (<xref ref="definition-MR" acro="MR"/>) with the set<me>B=\set{1,\,x,\,x^2,\,\dots,\,x^n}</me>employed as a basis of both the domain and codomain.  Then<md>
                    <mrow>\vectrep{B}{\lteval{D}{1}}&amp;=\vectrep{B}{0}
                    =\colvector{0\\0\\0\\\vdots\\0\\0}
                    &amp;
                    \vectrep{B}{\lteval{D}{x}}&amp;=\vectrep{B}{1}
                    =\colvector{1\\0\\0\\\vdots\\0\\0}</mrow>
                    <mrow>\vectrep{B}{\lteval{D}{x^2}}&amp;=\vectrep{B}{2x}
                    =\colvector{0\\2\\0\\\vdots\\0\\0}
                    &amp;
                    \vectrep{B}{\lteval{D}{x^3}}&amp;=\vectrep{B}{3x^2}
                    =\colvector{0\\0\\3\\\vdots\\0\\0}</mrow>
                    <mrow>&amp;\vdots</mrow>
                    <mrow>\vectrep{B}{\lteval{D}{x^n}}&amp;=\vectrep{B}{nx^{n-1}}
                    =\colvector{0\\0\\0\\\vdots\\n\\0}</mrow>
                </md>and the resulting matrix representation is<me>\matrixrep{D}{B}{B}=
                \begin{bmatrix}
                0 &amp; 1 &amp; 0 &amp; 0 &amp; \dots &amp; 0 &amp; 0 \\
                0 &amp; 0 &amp; 2 &amp; 0 &amp; \dots &amp; 0 &amp; 0 \\
                0 &amp; 0 &amp; 0 &amp; 3 &amp; \dots &amp; 0 &amp; 0 \\
                   &amp; \vdots &amp;&amp;&amp; \ddots &amp; &amp; \vdots \\
                0 &amp; 0 &amp; 0 &amp; 0 &amp; \dots &amp; 0 &amp; n \\
                0 &amp; 0 &amp; 0 &amp; 0 &amp; \dots &amp; 0 &amp; 0
                \end{bmatrix}</me>.  This <m>(n+1)\times(n+1)</m> matrix is very close to being in reduced row-echelon form.  Multiply row <m>i</m> by <m>\frac{1}{i}</m>, for <m>1\leq i\leq n</m>, to convert it to reduced row-echelon form.  From this we can see that matrix representation <m>\matrixrep{D}{B}{B}</m> has rank <m>n</m> and nullity <m>1</m>.  Applying <xref ref="theorem-RCSI" acro="RCSI"/> and <xref ref="theorem-KNSI" acro="KNSI"/> tells us that the linear transformation <m>D</m> will have the same values for the rank and nullity, as well.</p>
            </solution>
        </exercise>
        <exercise number="M60" xml:id="exercise-MR-M60">
            <statement>
                <p>Suppose <m>U</m> and <m>V</m> are vector spaces and define a function <m>\ltdefn{Z}{U}{V}</m> by <m>\lteval{Z}{\vect{u}}=\zerovector_{V}</m> for every <m>\vect{u}\in U</m>.  Then <xref ref="exercise-IVLT-M60" acro="IVLT.M60"/> asks you to formulate the theorem:  <m>Z</m> is invertible if and only if <m>U=\set{\zerovector_U}</m> and <m>V=\set{\zerovector_V}</m>.  What would a matrix representation of <m>Z</m> look like in this case?  How does <xref ref="theorem-IMR" acro="IMR"/> read in this case?</p>
            </statement>
        </exercise>
        <exercise number="M80" xml:id="exercise-MR-M80">
            <statement>
                <p>In light of <xref ref="theorem-KNSI" acro="KNSI"/> and <xref ref="theorem-MRCLT" acro="MRCLT"/>, write a short comparison of <xref ref="exercise-MM-T40" acro="MM.T40"/> with <xref ref="exercise-ILT-T15" acro="ILT.T15"/>.</p>
            </statement>
        </exercise>
        <exercise number="M81" xml:id="exercise-MR-M81">
            <statement>
                <p>In light of <xref ref="theorem-RCSI" acro="RCSI"/> and <xref ref="theorem-MRCLT" acro="MRCLT"/>, write a short comparison of <xref ref="exercise-CRS-T40" acro="CRS.T40"/> with <xref ref="exercise-SLT-T15" acro="SLT.T15"/>.</p>
            </statement>
        </exercise>
        <exercise number="M82" xml:id="exercise-MR-M82">
            <statement>
                <p>In light of <xref ref="theorem-MRCLT" acro="MRCLT"/> and <xref ref="theorem-IMR" acro="IMR"/>, write a short comparison of  <xref ref="theorem-SS" acro="SS"/> and <xref ref="theorem-ICLT" acro="ICLT"/>.</p>
            </statement>
        </exercise>
        <exercise number="M83" xml:id="exercise-MR-M83">
            <statement>
                <p>In light of <xref ref="theorem-MRCLT" acro="MRCLT"/> and <xref ref="theorem-IMR" acro="IMR"/>, write a short comparison of <xref ref="theorem-NPNT" acro="NPNT"/> and <xref ref="exercise-IVLT-T40" acro="IVLT.T40"/>.</p>
            </statement>
        </exercise>
        <exercise number="T20" xml:id="exercise-MR-T20">
            <statement>
                <p>Construct a new solution to <xref ref="exercise-B-T50" acro="B.T50"/> along the following outline.  From the <m>n\times n</m> matrix <m>A</m>, construct the linear transformation <m>\ltdefn{T}{\complex{n}}{\complex{n}}</m>, <m>\lteval{T}{\vect{x}}=A\vect{x}</m>.  Use <xref ref="theorem-NI" acro="NI"/>, <xref ref="theorem-IMILT" acro="IMILT"/> and <xref ref="theorem-ILTIS" acro="ILTIS"/> to translate between the nonsingularity of <m>A</m> and the surjectivity/injectivity of <m>T</m>.  Then apply <xref ref="theorem-ILTB" acro="ILTB"/> and  <xref ref="theorem-SLTB" acro="SLTB"/> to connect these properties with bases.</p>
            </statement>
            <solution xml:id="solution-MR-T20">
                <p>Given the nonsingular <m>n\times n</m> matrix <m>A</m>, create the linear transformation <m>\ltdefn{T}{\complex{n}}{\complex{n}}</m> defined by <m>\lteval{T}{\vect{x}}=A\vect{x}</m>.  Then<md>
                    <mrow>A\text{ nonsingular}
                    &amp;\iff A\text{ invertible}&amp;&amp;
                        <xref ref="theorem-NI" acro="NI"/></mrow>
                    <mrow>&amp;\iff T\text{ invertible}&amp;&amp;
                        <xref ref="theorem-IMILT" acro="IMILT"/></mrow>
                    <mrow>&amp;\iff T\text{ injective and surjective}&amp;&amp;
                        <xref ref="theorem-ILTIS" acro="ILTIS"/></mrow>
                    <mrow>&amp;\iff C\text{ linearly independent, and}&amp;&amp;
                        <xref ref="theorem-ILTB" acro="ILTB"/></mrow>
                    <mrow>&amp;\quad\quad\quad\quad C\text{ spans }\complex{n}&amp;&amp;
                        <xref ref="theorem-SLTB" acro="SLTB"/></mrow>
                    <mrow>&amp;\iff C\text{ basis for }\complex{n}&amp;&amp;
                        <xref ref="definition-B" acro="B"/></mrow>
                </md>.</p>
            </solution>
        </exercise>
        <exercise number="T40" xml:id="exercise-MR-T40">
            <statement contributor="chilijohnson">
                <p><xref ref="theorem-VSLT" acro="VSLT"/> defines the vector space <m>\vslt{U}{V}</m> containing all linear transformations with domain <m>U</m> and codomain <m>V</m>.  Suppose <m>\dimension{U}=n</m> and <m>\dimension{V}=m</m>.  Prove that <m>\vslt{U}{V}</m> is isomorphic to <m>M_{mn}</m>, the vector space of all <m>m\times n</m> matrices (<xref ref="example-VSM" acro="VSM"/>).  (Hint: we could have suggested this exercise in <xref ref="chapter-LT" acro="LT"/>, but have postponed it to this section.  Why?)</p>
            </statement>
        </exercise>
        <exercise number="T41" xml:id="exercise-MR-T41">
            <statement contributor="chilijohnson">
                <p><xref ref="theorem-VSLT" acro="VSLT"/> defines the vector space <m>\vslt{U}{V}</m> containing all linear transformations with domain <m>U</m> and codomain <m>V</m>.  Determine a basis for <m>\vslt{U}{V}</m>.  (Hint: study <xref ref="exercise-MR-T40" acro="MR.T40"/> first.)</p>
            </statement>
        </exercise>
        <exercise number="T60" xml:id="exercise-MR-T60">
            <statement>
                <p>Create an entirely different proof of <xref ref="theorem-IMILT" acro="IMILT"/>  that relies on <xref ref="definition-IVLT" acro="IVLT"/> to establish the invertibility of <m>T</m>, and that relies on <xref ref="definition-MI" acro="MI"/> to establish the invertibility of <m>A</m>.</p>
            </statement>
        </exercise>
        <exercise number="T80" xml:id="exercise-MR-T80">
            <statement>
                <p>Suppose that <m>\ltdefn{T}{U}{V}</m> and <m>\ltdefn{S}{V}{W}</m> are linear transformations, and that <m>B</m>, <m>C</m> and <m>D</m> are bases for <m>U</m>, <m>V</m>, and <m>W</m>.  Using only <xref ref="definition-MR" acro="MR"/> define matrix representations for <m>T</m> and <m>S</m>.  Using these two definitions, and <xref ref="definition-MR" acro="MR"/>, derive a matrix representation for the composition <m>\compose{S}{T}</m> in terms of the entries of the matrices <m>\matrixrep{T}{B}{C}</m> and <m>\matrixrep{S}{C}{D}</m>.  Explain how you would use this result to <em>motivate a definition</em> for matrix multiplication that is strikingly similar to <xref ref="theorem-EMP" acro="EMP"/>.</p>
            </statement>
            <solution xml:id="solution-MR-T80">
                <p>Suppose that <m>B=\set{\vectorlist{u}{m}}</m>, <m>C=\set{\vectorlist{v}{n}}</m> and <m>D=\set{\vectorlist{w}{p}}</m>.  For convenience, set <m>M=\matrixrep{T}{B}{C}</m>, <m>m_{ij}=\matrixentry{M}{ij}</m>, <m>1\leq i\leq n</m>, <m>1\leq j\leq m</m>, and similarly, set <m>N=\matrixrep{S}{C}{D}</m>, <m>n_{ij}=\matrixentry{N}{ij}</m>, <m>1\leq i\leq p</m>, <m>1\leq j\leq n</m>.   We want to learn about the matrix representation of <m>\ltdefn{\compose{S}{T}}{V}{W}</m> relative to <m>B</m> and <m>D</m>.  We will examine a single (generic) entry of this representation.</p>
                <p><md>
                    <mrow>\matrixentry{\matrixrep{\compose{S}{T}}{B}{D}}{ij}
                    &amp;=\vectorentry{\vectrep{D}{\lteval{\left(\compose{S}{T}\right)}{\vect{u}_j}}}{i}&amp;&amp;
                        <xref ref="definition-MR" acro="MR"/></mrow>
                    <mrow>&amp;=\vectorentry{\vectrep{D}{\lteval{S}{\lteval{T}{\vect{u}_j}}}}{i}&amp;&amp;
                        <xref ref="definition-LTC" acro="LTC"/></mrow>
                    <mrow>&amp;=\vectorentry{\vectrep{D}{\lteval{S}{\sum_{k=1}^{n}m_{kj}\vect{v}_k}}}{i}&amp;&amp;
                        <xref ref="definition-MR" acro="MR"/></mrow>
                    <mrow>&amp;=\vectorentry{\vectrep{D}{\sum_{k=1}^{n}m_{kj}\lteval{S}{\vect{v}_k}}}{i}&amp;&amp;
                        <xref ref="theorem-LTLC" acro="LTLC"/></mrow>
                    <mrow>&amp;=\vectorentry{\vectrep{D}{\sum_{k=1}^{n}m_{kj}\sum_{\ell=1}^{p}n_{\ell k}\vect{w}_\ell}}{i}&amp;&amp;
                        <xref ref="definition-MR" acro="MR"/></mrow>
                    <mrow>&amp;=\vectorentry{\vectrep{D}{\sum_{k=1}^{n}\sum_{\ell=1}^{p}m_{kj}n_{\ell k}\vect{w}_\ell}}{i}&amp;&amp;
                        <xref ref="property-DVA" acro="DVA"/></mrow>
                    <mrow>&amp;=\vectorentry{\vectrep{D}{\sum_{\ell=1}^{p}\sum_{k=1}^{n}m_{kj}n_{\ell k}\vect{w}_\ell}}{i}&amp;&amp;
                        <xref ref="property-C" acro="C"/></mrow>
                    <mrow>&amp;=\vectorentry{\vectrep{D}{\sum_{\ell=1}^{p}\left(\sum_{k=1}^{n}m_{kj}n_{\ell k}\right)\vect{w}_\ell}}{i}&amp;&amp;
                        <xref ref="property-DSA" acro="DSA"/></mrow>
                    <mrow>&amp;=\sum_{k=1}^{n}m_{kj}n_{ik}&amp;&amp;
                        <xref ref="definition-VR" acro="VR"/></mrow>
                    <mrow>&amp;=\sum_{k=1}^{n}n_{ik}m_{kj}&amp;&amp;
                        <xref ref="property-CMCN" acro="CMCN"/></mrow>
                    <mrow>&amp;=\sum_{k=1}^{n}\matrixentry{\matrixrep{S}{C}{D}}{ik}\matrixentry{\matrixrep{T}{B}{C}}{kj}&amp;&amp;
                        <xref ref="property-CMCN" acro="CMCN"/></mrow>
                </md>.  This formula for the entry of a matrix should remind you of <xref ref="theorem-EMP" acro="EMP"/>.  However, while the theorem presumed we knew how to multiply matrices, the solution before us never uses any understanding of matrix products.  It uses the definitions of vector and matrix representations, properties of linear transformations and vector spaces.  So if we began a course by first discussing vector space, and then linear transformations between vector spaces, we could carry matrix representations into a <em>motivation</em> for a definition of matrix multiplication that is grounded in function composition.  That is worth saying again <mdash/> a definition of matrix representations of linear transformations <em>results</em> in a matrix product being the representation of a composition of linear transformations.</p>
                <p>This exercise is meant to explain why many authors take the formula in <xref ref="theorem-EMP" acro="EMP"/> as their <em>definition</em> of matrix multiplication, and why it is a natural choice when the proper motivation is in place.  If we first defined matrix multiplication in the style of <xref ref="theorem-EMP" acro="EMP"/>, then the above argument, followed by a simple application of the definition of matrix equality (<xref ref="definition-ME" acro="ME"/>), would yield <xref ref="theorem-MRCLT" acro="MRCLT"/>.</p>
            </solution>
        </exercise>
    </exercises>
</section>
